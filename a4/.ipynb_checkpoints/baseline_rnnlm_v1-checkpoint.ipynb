{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Character-Level Language Model\n",
    "\n",
    "This is the \"working notebook\", with code to load and train the model, as well as run unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "#wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "#embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "#x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "#print('wordid placeholder shape:', wordid_ph.shape)\n",
    "#print('x shape:', x.shape)\n",
    "\n",
    "#sess = tf.Session()\n",
    "## Two sentences, each with four words.\n",
    "#wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "#x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "#print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "#print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "Load implementation, construct graph, and write a logdir for TensorBoard.\n",
    "```\n",
    "cd assignment/a4\n",
    "tensorboard --logdir /tmp/w266/a4_graph --port 6006\n",
    "```\n",
    "http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/artificial_hotel_reviews/a4_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  W_in:0\n",
      "Shape:  (10000, 200)\n",
      "[[-0.78748155 -0.05848813 -0.64172769 ...,  0.84141541 -0.94063973\n",
      "   0.6318059 ]\n",
      " [ 0.73271012  0.9191277   0.16166019 ..., -0.77998519  0.349545\n",
      "  -0.45933056]\n",
      " [ 0.56432152 -0.03329587  0.20995927 ..., -0.3828721   0.12824035\n",
      "  -0.13679957]\n",
      " ..., \n",
      " [-0.12901855 -0.29448342 -0.08786106 ..., -0.73254704 -0.37692833\n",
      "  -0.20111871]\n",
      " [-0.62848496 -0.50970197  0.9452734  ...,  0.99917698 -0.81992722\n",
      "  -0.37649083]\n",
      " [-0.09145284  0.00680947  0.67164588 ..., -0.92668557  0.46153927\n",
      "   0.74277234]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.02885439 -0.00585808 -0.04069098 ...,  0.00699186 -0.05404399\n",
      "   0.02202466]\n",
      " [ 0.06509807 -0.02223223  0.02077565 ..., -0.00037639  0.04783005\n",
      "  -0.00804034]\n",
      " [ 0.02742642  0.05705827 -0.03898749 ...,  0.00316557  0.06507974\n",
      "  -0.01836298]\n",
      " ..., \n",
      " [-0.00754741 -0.04928695  0.06990113 ..., -0.06414328 -0.06926721\n",
      "  -0.05963309]\n",
      " [-0.06505661 -0.0187963   0.05553678 ..., -0.00105456 -0.04487306\n",
      "   0.05926735]\n",
      " [ 0.05089028  0.03582108 -0.05539448 ...,  0.00139427  0.06008063\n",
      "   0.05134535]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.06411717 -0.059903    0.05128169 ...,  0.03041378  0.05353859\n",
      "   0.0544473 ]\n",
      " [ 0.06030189  0.02777085  0.0119326  ..., -0.00028028 -0.04805488\n",
      "  -0.03846691]\n",
      " [-0.02244554  0.04396319  0.06136966 ...,  0.02755227  0.01427022\n",
      "   0.0558702 ]\n",
      " ..., \n",
      " [-0.0622274  -0.03975505 -0.03073074 ...,  0.0241474  -0.04965901\n",
      "  -0.00885601]\n",
      " [ 0.0227494  -0.0019692  -0.05720998 ..., -0.03900233 -0.02878788\n",
      "  -0.05673516]\n",
      " [-0.04740062  0.00044262  0.0118935  ...,  0.02134912  0.04072783\n",
      "   0.00831893]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  W_out:0\n",
      "Shape:  (200, 10000)\n",
      "[[ 0.22999096 -0.98074627  0.2485311  ...,  0.31636119  0.64180779\n",
      "   0.48168755]\n",
      " [ 0.02860689  0.14025211  0.73703051 ...,  0.55039978 -0.11227965\n",
      "   0.06153202]\n",
      " [-0.74636269 -0.50103402  0.13248014 ..., -0.45557976  0.15847898\n",
      "   0.09724474]\n",
      " ..., \n",
      " [-0.21404767  0.9171977  -0.29322004 ..., -0.82013679  0.63434672\n",
      "  -0.56355882]\n",
      " [ 0.83722639 -0.26542854 -0.89726591 ..., -0.59170508  0.18550444\n",
      "   0.48118067]\n",
      " [ 0.55475283 -0.74218535 -0.24593616 ..., -0.88373137 -0.86611366\n",
      "  -0.00220013]]\n",
      "Variable:  b_out:0\n",
      "Shape:  (10000,)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#with lm.graph.as_default():\n",
    "    #initializer = tf.global_variables_initializer()\n",
    "\n",
    "#with tf.Session(graph=lm.graph) as session:\n",
    "    #session.run(initializer)\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unit Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 3.315s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNNLM\n",
    "\n",
    "Data loader functions in **`utils.py`**.\n",
    "\n",
    "`utils.rnnlm_batch_generator` returns an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "Toy corpus example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_epoch()\n",
    "\n",
    "`train=True` flag enables train mode. `train=False` runs `score_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Unit Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 0]: seen 50 words at 43.1 wps, loss = 2.028\n",
      "[batch 98]: seen 4950 words at 2284.2 wps, loss = 1.114\n",
      "[batch 210]: seen 10550 words at 3329.2 wps, loss = 0.959\n",
      "[batch 346]: seen 17350 words at 4155.3 wps, loss = 1.024\n",
      "[batch 481]: seen 24100 words at 4656.4 wps, loss = 0.983\n",
      "[batch 615]: seen 30800 words at 4987.1 wps, loss = 0.943\n",
      "[batch 751]: seen 37600 words at 5237.5 wps, loss = 0.954\n",
      "[batch 887]: seen 44400 words at 5425.6 wps, loss = 0.964\n",
      "[batch 1024]: seen 51250 words at 5579.2 wps, loss = 0.954\n",
      "[batch 1160]: seen 58050 words at 5697.4 wps, loss = 0.943\n",
      "[batch 1296]: seen 64850 words at 5794.1 wps, loss = 0.930\n",
      "[batch 1431]: seen 71600 words at 5872.2 wps, loss = 0.921\n",
      "[batch 1567]: seen 78400 words at 5940.1 wps, loss = 0.917\n",
      "[batch 1702]: seen 85150 words at 5995.7 wps, loss = 0.905\n",
      "[batch 1839]: seen 92000 words at 6050.8 wps, loss = 0.909\n",
      "[batch 1976]: seen 98850 words at 6098.0 wps, loss = 0.914\n",
      "[batch 2112]: seen 105650 words at 6138.3 wps, loss = 0.906\n",
      "[batch 2245]: seen 112300 words at 6164.1 wps, loss = 0.893\n",
      "[batch 2379]: seen 119000 words at 6191.6 wps, loss = 0.883\n",
      "[batch 2516]: seen 125850 words at 6222.1 wps, loss = 0.881\n",
      "[batch 2650]: seen 132550 words at 6243.5 wps, loss = 0.886\n",
      "[batch 2786]: seen 139350 words at 6268.4 wps, loss = 0.880\n",
      "[batch 2919]: seen 146000 words at 6283.9 wps, loss = 0.875\n",
      "[batch 3055]: seen 152800 words at 6303.9 wps, loss = 0.870\n",
      "[batch 3189]: seen 159500 words at 6319.6 wps, loss = 0.874\n",
      "[batch 3323]: seen 166200 words at 6334.0 wps, loss = 0.878\n",
      "[batch 3456]: seen 172850 words at 6344.5 wps, loss = 0.889\n",
      "[batch 3587]: seen 179400 words at 6350.9 wps, loss = 0.886\n",
      "[batch 3717]: seen 185900 words at 6355.3 wps, loss = 0.880\n",
      "[batch 3844]: seen 192250 words at 6353.9 wps, loss = 0.873\n",
      "[batch 3970]: seen 198550 words at 6351.4 wps, loss = 0.870\n",
      "Train set: avg. loss: 0.018  (perplexity: 1.02)\n",
      "Test set: avg. loss: 0.038  (perplexity: 1.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 33.626s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "At the end of training, `tf.train.Saver` saves a copy of the model to `/tmp/w266/artificial_hotel_reviews/rnnlm_trained`. To load this from disk, see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/kalvin_kao/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "## Load the dataset\n",
    "#V = 10000\n",
    "#vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv(review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_csv(business_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_review_df = review_df[review_df['stars']==5]\n",
    "five_star_review_series = five_star_review_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in five_star_review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "        #print(final_review)\n",
    "        review_list.append(final_review)\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed reviews\n",
    "#review_list = preprocess_review_series(five_star_review_series)\n",
    "review_list = preprocess_review_series(five_star_review_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24606067\n"
     ]
    }
   ],
   "source": [
    "#combined_review_list = [item for sublist in review_list for item in sublist]\n",
    "training_review_list = [item for sublist in review_list[:50000] for item in sublist]\n",
    "print(len(training_review_list))\n",
    "test_review_list = [item for sublist in review_list[50000:51000] for item in sublist]\n",
    "#unique_characters = list(set(combined_review_list))\n",
    "unique_characters = list(set(training_review_list + test_review_list))\n",
    "#len(unique_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary\n",
    "char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#print(char_dict)\n",
    "#print(ids_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "print(len(char_dict.keys()))\n",
    "print(len(unique_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to flat (1D) np.array(int) of ids\n",
    "#add memory to VM and remove 1000 slice\n",
    "#combined_review_ids = [char_dict.get(token) for token in combined_review_list[:1000]]\n",
    "training_review_ids = [char_dict.get(token) for token in training_review_list]\n",
    "test_review_ids = [char_dict.get(token) for token in test_review_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array(training_review_ids)\n",
    "test_ids = np.array(test_review_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "## add parameter sets for each attack/defense configuration\n",
    "#max_time = 25\n",
    "max_time = 100\n",
    "#batch_size = 100\n",
    "batch_size = 256\n",
    "#learning_rate = 0.01\n",
    "learning_rate = 0.002\n",
    "#num_epochs = 10\n",
    "num_epochs = 20\n",
    "\n",
    "# Model parameters\n",
    "#model_params = dict(V=vocab.size, \n",
    "                    #H=200, \n",
    "                    #softmax_ns=200,\n",
    "                    #num_layers=2)\n",
    "model_params = dict(V=len(unique_characters), \n",
    "                    H=1024, \n",
    "                    softmax_ns=len(unique_characters),\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 2]: seen 18000 words at 1583.1 wps, loss = 28.579\n",
      "[batch 6]: seen 42000 words at 1708.0 wps, loss = 21.015\n",
      "[batch 10]: seen 66000 words at 1746.8 wps, loss = 15.758\n",
      "[batch 14]: seen 90000 words at 1766.3 wps, loss = 12.785\n",
      "[batch 18]: seen 114000 words at 1780.4 wps, loss = 10.889\n",
      "[batch 22]: seen 138000 words at 1787.2 wps, loss = 9.577\n",
      "[batch 26]: seen 162000 words at 1793.4 wps, loss = 8.617\n",
      "[batch 30]: seen 186000 words at 1796.6 wps, loss = 7.878\n",
      "[batch 34]: seen 210000 words at 1800.7 wps, loss = 7.298\n",
      "[batch 38]: seen 234000 words at 1802.9 wps, loss = 6.828\n",
      "[batch 42]: seen 258000 words at 1804.2 wps, loss = 6.440\n",
      "[batch 46]: seen 282000 words at 1805.9 wps, loss = 6.112\n",
      "[batch 50]: seen 306000 words at 1807.0 wps, loss = 5.832\n",
      "[batch 54]: seen 330000 words at 1808.2 wps, loss = 5.588\n",
      "[batch 58]: seen 354000 words at 1809.1 wps, loss = 5.375\n",
      "[batch 62]: seen 378000 words at 1810.0 wps, loss = 5.188\n",
      "[batch 66]: seen 402000 words at 1811.0 wps, loss = 5.022\n",
      "[batch 70]: seen 426000 words at 1811.5 wps, loss = 4.873\n",
      "[batch 74]: seen 450000 words at 1812.1 wps, loss = 4.737\n",
      "[batch 78]: seen 474000 words at 1812.6 wps, loss = 4.613\n",
      "[batch 82]: seen 498000 words at 1812.8 wps, loss = 4.502\n",
      "[batch 86]: seen 522000 words at 1813.3 wps, loss = 4.400\n",
      "[batch 90]: seen 546000 words at 1813.9 wps, loss = 4.306\n",
      "[batch 94]: seen 570000 words at 1814.3 wps, loss = 4.220\n",
      "[batch 98]: seen 594000 words at 1814.9 wps, loss = 4.140\n",
      "[batch 102]: seen 618000 words at 1815.5 wps, loss = 4.066\n",
      "[batch 106]: seen 642000 words at 1815.9 wps, loss = 3.997\n",
      "[batch 110]: seen 666000 words at 1816.3 wps, loss = 3.933\n",
      "[batch 114]: seen 690000 words at 1816.1 wps, loss = 3.872\n",
      "[batch 118]: seen 714000 words at 1815.5 wps, loss = 3.816\n",
      "[batch 121]: seen 732000 words at 1815.1 wps, loss = 3.776\n",
      "[batch 125]: seen 756000 words at 1814.9 wps, loss = 3.725\n",
      "[batch 129]: seen 780000 words at 1814.7 wps, loss = 3.677\n",
      "[batch 133]: seen 804000 words at 1814.3 wps, loss = 3.631\n",
      "[batch 137]: seen 828000 words at 1814.4 wps, loss = 3.588\n",
      "[batch 141]: seen 852000 words at 1814.4 wps, loss = 3.548\n",
      "[batch 145]: seen 876000 words at 1814.5 wps, loss = 3.510\n",
      "[batch 149]: seen 900000 words at 1814.4 wps, loss = 3.473\n",
      "[batch 153]: seen 924000 words at 1814.1 wps, loss = 3.437\n",
      "[batch 157]: seen 948000 words at 1813.9 wps, loss = 3.404\n",
      "[batch 160]: seen 966000 words at 1813.6 wps, loss = 3.379\n",
      "[batch 164]: seen 990000 words at 1813.5 wps, loss = 3.349\n",
      "[batch 168]: seen 1014000 words at 1813.3 wps, loss = 3.320\n",
      "[batch 172]: seen 1038000 words at 1813.3 wps, loss = 3.292\n",
      "[batch 176]: seen 1062000 words at 1813.3 wps, loss = 3.265\n",
      "[batch 180]: seen 1086000 words at 1813.3 wps, loss = 3.239\n",
      "[batch 184]: seen 1110000 words at 1813.5 wps, loss = 3.214\n",
      "[batch 187]: seen 1128000 words at 1813.1 wps, loss = 3.196\n",
      "[batch 191]: seen 1152000 words at 1812.9 wps, loss = 3.172\n",
      "[batch 194]: seen 1170000 words at 1812.6 wps, loss = 3.156\n",
      "[batch 198]: seen 1194000 words at 1812.5 wps, loss = 3.134\n",
      "[batch 202]: seen 1218000 words at 1812.4 wps, loss = 3.113\n",
      "[batch 206]: seen 1242000 words at 1812.3 wps, loss = 3.093\n",
      "[batch 210]: seen 1266000 words at 1812.1 wps, loss = 3.072\n",
      "[batch 214]: seen 1290000 words at 1812.1 wps, loss = 3.054\n",
      "[batch 218]: seen 1314000 words at 1812.1 wps, loss = 3.035\n",
      "[batch 222]: seen 1338000 words at 1812.0 wps, loss = 3.017\n",
      "[batch 226]: seen 1362000 words at 1812.1 wps, loss = 2.999\n",
      "[batch 230]: seen 1386000 words at 1812.1 wps, loss = 2.982\n",
      "[batch 234]: seen 1410000 words at 1812.1 wps, loss = 2.966\n",
      "[batch 238]: seen 1434000 words at 1811.9 wps, loss = 2.951\n",
      "[batch 242]: seen 1458000 words at 1812.0 wps, loss = 2.935\n",
      "[batch 246]: seen 1482000 words at 1811.9 wps, loss = 2.920\n",
      "[batch 250]: seen 1506000 words at 1812.0 wps, loss = 2.905\n",
      "[batch 254]: seen 1530000 words at 1812.1 wps, loss = 2.890\n",
      "[batch 258]: seen 1554000 words at 1812.1 wps, loss = 2.876\n",
      "[batch 262]: seen 1578000 words at 1812.1 wps, loss = 2.862\n",
      "[batch 266]: seen 1602000 words at 1812.2 wps, loss = 2.849\n",
      "[batch 269]: seen 1620000 words at 1812.0 wps, loss = 2.840\n",
      "[batch 273]: seen 1644000 words at 1812.2 wps, loss = 2.826\n",
      "[batch 277]: seen 1668000 words at 1812.2 wps, loss = 2.814\n",
      "[batch 281]: seen 1692000 words at 1812.2 wps, loss = 2.802\n",
      "[batch 285]: seen 1716000 words at 1812.2 wps, loss = 2.790\n",
      "[batch 289]: seen 1740000 words at 1812.2 wps, loss = 2.778\n",
      "[batch 293]: seen 1764000 words at 1812.3 wps, loss = 2.766\n",
      "[batch 297]: seen 1788000 words at 1812.3 wps, loss = 2.755\n",
      "[batch 301]: seen 1812000 words at 1812.3 wps, loss = 2.744\n",
      "[batch 305]: seen 1836000 words at 1812.3 wps, loss = 2.733\n",
      "[batch 309]: seen 1860000 words at 1812.5 wps, loss = 2.723\n",
      "[batch 313]: seen 1884000 words at 1812.5 wps, loss = 2.712\n",
      "[batch 317]: seen 1908000 words at 1812.5 wps, loss = 2.702\n",
      "[batch 321]: seen 1932000 words at 1812.5 wps, loss = 2.692\n",
      "[batch 325]: seen 1956000 words at 1812.6 wps, loss = 2.682\n",
      "[batch 329]: seen 1980000 words at 1812.7 wps, loss = 2.672\n",
      "[batch 332]: seen 1998000 words at 1812.5 wps, loss = 2.665\n",
      "[batch 336]: seen 2022000 words at 1812.6 wps, loss = 2.656\n",
      "[batch 340]: seen 2046000 words at 1812.7 wps, loss = 2.647\n",
      "[batch 344]: seen 2070000 words at 1812.8 wps, loss = 2.638\n",
      "[batch 348]: seen 2094000 words at 1812.9 wps, loss = 2.629\n",
      "[batch 352]: seen 2118000 words at 1812.9 wps, loss = 2.621\n",
      "[batch 356]: seen 2142000 words at 1812.8 wps, loss = 2.612\n",
      "[batch 360]: seen 2166000 words at 1812.9 wps, loss = 2.604\n",
      "[batch 364]: seen 2190000 words at 1812.9 wps, loss = 2.596\n",
      "[batch 368]: seen 2214000 words at 1813.0 wps, loss = 2.588\n",
      "[batch 372]: seen 2238000 words at 1813.1 wps, loss = 2.580\n",
      "[batch 376]: seen 2262000 words at 1813.1 wps, loss = 2.573\n",
      "[batch 380]: seen 2286000 words at 1813.1 wps, loss = 2.566\n",
      "[batch 384]: seen 2310000 words at 1813.2 wps, loss = 2.558\n",
      "[batch 388]: seen 2334000 words at 1813.1 wps, loss = 2.551\n",
      "[batch 392]: seen 2358000 words at 1813.1 wps, loss = 2.543\n",
      "[batch 396]: seen 2382000 words at 1813.2 wps, loss = 2.536\n",
      "[batch 400]: seen 2406000 words at 1813.2 wps, loss = 2.529\n",
      "[batch 404]: seen 2430000 words at 1813.1 wps, loss = 2.522\n",
      "[batch 408]: seen 2454000 words at 1813.2 wps, loss = 2.515\n",
      "[batch 412]: seen 2478000 words at 1813.2 wps, loss = 2.508\n",
      "[batch 416]: seen 2502000 words at 1813.3 wps, loss = 2.502\n",
      "[batch 420]: seen 2526000 words at 1813.4 wps, loss = 2.495\n",
      "[batch 424]: seen 2550000 words at 1813.5 wps, loss = 2.489\n",
      "[batch 428]: seen 2574000 words at 1813.4 wps, loss = 2.482\n",
      "[batch 432]: seen 2598000 words at 1813.5 wps, loss = 2.476\n",
      "[batch 436]: seen 2622000 words at 1813.5 wps, loss = 2.470\n",
      "[batch 440]: seen 2646000 words at 1813.5 wps, loss = 2.464\n",
      "[batch 444]: seen 2670000 words at 1813.5 wps, loss = 2.458\n",
      "[batch 448]: seen 2694000 words at 1813.5 wps, loss = 2.452\n",
      "[batch 452]: seen 2718000 words at 1813.5 wps, loss = 2.446\n",
      "[batch 456]: seen 2742000 words at 1813.5 wps, loss = 2.441\n",
      "[batch 460]: seen 2766000 words at 1813.5 wps, loss = 2.435\n",
      "[batch 464]: seen 2790000 words at 1813.6 wps, loss = 2.430\n",
      "[batch 468]: seen 2814000 words at 1813.6 wps, loss = 2.424\n",
      "[batch 472]: seen 2838000 words at 1813.7 wps, loss = 2.419\n",
      "[batch 476]: seen 2862000 words at 1813.6 wps, loss = 2.414\n",
      "[batch 479]: seen 2880000 words at 1813.6 wps, loss = 2.410\n",
      "[batch 483]: seen 2904000 words at 1813.5 wps, loss = 2.405\n",
      "[batch 487]: seen 2928000 words at 1813.5 wps, loss = 2.400\n",
      "[batch 491]: seen 2952000 words at 1813.5 wps, loss = 2.395\n",
      "[batch 495]: seen 2976000 words at 1813.6 wps, loss = 2.390\n",
      "[batch 499]: seen 3000000 words at 1813.6 wps, loss = 2.385\n",
      "[batch 503]: seen 3024000 words at 1813.6 wps, loss = 2.380\n",
      "[batch 507]: seen 3048000 words at 1813.6 wps, loss = 2.375\n",
      "[batch 511]: seen 3072000 words at 1813.6 wps, loss = 2.370\n",
      "[batch 515]: seen 3096000 words at 1813.6 wps, loss = 2.365\n",
      "[batch 519]: seen 3120000 words at 1813.6 wps, loss = 2.360\n",
      "[batch 523]: seen 3144000 words at 1813.7 wps, loss = 2.356\n",
      "[batch 527]: seen 3168000 words at 1813.7 wps, loss = 2.351\n",
      "[batch 531]: seen 3192000 words at 1813.7 wps, loss = 2.347\n",
      "[batch 535]: seen 3216000 words at 1813.7 wps, loss = 2.342\n",
      "[batch 539]: seen 3240000 words at 1813.7 wps, loss = 2.338\n",
      "[batch 543]: seen 3264000 words at 1813.7 wps, loss = 2.334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 547]: seen 3288000 words at 1813.7 wps, loss = 2.329\n",
      "[batch 551]: seen 3312000 words at 1813.7 wps, loss = 2.325\n",
      "[batch 555]: seen 3336000 words at 1813.8 wps, loss = 2.321\n",
      "[batch 559]: seen 3360000 words at 1813.8 wps, loss = 2.316\n",
      "[batch 563]: seen 3384000 words at 1813.8 wps, loss = 2.312\n",
      "[batch 567]: seen 3408000 words at 1813.7 wps, loss = 2.308\n",
      "[batch 571]: seen 3432000 words at 1813.7 wps, loss = 2.303\n",
      "[batch 575]: seen 3456000 words at 1813.8 wps, loss = 2.299\n",
      "[batch 579]: seen 3480000 words at 1813.7 wps, loss = 2.295\n",
      "[batch 583]: seen 3504000 words at 1813.8 wps, loss = 2.291\n",
      "[batch 587]: seen 3528000 words at 1813.7 wps, loss = 2.287\n",
      "[batch 591]: seen 3552000 words at 1813.8 wps, loss = 2.283\n",
      "[batch 595]: seen 3576000 words at 1813.7 wps, loss = 2.279\n",
      "[batch 599]: seen 3600000 words at 1813.7 wps, loss = 2.275\n",
      "[batch 603]: seen 3624000 words at 1813.8 wps, loss = 2.271\n",
      "[batch 607]: seen 3648000 words at 1813.8 wps, loss = 2.267\n",
      "[batch 611]: seen 3672000 words at 1813.8 wps, loss = 2.263\n",
      "[batch 615]: seen 3696000 words at 1813.9 wps, loss = 2.259\n",
      "[batch 619]: seen 3720000 words at 1813.9 wps, loss = 2.255\n",
      "[batch 623]: seen 3744000 words at 1813.9 wps, loss = 2.252\n",
      "[batch 627]: seen 3768000 words at 1814.0 wps, loss = 2.248\n",
      "[batch 631]: seen 3792000 words at 1813.9 wps, loss = 2.244\n",
      "[batch 635]: seen 3816000 words at 1814.0 wps, loss = 2.241\n",
      "[batch 639]: seen 3840000 words at 1814.0 wps, loss = 2.237\n",
      "[batch 643]: seen 3864000 words at 1814.1 wps, loss = 2.234\n",
      "[batch 647]: seen 3888000 words at 1814.1 wps, loss = 2.230\n",
      "[batch 651]: seen 3912000 words at 1814.1 wps, loss = 2.227\n",
      "[batch 655]: seen 3936000 words at 1814.2 wps, loss = 2.223\n",
      "[batch 659]: seen 3960000 words at 1814.2 wps, loss = 2.220\n",
      "[batch 663]: seen 3984000 words at 1814.3 wps, loss = 2.216\n",
      "[batch 667]: seen 4008000 words at 1814.3 wps, loss = 2.213\n",
      "[batch 671]: seen 4032000 words at 1814.3 wps, loss = 2.209\n",
      "[batch 675]: seen 4056000 words at 1814.3 wps, loss = 2.206\n",
      "[batch 679]: seen 4080000 words at 1814.3 wps, loss = 2.203\n",
      "[batch 683]: seen 4104000 words at 1814.2 wps, loss = 2.199\n",
      "[batch 687]: seen 4128000 words at 1814.3 wps, loss = 2.196\n",
      "[batch 691]: seen 4152000 words at 1814.3 wps, loss = 2.193\n",
      "[batch 695]: seen 4176000 words at 1814.3 wps, loss = 2.189\n",
      "[batch 699]: seen 4200000 words at 1814.4 wps, loss = 2.186\n",
      "[batch 703]: seen 4224000 words at 1814.4 wps, loss = 2.183\n",
      "[batch 707]: seen 4248000 words at 1814.4 wps, loss = 2.179\n",
      "[batch 711]: seen 4272000 words at 1814.5 wps, loss = 2.176\n",
      "[batch 715]: seen 4296000 words at 1814.4 wps, loss = 2.173\n",
      "[batch 719]: seen 4320000 words at 1814.4 wps, loss = 2.170\n",
      "[batch 723]: seen 4344000 words at 1814.5 wps, loss = 2.167\n",
      "[batch 727]: seen 4368000 words at 1814.6 wps, loss = 2.164\n",
      "[batch 731]: seen 4392000 words at 1814.6 wps, loss = 2.161\n",
      "[batch 735]: seen 4416000 words at 1814.6 wps, loss = 2.158\n",
      "[batch 739]: seen 4440000 words at 1814.7 wps, loss = 2.155\n",
      "[batch 743]: seen 4464000 words at 1814.7 wps, loss = 2.152\n",
      "[batch 747]: seen 4488000 words at 1814.8 wps, loss = 2.149\n",
      "[batch 751]: seen 4512000 words at 1814.8 wps, loss = 2.146\n",
      "[batch 755]: seen 4536000 words at 1814.8 wps, loss = 2.143\n",
      "[batch 759]: seen 4560000 words at 1814.9 wps, loss = 2.140\n",
      "[batch 763]: seen 4584000 words at 1815.0 wps, loss = 2.137\n",
      "[batch 767]: seen 4608000 words at 1815.0 wps, loss = 2.134\n",
      "[batch 771]: seen 4632000 words at 1815.0 wps, loss = 2.131\n",
      "[batch 775]: seen 4656000 words at 1815.1 wps, loss = 2.128\n",
      "[batch 779]: seen 4680000 words at 1815.1 wps, loss = 2.126\n",
      "[batch 783]: seen 4704000 words at 1815.1 wps, loss = 2.123\n",
      "[batch 787]: seen 4728000 words at 1815.1 wps, loss = 2.120\n",
      "[batch 791]: seen 4752000 words at 1815.2 wps, loss = 2.117\n",
      "[batch 795]: seen 4776000 words at 1815.2 wps, loss = 2.114\n",
      "[batch 799]: seen 4800000 words at 1815.3 wps, loss = 2.111\n",
      "[batch 803]: seen 4824000 words at 1815.3 wps, loss = 2.109\n",
      "[batch 807]: seen 4848000 words at 1815.3 wps, loss = 2.106\n",
      "[batch 811]: seen 4872000 words at 1815.3 wps, loss = 2.103\n",
      "[batch 815]: seen 4896000 words at 1815.3 wps, loss = 2.101\n",
      "[batch 819]: seen 4920000 words at 1815.4 wps, loss = 2.098\n",
      "[batch 823]: seen 4944000 words at 1815.4 wps, loss = 2.095\n",
      "[batch 827]: seen 4968000 words at 1815.4 wps, loss = 2.093\n",
      "[batch 831]: seen 4992000 words at 1815.4 wps, loss = 2.090\n",
      "[batch 835]: seen 5016000 words at 1815.4 wps, loss = 2.088\n",
      "[batch 839]: seen 5040000 words at 1815.4 wps, loss = 2.085\n",
      "[batch 843]: seen 5064000 words at 1815.4 wps, loss = 2.083\n",
      "[batch 847]: seen 5088000 words at 1815.4 wps, loss = 2.080\n",
      "[batch 851]: seen 5112000 words at 1815.5 wps, loss = 2.078\n",
      "[batch 855]: seen 5136000 words at 1815.5 wps, loss = 2.075\n",
      "[batch 859]: seen 5160000 words at 1815.4 wps, loss = 2.073\n",
      "[batch 863]: seen 5184000 words at 1815.4 wps, loss = 2.070\n",
      "[batch 867]: seen 5208000 words at 1815.4 wps, loss = 2.068\n",
      "[batch 871]: seen 5232000 words at 1815.3 wps, loss = 2.065\n",
      "[batch 875]: seen 5256000 words at 1815.4 wps, loss = 2.063\n",
      "[batch 879]: seen 5280000 words at 1815.3 wps, loss = 2.061\n",
      "[batch 883]: seen 5304000 words at 1815.4 wps, loss = 2.058\n",
      "[batch 887]: seen 5328000 words at 1815.4 wps, loss = 2.056\n",
      "[batch 891]: seen 5352000 words at 1815.5 wps, loss = 2.054\n",
      "[batch 895]: seen 5376000 words at 1815.5 wps, loss = 2.051\n",
      "[batch 899]: seen 5400000 words at 1815.5 wps, loss = 2.049\n",
      "[batch 903]: seen 5424000 words at 1815.5 wps, loss = 2.046\n",
      "[batch 907]: seen 5448000 words at 1815.6 wps, loss = 2.044\n",
      "[batch 911]: seen 5472000 words at 1815.6 wps, loss = 2.042\n",
      "[batch 915]: seen 5496000 words at 1815.7 wps, loss = 2.040\n",
      "[batch 919]: seen 5520000 words at 1815.7 wps, loss = 2.037\n",
      "[batch 923]: seen 5544000 words at 1815.7 wps, loss = 2.035\n",
      "[batch 927]: seen 5568000 words at 1815.7 wps, loss = 2.033\n",
      "[batch 931]: seen 5592000 words at 1815.8 wps, loss = 2.031\n",
      "[batch 935]: seen 5616000 words at 1815.8 wps, loss = 2.028\n",
      "[batch 939]: seen 5640000 words at 1815.8 wps, loss = 2.026\n",
      "[batch 943]: seen 5664000 words at 1815.9 wps, loss = 2.024\n",
      "[batch 947]: seen 5688000 words at 1815.9 wps, loss = 2.022\n",
      "[batch 951]: seen 5712000 words at 1815.8 wps, loss = 2.020\n",
      "[batch 955]: seen 5736000 words at 1815.9 wps, loss = 2.018\n",
      "[batch 959]: seen 5760000 words at 1815.9 wps, loss = 2.016\n",
      "[batch 963]: seen 5784000 words at 1815.9 wps, loss = 2.013\n",
      "[batch 967]: seen 5808000 words at 1815.9 wps, loss = 2.011\n",
      "[batch 971]: seen 5832000 words at 1815.9 wps, loss = 2.009\n",
      "[batch 975]: seen 5856000 words at 1815.9 wps, loss = 2.007\n",
      "[batch 979]: seen 5880000 words at 1815.9 wps, loss = 2.005\n",
      "[batch 983]: seen 5904000 words at 1816.0 wps, loss = 2.003\n",
      "[batch 987]: seen 5928000 words at 1815.9 wps, loss = 2.001\n",
      "[batch 991]: seen 5952000 words at 1815.9 wps, loss = 1.999\n",
      "[batch 995]: seen 5976000 words at 1815.9 wps, loss = 1.997\n",
      "[batch 999]: seen 6000000 words at 1815.9 wps, loss = 1.995\n",
      "[batch 1003]: seen 6024000 words at 1815.9 wps, loss = 1.993\n",
      "[batch 1007]: seen 6048000 words at 1815.9 wps, loss = 1.991\n",
      "[batch 1011]: seen 6072000 words at 1816.0 wps, loss = 1.989\n",
      "[batch 1015]: seen 6096000 words at 1815.9 wps, loss = 1.987\n",
      "[batch 1019]: seen 6120000 words at 1815.9 wps, loss = 1.985\n",
      "[batch 1022]: seen 6138000 words at 1815.9 wps, loss = 1.984\n",
      "[batch 1026]: seen 6162000 words at 1815.9 wps, loss = 1.982\n",
      "[batch 1030]: seen 6186000 words at 1816.0 wps, loss = 1.980\n",
      "[batch 1034]: seen 6210000 words at 1816.0 wps, loss = 1.978\n",
      "[batch 1038]: seen 6234000 words at 1816.1 wps, loss = 1.976\n",
      "[batch 1042]: seen 6258000 words at 1816.2 wps, loss = 1.974\n",
      "[batch 1046]: seen 6282000 words at 1816.2 wps, loss = 1.973\n",
      "[batch 1050]: seen 6306000 words at 1816.3 wps, loss = 1.971\n",
      "[batch 1054]: seen 6330000 words at 1816.4 wps, loss = 1.969\n",
      "[batch 1058]: seen 6354000 words at 1816.3 wps, loss = 1.967\n",
      "[batch 1062]: seen 6378000 words at 1816.4 wps, loss = 1.966\n",
      "[batch 1066]: seen 6402000 words at 1816.4 wps, loss = 1.964\n",
      "[batch 1070]: seen 6426000 words at 1816.5 wps, loss = 1.962\n",
      "[batch 1074]: seen 6450000 words at 1816.5 wps, loss = 1.960\n",
      "[batch 1078]: seen 6474000 words at 1816.5 wps, loss = 1.959\n",
      "[batch 1082]: seen 6498000 words at 1816.6 wps, loss = 1.957\n",
      "[batch 1086]: seen 6522000 words at 1816.7 wps, loss = 1.955\n",
      "[batch 1090]: seen 6546000 words at 1816.7 wps, loss = 1.953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 1094]: seen 6570000 words at 1816.7 wps, loss = 1.952\n",
      "[batch 1098]: seen 6594000 words at 1816.8 wps, loss = 1.950\n",
      "[batch 1102]: seen 6618000 words at 1816.8 wps, loss = 1.948\n",
      "[batch 1106]: seen 6642000 words at 1816.8 wps, loss = 1.947\n",
      "[batch 1110]: seen 6666000 words at 1816.9 wps, loss = 1.945\n",
      "[batch 1114]: seen 6690000 words at 1816.9 wps, loss = 1.943\n",
      "[batch 1118]: seen 6714000 words at 1817.0 wps, loss = 1.942\n",
      "[batch 1122]: seen 6738000 words at 1817.1 wps, loss = 1.940\n",
      "[batch 1126]: seen 6762000 words at 1817.1 wps, loss = 1.938\n",
      "[batch 1130]: seen 6786000 words at 1817.2 wps, loss = 1.937\n",
      "[batch 1134]: seen 6810000 words at 1817.2 wps, loss = 1.935\n",
      "[batch 1138]: seen 6834000 words at 1817.3 wps, loss = 1.934\n",
      "[batch 1142]: seen 6858000 words at 1817.3 wps, loss = 1.932\n",
      "[batch 1146]: seen 6882000 words at 1817.3 wps, loss = 1.931\n",
      "[batch 1150]: seen 6906000 words at 1817.3 wps, loss = 1.929\n",
      "[batch 1154]: seen 6930000 words at 1817.4 wps, loss = 1.928\n",
      "[batch 1158]: seen 6954000 words at 1817.4 wps, loss = 1.926\n",
      "[batch 1162]: seen 6978000 words at 1817.4 wps, loss = 1.924\n",
      "[batch 1166]: seen 7002000 words at 1817.5 wps, loss = 1.923\n",
      "[batch 1170]: seen 7026000 words at 1817.5 wps, loss = 1.921\n",
      "[batch 1174]: seen 7050000 words at 1817.5 wps, loss = 1.920\n",
      "[batch 1178]: seen 7074000 words at 1817.6 wps, loss = 1.918\n",
      "[batch 1182]: seen 7098000 words at 1817.6 wps, loss = 1.916\n",
      "[batch 1186]: seen 7122000 words at 1817.6 wps, loss = 1.915\n",
      "[batch 1190]: seen 7146000 words at 1817.7 wps, loss = 1.913\n",
      "[batch 1194]: seen 7170000 words at 1817.8 wps, loss = 1.912\n",
      "[batch 1198]: seen 7194000 words at 1817.8 wps, loss = 1.910\n",
      "[batch 1202]: seen 7218000 words at 1817.9 wps, loss = 1.909\n",
      "[batch 1206]: seen 7242000 words at 1818.0 wps, loss = 1.907\n",
      "[batch 1210]: seen 7266000 words at 1818.0 wps, loss = 1.906\n",
      "[batch 1214]: seen 7290000 words at 1818.1 wps, loss = 1.904\n",
      "[batch 1218]: seen 7314000 words at 1818.1 wps, loss = 1.903\n",
      "[batch 1222]: seen 7338000 words at 1818.2 wps, loss = 1.901\n",
      "[batch 1226]: seen 7362000 words at 1818.2 wps, loss = 1.900\n",
      "[batch 1230]: seen 7386000 words at 1818.2 wps, loss = 1.898\n",
      "[batch 1234]: seen 7410000 words at 1818.2 wps, loss = 1.897\n",
      "[batch 1238]: seen 7434000 words at 1818.3 wps, loss = 1.895\n",
      "[batch 1242]: seen 7458000 words at 1818.4 wps, loss = 1.894\n",
      "[batch 1246]: seen 7482000 words at 1818.4 wps, loss = 1.893\n",
      "[batch 1250]: seen 7506000 words at 1818.5 wps, loss = 1.891\n",
      "[batch 1254]: seen 7530000 words at 1818.5 wps, loss = 1.890\n",
      "[batch 1258]: seen 7554000 words at 1818.6 wps, loss = 1.889\n",
      "[batch 1262]: seen 7578000 words at 1818.6 wps, loss = 1.887\n",
      "[batch 1266]: seen 7602000 words at 1818.7 wps, loss = 1.886\n",
      "[batch 1270]: seen 7626000 words at 1818.7 wps, loss = 1.884\n",
      "[batch 1274]: seen 7650000 words at 1818.8 wps, loss = 1.883\n",
      "[batch 1278]: seen 7674000 words at 1818.8 wps, loss = 1.882\n",
      "[batch 1282]: seen 7698000 words at 1818.8 wps, loss = 1.880\n",
      "[batch 1286]: seen 7722000 words at 1818.9 wps, loss = 1.879\n",
      "[batch 1290]: seen 7746000 words at 1818.9 wps, loss = 1.878\n",
      "[batch 1294]: seen 7770000 words at 1819.0 wps, loss = 1.876\n",
      "[batch 1298]: seen 7794000 words at 1819.0 wps, loss = 1.875\n",
      "[batch 1302]: seen 7818000 words at 1819.0 wps, loss = 1.874\n",
      "[batch 1306]: seen 7842000 words at 1819.0 wps, loss = 1.872\n",
      "[batch 1310]: seen 7866000 words at 1819.1 wps, loss = 1.871\n",
      "[batch 1314]: seen 7890000 words at 1819.1 wps, loss = 1.870\n",
      "[batch 1318]: seen 7914000 words at 1819.1 wps, loss = 1.868\n",
      "[batch 1322]: seen 7938000 words at 1819.2 wps, loss = 1.867\n",
      "[batch 1326]: seen 7962000 words at 1819.2 wps, loss = 1.866\n",
      "[batch 1330]: seen 7986000 words at 1819.2 wps, loss = 1.864\n",
      "[batch 1334]: seen 8010000 words at 1819.2 wps, loss = 1.863\n",
      "[batch 1338]: seen 8034000 words at 1819.3 wps, loss = 1.862\n",
      "[batch 1342]: seen 8058000 words at 1819.3 wps, loss = 1.861\n",
      "[batch 1346]: seen 8082000 words at 1819.3 wps, loss = 1.859\n",
      "[batch 1350]: seen 8106000 words at 1819.4 wps, loss = 1.858\n",
      "[batch 1354]: seen 8130000 words at 1819.4 wps, loss = 1.857\n",
      "[batch 1358]: seen 8154000 words at 1819.5 wps, loss = 1.856\n",
      "[batch 1362]: seen 8178000 words at 1819.5 wps, loss = 1.854\n",
      "[batch 1366]: seen 8202000 words at 1819.5 wps, loss = 1.853\n",
      "[batch 1370]: seen 8226000 words at 1819.5 wps, loss = 1.852\n",
      "[batch 1374]: seen 8250000 words at 1819.5 wps, loss = 1.851\n",
      "[batch 1378]: seen 8274000 words at 1819.6 wps, loss = 1.849\n",
      "[batch 1382]: seen 8298000 words at 1819.6 wps, loss = 1.848\n",
      "[batch 1386]: seen 8322000 words at 1819.6 wps, loss = 1.847\n",
      "[batch 1390]: seen 8346000 words at 1819.6 wps, loss = 1.846\n",
      "[batch 1394]: seen 8370000 words at 1819.6 wps, loss = 1.844\n",
      "[batch 1398]: seen 8394000 words at 1819.6 wps, loss = 1.843\n",
      "[batch 1402]: seen 8418000 words at 1819.7 wps, loss = 1.842\n",
      "[batch 1406]: seen 8442000 words at 1819.7 wps, loss = 1.841\n",
      "[batch 1410]: seen 8466000 words at 1819.8 wps, loss = 1.840\n",
      "[batch 1414]: seen 8490000 words at 1819.8 wps, loss = 1.838\n",
      "[batch 1418]: seen 8514000 words at 1819.9 wps, loss = 1.837\n",
      "[batch 1422]: seen 8538000 words at 1819.9 wps, loss = 1.836\n",
      "[batch 1426]: seen 8562000 words at 1820.0 wps, loss = 1.835\n",
      "[batch 1430]: seen 8586000 words at 1820.0 wps, loss = 1.834\n",
      "[batch 1434]: seen 8610000 words at 1820.1 wps, loss = 1.833\n",
      "[batch 1438]: seen 8634000 words at 1820.1 wps, loss = 1.831\n",
      "[batch 1442]: seen 8658000 words at 1820.1 wps, loss = 1.830\n",
      "[batch 1446]: seen 8682000 words at 1820.2 wps, loss = 1.829\n",
      "[batch 1450]: seen 8706000 words at 1820.2 wps, loss = 1.828\n",
      "[batch 1454]: seen 8730000 words at 1820.3 wps, loss = 1.827\n",
      "[batch 1458]: seen 8754000 words at 1820.3 wps, loss = 1.826\n",
      "[batch 1462]: seen 8778000 words at 1820.3 wps, loss = 1.825\n",
      "[batch 1466]: seen 8802000 words at 1820.4 wps, loss = 1.823\n",
      "[batch 1470]: seen 8826000 words at 1820.4 wps, loss = 1.822\n",
      "[batch 1474]: seen 8850000 words at 1820.5 wps, loss = 1.821\n",
      "[batch 1478]: seen 8874000 words at 1820.5 wps, loss = 1.820\n",
      "[batch 1482]: seen 8898000 words at 1820.6 wps, loss = 1.819\n",
      "[batch 1486]: seen 8922000 words at 1820.6 wps, loss = 1.818\n",
      "[batch 1490]: seen 8946000 words at 1820.6 wps, loss = 1.817\n",
      "[batch 1494]: seen 8970000 words at 1820.7 wps, loss = 1.816\n",
      "[batch 1498]: seen 8994000 words at 1820.7 wps, loss = 1.815\n",
      "[batch 1502]: seen 9018000 words at 1820.8 wps, loss = 1.813\n",
      "[batch 1506]: seen 9042000 words at 1820.8 wps, loss = 1.812\n",
      "[batch 1510]: seen 9066000 words at 1820.9 wps, loss = 1.811\n",
      "[batch 1514]: seen 9090000 words at 1820.9 wps, loss = 1.810\n",
      "[batch 1518]: seen 9114000 words at 1820.9 wps, loss = 1.809\n",
      "[batch 1522]: seen 9138000 words at 1821.0 wps, loss = 1.808\n",
      "[batch 1526]: seen 9162000 words at 1821.0 wps, loss = 1.807\n",
      "[batch 1530]: seen 9186000 words at 1821.1 wps, loss = 1.806\n",
      "[batch 1534]: seen 9210000 words at 1821.1 wps, loss = 1.805\n",
      "[batch 1538]: seen 9234000 words at 1821.1 wps, loss = 1.804\n",
      "[batch 1542]: seen 9258000 words at 1821.1 wps, loss = 1.803\n",
      "[batch 1546]: seen 9282000 words at 1821.1 wps, loss = 1.802\n",
      "[batch 1550]: seen 9306000 words at 1821.2 wps, loss = 1.801\n",
      "[batch 1554]: seen 9330000 words at 1821.2 wps, loss = 1.800\n",
      "[batch 1558]: seen 9354000 words at 1821.3 wps, loss = 1.799\n",
      "[batch 1562]: seen 9378000 words at 1821.3 wps, loss = 1.798\n",
      "[batch 1566]: seen 9402000 words at 1821.4 wps, loss = 1.797\n",
      "[batch 1570]: seen 9426000 words at 1821.4 wps, loss = 1.796\n",
      "[batch 1574]: seen 9450000 words at 1821.4 wps, loss = 1.795\n",
      "[batch 1578]: seen 9474000 words at 1821.5 wps, loss = 1.794\n",
      "[batch 1582]: seen 9498000 words at 1821.5 wps, loss = 1.793\n",
      "[batch 1586]: seen 9522000 words at 1821.6 wps, loss = 1.792\n",
      "[batch 1590]: seen 9546000 words at 1821.6 wps, loss = 1.791\n",
      "[batch 1594]: seen 9570000 words at 1821.7 wps, loss = 1.790\n",
      "[batch 1598]: seen 9594000 words at 1821.7 wps, loss = 1.789\n",
      "[batch 1602]: seen 9618000 words at 1821.7 wps, loss = 1.788\n",
      "[batch 1606]: seen 9642000 words at 1821.8 wps, loss = 1.787\n",
      "[batch 1610]: seen 9666000 words at 1821.8 wps, loss = 1.786\n",
      "[batch 1614]: seen 9690000 words at 1821.9 wps, loss = 1.785\n",
      "[batch 1618]: seen 9714000 words at 1821.9 wps, loss = 1.784\n",
      "[batch 1622]: seen 9738000 words at 1821.9 wps, loss = 1.783\n",
      "[batch 1626]: seen 9762000 words at 1821.9 wps, loss = 1.782\n",
      "[batch 1630]: seen 9786000 words at 1822.0 wps, loss = 1.781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 1634]: seen 9810000 words at 1822.0 wps, loss = 1.780\n",
      "[batch 1638]: seen 9834000 words at 1822.0 wps, loss = 1.779\n",
      "[batch 1642]: seen 9858000 words at 1822.1 wps, loss = 1.778\n",
      "[batch 1646]: seen 9882000 words at 1822.1 wps, loss = 1.777\n",
      "[batch 1650]: seen 9906000 words at 1822.1 wps, loss = 1.776\n",
      "[batch 1654]: seen 9930000 words at 1822.2 wps, loss = 1.775\n",
      "[batch 1658]: seen 9954000 words at 1822.2 wps, loss = 1.774\n",
      "[batch 1662]: seen 9978000 words at 1822.2 wps, loss = 1.773\n",
      "[batch 1666]: seen 10002000 words at 1822.2 wps, loss = 1.772\n",
      "[batch 1670]: seen 10026000 words at 1822.2 wps, loss = 1.771\n",
      "[batch 1674]: seen 10050000 words at 1822.2 wps, loss = 1.770\n",
      "[batch 1678]: seen 10074000 words at 1822.3 wps, loss = 1.769\n",
      "[batch 1682]: seen 10098000 words at 1822.3 wps, loss = 1.768\n",
      "[batch 1686]: seen 10122000 words at 1822.3 wps, loss = 1.767\n",
      "[batch 1690]: seen 10146000 words at 1822.3 wps, loss = 1.766\n",
      "[batch 1694]: seen 10170000 words at 1822.3 wps, loss = 1.765\n",
      "[batch 1698]: seen 10194000 words at 1822.4 wps, loss = 1.764\n",
      "[batch 1702]: seen 10218000 words at 1822.4 wps, loss = 1.763\n",
      "[batch 1706]: seen 10242000 words at 1822.4 wps, loss = 1.763\n",
      "[batch 1710]: seen 10266000 words at 1822.5 wps, loss = 1.762\n",
      "[batch 1714]: seen 10290000 words at 1822.5 wps, loss = 1.761\n",
      "[batch 1718]: seen 10314000 words at 1822.5 wps, loss = 1.760\n",
      "[batch 1722]: seen 10338000 words at 1822.6 wps, loss = 1.759\n",
      "[batch 1726]: seen 10362000 words at 1822.6 wps, loss = 1.758\n",
      "[batch 1730]: seen 10386000 words at 1822.6 wps, loss = 1.757\n",
      "[batch 1734]: seen 10410000 words at 1822.6 wps, loss = 1.756\n",
      "[batch 1738]: seen 10434000 words at 1822.6 wps, loss = 1.755\n",
      "[batch 1742]: seen 10458000 words at 1822.7 wps, loss = 1.755\n",
      "[batch 1746]: seen 10482000 words at 1822.7 wps, loss = 1.754\n",
      "[batch 1750]: seen 10506000 words at 1822.8 wps, loss = 1.753\n",
      "[batch 1754]: seen 10530000 words at 1822.8 wps, loss = 1.752\n",
      "[batch 1758]: seen 10554000 words at 1822.8 wps, loss = 1.751\n",
      "[batch 1762]: seen 10578000 words at 1822.8 wps, loss = 1.750\n",
      "[batch 1766]: seen 10602000 words at 1822.9 wps, loss = 1.749\n",
      "[batch 1770]: seen 10626000 words at 1822.9 wps, loss = 1.749\n",
      "[batch 1774]: seen 10650000 words at 1822.9 wps, loss = 1.748\n",
      "[batch 1778]: seen 10674000 words at 1822.9 wps, loss = 1.747\n",
      "[batch 1782]: seen 10698000 words at 1822.9 wps, loss = 1.746\n",
      "[batch 1786]: seen 10722000 words at 1822.9 wps, loss = 1.745\n",
      "[batch 1790]: seen 10746000 words at 1822.9 wps, loss = 1.744\n",
      "[batch 1794]: seen 10770000 words at 1822.9 wps, loss = 1.743\n",
      "[batch 1798]: seen 10794000 words at 1823.0 wps, loss = 1.743\n",
      "[batch 1802]: seen 10818000 words at 1823.0 wps, loss = 1.742\n",
      "[batch 1806]: seen 10842000 words at 1823.0 wps, loss = 1.741\n",
      "[batch 1810]: seen 10866000 words at 1823.0 wps, loss = 1.740\n",
      "[batch 1814]: seen 10890000 words at 1823.0 wps, loss = 1.740\n",
      "[batch 1818]: seen 10914000 words at 1823.1 wps, loss = 1.739\n",
      "[batch 1822]: seen 10938000 words at 1823.1 wps, loss = 1.738\n",
      "[batch 1826]: seen 10962000 words at 1823.1 wps, loss = 1.737\n",
      "[batch 1830]: seen 10986000 words at 1823.1 wps, loss = 1.737\n",
      "[batch 1834]: seen 11010000 words at 1823.1 wps, loss = 1.736\n",
      "[batch 1838]: seen 11034000 words at 1823.1 wps, loss = 1.735\n",
      "[batch 1842]: seen 11058000 words at 1823.2 wps, loss = 1.734\n",
      "[batch 1846]: seen 11082000 words at 1823.2 wps, loss = 1.734\n",
      "[batch 1850]: seen 11106000 words at 1823.2 wps, loss = 1.733\n",
      "[batch 1854]: seen 11130000 words at 1823.3 wps, loss = 1.732\n",
      "[batch 1858]: seen 11154000 words at 1823.3 wps, loss = 1.731\n",
      "[batch 1862]: seen 11178000 words at 1823.3 wps, loss = 1.731\n",
      "[batch 1866]: seen 11202000 words at 1823.3 wps, loss = 1.730\n",
      "[batch 1870]: seen 11226000 words at 1823.3 wps, loss = 1.729\n",
      "[batch 1874]: seen 11250000 words at 1823.4 wps, loss = 1.728\n",
      "[batch 1878]: seen 11274000 words at 1823.4 wps, loss = 1.727\n",
      "[batch 1882]: seen 11298000 words at 1823.4 wps, loss = 1.727\n",
      "[batch 1886]: seen 11322000 words at 1823.4 wps, loss = 1.726\n",
      "[batch 1890]: seen 11346000 words at 1823.4 wps, loss = 1.725\n",
      "[batch 1894]: seen 11370000 words at 1823.5 wps, loss = 1.724\n",
      "[batch 1898]: seen 11394000 words at 1823.5 wps, loss = 1.724\n",
      "[batch 1902]: seen 11418000 words at 1823.5 wps, loss = 1.723\n",
      "[batch 1906]: seen 11442000 words at 1823.5 wps, loss = 1.722\n",
      "[batch 1910]: seen 11466000 words at 1823.5 wps, loss = 1.722\n",
      "[batch 1914]: seen 11490000 words at 1823.5 wps, loss = 1.721\n",
      "[batch 1918]: seen 11514000 words at 1823.5 wps, loss = 1.720\n",
      "[batch 1922]: seen 11538000 words at 1823.6 wps, loss = 1.719\n",
      "[batch 1926]: seen 11562000 words at 1823.6 wps, loss = 1.719\n",
      "[batch 1930]: seen 11586000 words at 1823.6 wps, loss = 1.718\n",
      "[batch 1934]: seen 11610000 words at 1823.6 wps, loss = 1.717\n",
      "[batch 1938]: seen 11634000 words at 1823.6 wps, loss = 1.717\n",
      "[batch 1942]: seen 11658000 words at 1823.6 wps, loss = 1.716\n",
      "[batch 1946]: seen 11682000 words at 1823.7 wps, loss = 1.715\n",
      "[batch 1950]: seen 11706000 words at 1823.7 wps, loss = 1.714\n",
      "[batch 1954]: seen 11730000 words at 1823.7 wps, loss = 1.714\n",
      "[batch 1958]: seen 11754000 words at 1823.7 wps, loss = 1.713\n",
      "[batch 1962]: seen 11778000 words at 1823.7 wps, loss = 1.712\n",
      "[batch 1966]: seen 11802000 words at 1823.7 wps, loss = 1.712\n",
      "[batch 1970]: seen 11826000 words at 1823.8 wps, loss = 1.711\n",
      "[batch 1974]: seen 11850000 words at 1823.8 wps, loss = 1.710\n",
      "[batch 1978]: seen 11874000 words at 1823.8 wps, loss = 1.709\n",
      "[batch 1982]: seen 11898000 words at 1823.8 wps, loss = 1.709\n",
      "[batch 1986]: seen 11922000 words at 1823.8 wps, loss = 1.708\n",
      "[batch 1990]: seen 11946000 words at 1823.9 wps, loss = 1.707\n",
      "[batch 1994]: seen 11970000 words at 1823.8 wps, loss = 1.707\n",
      "[batch 1998]: seen 11994000 words at 1823.9 wps, loss = 1.706\n",
      "[batch 2002]: seen 12018000 words at 1823.9 wps, loss = 1.705\n",
      "[batch 2006]: seen 12042000 words at 1823.9 wps, loss = 1.705\n",
      "[batch 2010]: seen 12066000 words at 1823.9 wps, loss = 1.704\n",
      "[batch 2014]: seen 12090000 words at 1823.9 wps, loss = 1.703\n",
      "[batch 2018]: seen 12114000 words at 1823.9 wps, loss = 1.703\n",
      "[batch 2022]: seen 12138000 words at 1824.0 wps, loss = 1.702\n",
      "[batch 2026]: seen 12162000 words at 1824.0 wps, loss = 1.701\n",
      "[batch 2030]: seen 12186000 words at 1824.0 wps, loss = 1.701\n",
      "[batch 2034]: seen 12210000 words at 1824.0 wps, loss = 1.700\n",
      "[batch 2038]: seen 12234000 words at 1824.0 wps, loss = 1.700\n",
      "[batch 2042]: seen 12258000 words at 1824.0 wps, loss = 1.699\n",
      "[batch 2046]: seen 12282000 words at 1824.1 wps, loss = 1.698\n",
      "[batch 2050]: seen 12306000 words at 1824.1 wps, loss = 1.698\n",
      "[batch 2054]: seen 12330000 words at 1824.1 wps, loss = 1.697\n",
      "[batch 2058]: seen 12354000 words at 1824.1 wps, loss = 1.697\n",
      "[batch 2062]: seen 12378000 words at 1824.1 wps, loss = 1.696\n",
      "[batch 2066]: seen 12402000 words at 1824.1 wps, loss = 1.695\n",
      "[batch 2070]: seen 12426000 words at 1824.1 wps, loss = 1.695\n",
      "[batch 2074]: seen 12450000 words at 1824.1 wps, loss = 1.694\n",
      "[batch 2078]: seen 12474000 words at 1824.1 wps, loss = 1.694\n",
      "[batch 2082]: seen 12498000 words at 1824.1 wps, loss = 1.693\n",
      "[batch 2086]: seen 12522000 words at 1824.1 wps, loss = 1.693\n",
      "[batch 2090]: seen 12546000 words at 1824.1 wps, loss = 1.692\n",
      "[batch 2094]: seen 12570000 words at 1824.2 wps, loss = 1.691\n",
      "[batch 2098]: seen 12594000 words at 1824.2 wps, loss = 1.691\n",
      "[batch 2102]: seen 12618000 words at 1824.2 wps, loss = 1.690\n",
      "[batch 2106]: seen 12642000 words at 1824.2 wps, loss = 1.690\n",
      "[batch 2110]: seen 12666000 words at 1824.2 wps, loss = 1.689\n",
      "[batch 2114]: seen 12690000 words at 1824.2 wps, loss = 1.688\n",
      "[batch 2118]: seen 12714000 words at 1824.3 wps, loss = 1.688\n",
      "[batch 2122]: seen 12738000 words at 1824.3 wps, loss = 1.687\n",
      "[batch 2126]: seen 12762000 words at 1824.3 wps, loss = 1.687\n",
      "[batch 2130]: seen 12786000 words at 1824.3 wps, loss = 1.686\n",
      "[batch 2134]: seen 12810000 words at 1824.3 wps, loss = 1.685\n",
      "[batch 2138]: seen 12834000 words at 1824.4 wps, loss = 1.685\n",
      "[batch 2142]: seen 12858000 words at 1824.4 wps, loss = 1.684\n",
      "[batch 2146]: seen 12882000 words at 1824.4 wps, loss = 1.684\n",
      "[batch 2150]: seen 12906000 words at 1824.5 wps, loss = 1.683\n",
      "[batch 2154]: seen 12930000 words at 1824.5 wps, loss = 1.683\n",
      "[batch 2158]: seen 12954000 words at 1824.5 wps, loss = 1.682\n",
      "[batch 2162]: seen 12978000 words at 1824.5 wps, loss = 1.681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 2166]: seen 13002000 words at 1824.6 wps, loss = 1.681\n",
      "[batch 2170]: seen 13026000 words at 1824.6 wps, loss = 1.680\n",
      "[batch 2174]: seen 13050000 words at 1824.6 wps, loss = 1.680\n",
      "[batch 2178]: seen 13074000 words at 1824.6 wps, loss = 1.679\n",
      "[batch 2182]: seen 13098000 words at 1824.6 wps, loss = 1.679\n",
      "[batch 2186]: seen 13122000 words at 1824.7 wps, loss = 1.678\n",
      "[batch 2190]: seen 13146000 words at 1824.7 wps, loss = 1.677\n",
      "[batch 2194]: seen 13170000 words at 1824.7 wps, loss = 1.677\n",
      "[batch 2198]: seen 13194000 words at 1824.7 wps, loss = 1.676\n",
      "[batch 2202]: seen 13218000 words at 1824.7 wps, loss = 1.676\n",
      "[batch 2206]: seen 13242000 words at 1824.7 wps, loss = 1.675\n",
      "[batch 2210]: seen 13266000 words at 1824.8 wps, loss = 1.675\n",
      "[batch 2214]: seen 13290000 words at 1824.8 wps, loss = 1.674\n",
      "[batch 2218]: seen 13314000 words at 1824.8 wps, loss = 1.673\n",
      "[batch 2222]: seen 13338000 words at 1824.8 wps, loss = 1.673\n",
      "[batch 2226]: seen 13362000 words at 1824.9 wps, loss = 1.672\n",
      "[batch 2230]: seen 13386000 words at 1824.9 wps, loss = 1.672\n",
      "[batch 2234]: seen 13410000 words at 1824.9 wps, loss = 1.671\n",
      "[batch 2238]: seen 13434000 words at 1824.9 wps, loss = 1.671\n",
      "[batch 2242]: seen 13458000 words at 1824.9 wps, loss = 1.670\n",
      "[batch 2246]: seen 13482000 words at 1825.0 wps, loss = 1.669\n",
      "[batch 2250]: seen 13506000 words at 1825.0 wps, loss = 1.669\n",
      "[batch 2254]: seen 13530000 words at 1825.0 wps, loss = 1.668\n",
      "[batch 2258]: seen 13554000 words at 1825.0 wps, loss = 1.668\n",
      "[batch 2262]: seen 13578000 words at 1825.1 wps, loss = 1.667\n",
      "[batch 2266]: seen 13602000 words at 1825.1 wps, loss = 1.667\n",
      "[batch 2270]: seen 13626000 words at 1825.1 wps, loss = 1.666\n",
      "[batch 2274]: seen 13650000 words at 1825.1 wps, loss = 1.665\n",
      "[batch 2278]: seen 13674000 words at 1825.1 wps, loss = 1.665\n",
      "[batch 2282]: seen 13698000 words at 1825.2 wps, loss = 1.664\n",
      "[batch 2286]: seen 13722000 words at 1825.2 wps, loss = 1.664\n",
      "[batch 2290]: seen 13746000 words at 1825.2 wps, loss = 1.663\n",
      "[batch 2294]: seen 13770000 words at 1825.2 wps, loss = 1.663\n",
      "[batch 2298]: seen 13794000 words at 1825.2 wps, loss = 1.662\n",
      "[batch 2302]: seen 13818000 words at 1825.2 wps, loss = 1.662\n",
      "[batch 2306]: seen 13842000 words at 1825.2 wps, loss = 1.661\n",
      "[batch 2310]: seen 13866000 words at 1825.2 wps, loss = 1.660\n",
      "[batch 2314]: seen 13890000 words at 1825.2 wps, loss = 1.660\n",
      "[batch 2318]: seen 13914000 words at 1825.3 wps, loss = 1.659\n",
      "[batch 2322]: seen 13938000 words at 1825.3 wps, loss = 1.659\n",
      "[batch 2326]: seen 13962000 words at 1825.3 wps, loss = 1.658\n",
      "[batch 2330]: seen 13986000 words at 1825.3 wps, loss = 1.658\n",
      "[batch 2334]: seen 14010000 words at 1825.3 wps, loss = 1.657\n",
      "[batch 2338]: seen 14034000 words at 1825.4 wps, loss = 1.657\n",
      "[batch 2342]: seen 14058000 words at 1825.4 wps, loss = 1.656\n",
      "[batch 2346]: seen 14082000 words at 1825.4 wps, loss = 1.656\n",
      "[batch 2350]: seen 14106000 words at 1825.4 wps, loss = 1.655\n",
      "[batch 2354]: seen 14130000 words at 1825.4 wps, loss = 1.655\n",
      "[batch 2358]: seen 14154000 words at 1825.4 wps, loss = 1.654\n",
      "[batch 2362]: seen 14178000 words at 1825.5 wps, loss = 1.654\n",
      "[batch 2366]: seen 14202000 words at 1825.5 wps, loss = 1.653\n",
      "[batch 2370]: seen 14226000 words at 1825.5 wps, loss = 1.653\n",
      "[batch 2374]: seen 14250000 words at 1825.5 wps, loss = 1.652\n",
      "[batch 2378]: seen 14274000 words at 1825.5 wps, loss = 1.652\n",
      "[batch 2382]: seen 14298000 words at 1825.5 wps, loss = 1.652\n",
      "[batch 2386]: seen 14322000 words at 1825.6 wps, loss = 1.651\n",
      "[batch 2390]: seen 14346000 words at 1825.6 wps, loss = 1.651\n",
      "[batch 2394]: seen 14370000 words at 1825.6 wps, loss = 1.650\n",
      "[batch 2398]: seen 14394000 words at 1825.6 wps, loss = 1.650\n",
      "[batch 2402]: seen 14418000 words at 1825.6 wps, loss = 1.649\n",
      "[batch 2406]: seen 14442000 words at 1825.6 wps, loss = 1.649\n",
      "[batch 2410]: seen 14466000 words at 1825.7 wps, loss = 1.648\n",
      "[batch 2414]: seen 14490000 words at 1825.7 wps, loss = 1.648\n",
      "[batch 2418]: seen 14514000 words at 1825.7 wps, loss = 1.647\n",
      "[batch 2422]: seen 14538000 words at 1825.7 wps, loss = 1.647\n",
      "[batch 2426]: seen 14562000 words at 1825.7 wps, loss = 1.646\n",
      "[batch 2430]: seen 14586000 words at 1825.7 wps, loss = 1.646\n",
      "[batch 2434]: seen 14610000 words at 1825.7 wps, loss = 1.645\n",
      "[batch 2438]: seen 14634000 words at 1825.7 wps, loss = 1.645\n",
      "[batch 2442]: seen 14658000 words at 1825.8 wps, loss = 1.644\n",
      "[batch 2446]: seen 14682000 words at 1825.8 wps, loss = 1.644\n",
      "[batch 2450]: seen 14706000 words at 1825.8 wps, loss = 1.643\n",
      "[batch 2454]: seen 14730000 words at 1825.8 wps, loss = 1.643\n",
      "[batch 2458]: seen 14754000 words at 1825.7 wps, loss = 1.642\n",
      "[batch 2462]: seen 14778000 words at 1825.7 wps, loss = 1.642\n",
      "[batch 2466]: seen 14802000 words at 1825.8 wps, loss = 1.641\n",
      "[batch 2470]: seen 14826000 words at 1825.8 wps, loss = 1.641\n",
      "[batch 2474]: seen 14850000 words at 1825.8 wps, loss = 1.640\n",
      "[batch 2478]: seen 14874000 words at 1825.8 wps, loss = 1.640\n",
      "[batch 2482]: seen 14898000 words at 1825.8 wps, loss = 1.640\n",
      "[batch 2486]: seen 14922000 words at 1825.8 wps, loss = 1.639\n",
      "[batch 2490]: seen 14946000 words at 1825.8 wps, loss = 1.639\n",
      "[batch 2494]: seen 14970000 words at 1825.8 wps, loss = 1.638\n",
      "[batch 2498]: seen 14994000 words at 1825.8 wps, loss = 1.638\n",
      "[batch 2502]: seen 15018000 words at 1825.9 wps, loss = 1.637\n",
      "[batch 2506]: seen 15042000 words at 1825.9 wps, loss = 1.637\n",
      "[batch 2510]: seen 15066000 words at 1825.9 wps, loss = 1.636\n",
      "[batch 2514]: seen 15090000 words at 1825.9 wps, loss = 1.636\n",
      "[batch 2518]: seen 15114000 words at 1825.9 wps, loss = 1.635\n",
      "[batch 2522]: seen 15138000 words at 1826.0 wps, loss = 1.635\n",
      "[batch 2526]: seen 15162000 words at 1826.0 wps, loss = 1.634\n",
      "[batch 2530]: seen 15186000 words at 1826.0 wps, loss = 1.634\n",
      "[batch 2534]: seen 15210000 words at 1826.0 wps, loss = 1.633\n",
      "[batch 2538]: seen 15234000 words at 1826.0 wps, loss = 1.633\n",
      "[batch 2542]: seen 15258000 words at 1826.0 wps, loss = 1.632\n",
      "[batch 2546]: seen 15282000 words at 1826.0 wps, loss = 1.632\n",
      "[batch 2550]: seen 15306000 words at 1826.1 wps, loss = 1.632\n",
      "[batch 2554]: seen 15330000 words at 1826.1 wps, loss = 1.631\n",
      "[batch 2558]: seen 15354000 words at 1826.1 wps, loss = 1.631\n",
      "[batch 2562]: seen 15378000 words at 1826.1 wps, loss = 1.630\n",
      "[batch 2566]: seen 15402000 words at 1826.1 wps, loss = 1.630\n",
      "[batch 2570]: seen 15426000 words at 1826.2 wps, loss = 1.629\n",
      "[batch 2574]: seen 15450000 words at 1826.2 wps, loss = 1.629\n",
      "[batch 2578]: seen 15474000 words at 1826.2 wps, loss = 1.628\n",
      "[batch 2582]: seen 15498000 words at 1826.2 wps, loss = 1.628\n",
      "[batch 2586]: seen 15522000 words at 1826.2 wps, loss = 1.627\n",
      "[batch 2590]: seen 15546000 words at 1826.2 wps, loss = 1.627\n",
      "[batch 2594]: seen 15570000 words at 1826.3 wps, loss = 1.626\n",
      "[batch 2598]: seen 15594000 words at 1826.3 wps, loss = 1.626\n",
      "[batch 2602]: seen 15618000 words at 1826.3 wps, loss = 1.625\n",
      "[batch 2606]: seen 15642000 words at 1826.3 wps, loss = 1.625\n",
      "[batch 2610]: seen 15666000 words at 1826.3 wps, loss = 1.625\n",
      "[batch 2614]: seen 15690000 words at 1826.4 wps, loss = 1.624\n",
      "[batch 2618]: seen 15714000 words at 1826.4 wps, loss = 1.624\n",
      "[batch 2622]: seen 15738000 words at 1826.4 wps, loss = 1.623\n",
      "[batch 2626]: seen 15762000 words at 1826.4 wps, loss = 1.623\n",
      "[batch 2630]: seen 15786000 words at 1826.4 wps, loss = 1.622\n",
      "[batch 2634]: seen 15810000 words at 1826.4 wps, loss = 1.622\n",
      "[batch 2638]: seen 15834000 words at 1826.4 wps, loss = 1.621\n",
      "[batch 2642]: seen 15858000 words at 1826.4 wps, loss = 1.621\n",
      "[batch 2646]: seen 15882000 words at 1826.4 wps, loss = 1.621\n",
      "[batch 2650]: seen 15906000 words at 1826.5 wps, loss = 1.620\n",
      "[batch 2654]: seen 15930000 words at 1826.5 wps, loss = 1.620\n",
      "[batch 2658]: seen 15954000 words at 1826.5 wps, loss = 1.619\n",
      "[batch 2662]: seen 15978000 words at 1826.5 wps, loss = 1.619\n",
      "[batch 2666]: seen 16002000 words at 1826.5 wps, loss = 1.619\n",
      "[batch 2670]: seen 16026000 words at 1826.6 wps, loss = 1.618\n",
      "[batch 2674]: seen 16050000 words at 1826.6 wps, loss = 1.618\n",
      "[batch 2678]: seen 16074000 words at 1826.6 wps, loss = 1.617\n",
      "[batch 2682]: seen 16098000 words at 1826.6 wps, loss = 1.617\n",
      "[batch 2686]: seen 16122000 words at 1826.6 wps, loss = 1.616\n",
      "[batch 2690]: seen 16146000 words at 1826.6 wps, loss = 1.616\n",
      "[batch 2694]: seen 16170000 words at 1826.7 wps, loss = 1.616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 2698]: seen 16194000 words at 1826.7 wps, loss = 1.615\n",
      "[batch 2702]: seen 16218000 words at 1826.7 wps, loss = 1.615\n",
      "[batch 2706]: seen 16242000 words at 1826.7 wps, loss = 1.614\n",
      "[batch 2710]: seen 16266000 words at 1826.8 wps, loss = 1.614\n",
      "[batch 2714]: seen 16290000 words at 1826.8 wps, loss = 1.614\n",
      "[batch 2718]: seen 16314000 words at 1826.8 wps, loss = 1.613\n",
      "[batch 2722]: seen 16338000 words at 1826.8 wps, loss = 1.613\n",
      "[batch 2726]: seen 16362000 words at 1826.8 wps, loss = 1.612\n",
      "[batch 2730]: seen 16386000 words at 1826.9 wps, loss = 1.612\n",
      "[batch 2734]: seen 16410000 words at 1826.9 wps, loss = 1.612\n",
      "[batch 2738]: seen 16434000 words at 1826.9 wps, loss = 1.611\n",
      "[batch 2742]: seen 16458000 words at 1826.9 wps, loss = 1.611\n",
      "[batch 2746]: seen 16482000 words at 1826.9 wps, loss = 1.611\n",
      "[batch 2750]: seen 16506000 words at 1826.9 wps, loss = 1.610\n",
      "[batch 2754]: seen 16530000 words at 1826.9 wps, loss = 1.610\n",
      "[batch 2758]: seen 16554000 words at 1826.9 wps, loss = 1.609\n",
      "[batch 2762]: seen 16578000 words at 1827.0 wps, loss = 1.609\n",
      "[batch 2766]: seen 16602000 words at 1827.0 wps, loss = 1.609\n",
      "[batch 2770]: seen 16626000 words at 1827.0 wps, loss = 1.608\n",
      "[batch 2774]: seen 16650000 words at 1827.0 wps, loss = 1.608\n",
      "[batch 2778]: seen 16674000 words at 1827.0 wps, loss = 1.607\n",
      "[batch 2782]: seen 16698000 words at 1827.0 wps, loss = 1.607\n",
      "[batch 2786]: seen 16722000 words at 1827.1 wps, loss = 1.607\n",
      "[batch 2790]: seen 16746000 words at 1827.1 wps, loss = 1.606\n",
      "[batch 2794]: seen 16770000 words at 1827.1 wps, loss = 1.606\n",
      "[batch 2798]: seen 16794000 words at 1827.1 wps, loss = 1.605\n",
      "[batch 2802]: seen 16818000 words at 1827.1 wps, loss = 1.605\n",
      "[batch 2806]: seen 16842000 words at 1827.1 wps, loss = 1.605\n",
      "[batch 2810]: seen 16866000 words at 1827.1 wps, loss = 1.604\n",
      "[batch 2814]: seen 16890000 words at 1827.1 wps, loss = 1.604\n",
      "[batch 2818]: seen 16914000 words at 1827.1 wps, loss = 1.604\n",
      "[batch 2822]: seen 16938000 words at 1827.1 wps, loss = 1.603\n",
      "[batch 2826]: seen 16962000 words at 1827.2 wps, loss = 1.603\n",
      "[batch 2830]: seen 16986000 words at 1827.2 wps, loss = 1.602\n",
      "[batch 2834]: seen 17010000 words at 1827.2 wps, loss = 1.602\n",
      "[batch 2838]: seen 17034000 words at 1827.2 wps, loss = 1.602\n",
      "[batch 2842]: seen 17058000 words at 1827.2 wps, loss = 1.601\n",
      "[batch 2846]: seen 17082000 words at 1827.2 wps, loss = 1.601\n",
      "[batch 2850]: seen 17106000 words at 1827.2 wps, loss = 1.601\n",
      "[batch 2854]: seen 17130000 words at 1827.2 wps, loss = 1.600\n",
      "[batch 2858]: seen 17154000 words at 1827.2 wps, loss = 1.600\n",
      "[batch 2862]: seen 17178000 words at 1827.3 wps, loss = 1.599\n",
      "[batch 2866]: seen 17202000 words at 1827.3 wps, loss = 1.599\n",
      "[batch 2870]: seen 17226000 words at 1827.3 wps, loss = 1.599\n",
      "[batch 2874]: seen 17250000 words at 1827.3 wps, loss = 1.598\n",
      "[batch 2878]: seen 17274000 words at 1827.3 wps, loss = 1.598\n",
      "[batch 2882]: seen 17298000 words at 1827.3 wps, loss = 1.598\n",
      "[batch 2886]: seen 17322000 words at 1827.4 wps, loss = 1.597\n",
      "[batch 2890]: seen 17346000 words at 1827.4 wps, loss = 1.597\n",
      "[batch 2894]: seen 17370000 words at 1827.4 wps, loss = 1.596\n",
      "[batch 2898]: seen 17394000 words at 1827.4 wps, loss = 1.596\n",
      "[batch 2902]: seen 17418000 words at 1827.4 wps, loss = 1.596\n",
      "[batch 2906]: seen 17442000 words at 1827.4 wps, loss = 1.595\n",
      "[batch 2910]: seen 17466000 words at 1827.4 wps, loss = 1.595\n",
      "[batch 2914]: seen 17490000 words at 1827.5 wps, loss = 1.594\n",
      "[batch 2918]: seen 17514000 words at 1827.5 wps, loss = 1.594\n",
      "[batch 2922]: seen 17538000 words at 1827.5 wps, loss = 1.594\n",
      "[batch 2926]: seen 17562000 words at 1827.5 wps, loss = 1.593\n",
      "[batch 2930]: seen 17586000 words at 1827.5 wps, loss = 1.593\n",
      "[batch 2934]: seen 17610000 words at 1827.5 wps, loss = 1.593\n",
      "[batch 2938]: seen 17634000 words at 1827.5 wps, loss = 1.592\n",
      "[batch 2942]: seen 17658000 words at 1827.5 wps, loss = 1.592\n",
      "[batch 2946]: seen 17682000 words at 1827.5 wps, loss = 1.591\n",
      "[batch 2950]: seen 17706000 words at 1827.6 wps, loss = 1.591\n",
      "[batch 2954]: seen 17730000 words at 1827.6 wps, loss = 1.591\n",
      "[batch 2958]: seen 17754000 words at 1827.6 wps, loss = 1.590\n",
      "[batch 2962]: seen 17778000 words at 1827.6 wps, loss = 1.590\n",
      "[batch 2966]: seen 17802000 words at 1827.6 wps, loss = 1.590\n",
      "[batch 2970]: seen 17826000 words at 1827.6 wps, loss = 1.589\n",
      "[batch 2974]: seen 17850000 words at 1827.6 wps, loss = 1.589\n",
      "[batch 2978]: seen 17874000 words at 1827.6 wps, loss = 1.589\n",
      "[batch 2982]: seen 17898000 words at 1827.6 wps, loss = 1.588\n",
      "[batch 2986]: seen 17922000 words at 1827.6 wps, loss = 1.588\n",
      "[batch 2990]: seen 17946000 words at 1827.6 wps, loss = 1.588\n",
      "[batch 2994]: seen 17970000 words at 1827.7 wps, loss = 1.587\n",
      "[batch 2998]: seen 17994000 words at 1827.7 wps, loss = 1.587\n",
      "[batch 3002]: seen 18018000 words at 1827.7 wps, loss = 1.587\n",
      "[batch 3006]: seen 18042000 words at 1827.7 wps, loss = 1.586\n",
      "[batch 3010]: seen 18066000 words at 1827.7 wps, loss = 1.586\n",
      "[batch 3014]: seen 18090000 words at 1827.7 wps, loss = 1.586\n",
      "[batch 3018]: seen 18114000 words at 1827.7 wps, loss = 1.585\n",
      "[batch 3022]: seen 18138000 words at 1827.7 wps, loss = 1.585\n",
      "[batch 3026]: seen 18162000 words at 1827.7 wps, loss = 1.585\n",
      "[batch 3030]: seen 18186000 words at 1827.7 wps, loss = 1.584\n",
      "[batch 3034]: seen 18210000 words at 1827.8 wps, loss = 1.584\n",
      "[batch 3038]: seen 18234000 words at 1827.7 wps, loss = 1.583\n",
      "[batch 3042]: seen 18258000 words at 1827.7 wps, loss = 1.583\n",
      "[batch 3046]: seen 18282000 words at 1827.8 wps, loss = 1.583\n",
      "[batch 3050]: seen 18306000 words at 1827.8 wps, loss = 1.582\n",
      "[batch 3054]: seen 18330000 words at 1827.8 wps, loss = 1.582\n",
      "[batch 3058]: seen 18354000 words at 1827.8 wps, loss = 1.582\n",
      "[batch 3062]: seen 18378000 words at 1827.8 wps, loss = 1.581\n",
      "[batch 3066]: seen 18402000 words at 1827.8 wps, loss = 1.581\n",
      "[batch 3070]: seen 18426000 words at 1827.8 wps, loss = 1.581\n",
      "[batch 3074]: seen 18450000 words at 1827.8 wps, loss = 1.581\n",
      "[batch 3078]: seen 18474000 words at 1827.8 wps, loss = 1.580\n",
      "[batch 3082]: seen 18498000 words at 1827.8 wps, loss = 1.580\n",
      "[batch 3086]: seen 18522000 words at 1827.8 wps, loss = 1.580\n",
      "[batch 3090]: seen 18546000 words at 1827.8 wps, loss = 1.579\n",
      "[batch 3094]: seen 18570000 words at 1827.8 wps, loss = 1.579\n",
      "[batch 3098]: seen 18594000 words at 1827.8 wps, loss = 1.579\n",
      "[batch 3102]: seen 18618000 words at 1827.8 wps, loss = 1.578\n",
      "[batch 3106]: seen 18642000 words at 1827.8 wps, loss = 1.578\n",
      "[batch 3110]: seen 18666000 words at 1827.8 wps, loss = 1.578\n",
      "[batch 3114]: seen 18690000 words at 1827.8 wps, loss = 1.578\n",
      "[batch 3118]: seen 18714000 words at 1827.9 wps, loss = 1.577\n",
      "[batch 3122]: seen 18738000 words at 1827.9 wps, loss = 1.577\n",
      "[batch 3126]: seen 18762000 words at 1827.9 wps, loss = 1.577\n",
      "[batch 3130]: seen 18786000 words at 1827.9 wps, loss = 1.576\n",
      "[batch 3134]: seen 18810000 words at 1827.9 wps, loss = 1.576\n",
      "[batch 3138]: seen 18834000 words at 1827.9 wps, loss = 1.576\n",
      "[batch 3142]: seen 18858000 words at 1827.9 wps, loss = 1.575\n",
      "[batch 3146]: seen 18882000 words at 1827.9 wps, loss = 1.575\n",
      "[batch 3150]: seen 18906000 words at 1827.9 wps, loss = 1.575\n",
      "[batch 3154]: seen 18930000 words at 1827.9 wps, loss = 1.574\n",
      "[batch 3158]: seen 18954000 words at 1827.9 wps, loss = 1.574\n",
      "[batch 3162]: seen 18978000 words at 1827.9 wps, loss = 1.574\n",
      "[batch 3166]: seen 19002000 words at 1827.9 wps, loss = 1.573\n",
      "[batch 3170]: seen 19026000 words at 1827.9 wps, loss = 1.573\n",
      "[batch 3174]: seen 19050000 words at 1828.0 wps, loss = 1.573\n",
      "[batch 3178]: seen 19074000 words at 1828.0 wps, loss = 1.573\n",
      "[batch 3182]: seen 19098000 words at 1828.0 wps, loss = 1.572\n",
      "[batch 3186]: seen 19122000 words at 1828.0 wps, loss = 1.572\n",
      "[batch 3190]: seen 19146000 words at 1828.0 wps, loss = 1.572\n",
      "[batch 3194]: seen 19170000 words at 1828.0 wps, loss = 1.571\n",
      "[batch 3198]: seen 19194000 words at 1828.0 wps, loss = 1.571\n",
      "[batch 3202]: seen 19218000 words at 1828.0 wps, loss = 1.571\n",
      "[batch 3206]: seen 19242000 words at 1828.0 wps, loss = 1.570\n",
      "[batch 3210]: seen 19266000 words at 1828.0 wps, loss = 1.570\n",
      "[batch 3214]: seen 19290000 words at 1828.1 wps, loss = 1.570\n",
      "[batch 3218]: seen 19314000 words at 1828.1 wps, loss = 1.569\n",
      "[batch 3222]: seen 19338000 words at 1828.1 wps, loss = 1.569\n",
      "[batch 3226]: seen 19362000 words at 1828.1 wps, loss = 1.569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 3230]: seen 19386000 words at 1828.1 wps, loss = 1.569\n",
      "[batch 3234]: seen 19410000 words at 1828.1 wps, loss = 1.568\n",
      "[batch 3238]: seen 19434000 words at 1828.1 wps, loss = 1.568\n",
      "[batch 3242]: seen 19458000 words at 1828.1 wps, loss = 1.568\n",
      "[batch 3246]: seen 19482000 words at 1828.1 wps, loss = 1.567\n",
      "[batch 3250]: seen 19506000 words at 1828.1 wps, loss = 1.567\n",
      "[batch 3254]: seen 19530000 words at 1828.1 wps, loss = 1.567\n",
      "[batch 3258]: seen 19554000 words at 1828.1 wps, loss = 1.567\n",
      "[batch 3262]: seen 19578000 words at 1828.1 wps, loss = 1.566\n",
      "[batch 3266]: seen 19602000 words at 1828.1 wps, loss = 1.566\n",
      "[batch 3270]: seen 19626000 words at 1828.1 wps, loss = 1.566\n",
      "[batch 3274]: seen 19650000 words at 1828.1 wps, loss = 1.565\n",
      "[batch 3278]: seen 19674000 words at 1828.1 wps, loss = 1.565\n",
      "[batch 3282]: seen 19698000 words at 1828.1 wps, loss = 1.565\n",
      "[batch 3286]: seen 19722000 words at 1828.1 wps, loss = 1.565\n",
      "[batch 3290]: seen 19746000 words at 1828.1 wps, loss = 1.564\n",
      "[batch 3294]: seen 19770000 words at 1828.1 wps, loss = 1.564\n",
      "[batch 3298]: seen 19794000 words at 1828.1 wps, loss = 1.564\n",
      "[batch 3302]: seen 19818000 words at 1828.1 wps, loss = 1.564\n",
      "[batch 3306]: seen 19842000 words at 1828.1 wps, loss = 1.563\n",
      "[batch 3310]: seen 19866000 words at 1828.2 wps, loss = 1.563\n",
      "[batch 3314]: seen 19890000 words at 1828.2 wps, loss = 1.563\n",
      "[batch 3318]: seen 19914000 words at 1828.2 wps, loss = 1.562\n",
      "[batch 3322]: seen 19938000 words at 1828.2 wps, loss = 1.562\n",
      "[batch 3326]: seen 19962000 words at 1828.2 wps, loss = 1.562\n",
      "[batch 3330]: seen 19986000 words at 1828.2 wps, loss = 1.562\n",
      "[batch 3334]: seen 20010000 words at 1828.2 wps, loss = 1.561\n",
      "[batch 3338]: seen 20034000 words at 1828.2 wps, loss = 1.561\n",
      "[batch 3342]: seen 20058000 words at 1828.2 wps, loss = 1.561\n",
      "[batch 3346]: seen 20082000 words at 1828.2 wps, loss = 1.560\n",
      "[batch 3350]: seen 20106000 words at 1828.2 wps, loss = 1.560\n",
      "[batch 3354]: seen 20130000 words at 1828.2 wps, loss = 1.560\n",
      "[batch 3358]: seen 20154000 words at 1828.2 wps, loss = 1.559\n",
      "[batch 3362]: seen 20178000 words at 1828.2 wps, loss = 1.559\n",
      "[batch 3366]: seen 20202000 words at 1828.3 wps, loss = 1.559\n",
      "[batch 3370]: seen 20226000 words at 1828.3 wps, loss = 1.559\n",
      "[batch 3374]: seen 20250000 words at 1828.3 wps, loss = 1.558\n",
      "[batch 3378]: seen 20274000 words at 1828.3 wps, loss = 1.558\n",
      "[batch 3382]: seen 20298000 words at 1828.3 wps, loss = 1.558\n",
      "[batch 3386]: seen 20322000 words at 1828.3 wps, loss = 1.558\n",
      "[batch 3390]: seen 20346000 words at 1828.3 wps, loss = 1.557\n",
      "[batch 3394]: seen 20370000 words at 1828.3 wps, loss = 1.557\n",
      "[batch 3398]: seen 20394000 words at 1828.3 wps, loss = 1.557\n",
      "[batch 3402]: seen 20418000 words at 1828.3 wps, loss = 1.556\n",
      "[batch 3406]: seen 20442000 words at 1828.3 wps, loss = 1.556\n",
      "[batch 3410]: seen 20466000 words at 1828.3 wps, loss = 1.556\n",
      "[batch 3414]: seen 20490000 words at 1828.4 wps, loss = 1.556\n",
      "[batch 3418]: seen 20514000 words at 1828.4 wps, loss = 1.555\n",
      "[batch 3422]: seen 20538000 words at 1828.4 wps, loss = 1.555\n",
      "[batch 3426]: seen 20562000 words at 1828.4 wps, loss = 1.555\n",
      "[batch 3430]: seen 20586000 words at 1828.4 wps, loss = 1.554\n",
      "[batch 3434]: seen 20610000 words at 1828.4 wps, loss = 1.554\n",
      "[batch 3438]: seen 20634000 words at 1828.4 wps, loss = 1.554\n",
      "[batch 3442]: seen 20658000 words at 1828.4 wps, loss = 1.554\n",
      "[batch 3446]: seen 20682000 words at 1828.4 wps, loss = 1.553\n",
      "[batch 3450]: seen 20706000 words at 1828.4 wps, loss = 1.553\n",
      "[batch 3454]: seen 20730000 words at 1828.4 wps, loss = 1.553\n",
      "[batch 3458]: seen 20754000 words at 1828.4 wps, loss = 1.553\n",
      "[batch 3462]: seen 20778000 words at 1828.4 wps, loss = 1.552\n",
      "[batch 3466]: seen 20802000 words at 1828.4 wps, loss = 1.552\n",
      "[batch 3470]: seen 20826000 words at 1828.5 wps, loss = 1.552\n",
      "[batch 3474]: seen 20850000 words at 1828.4 wps, loss = 1.551\n",
      "[batch 3478]: seen 20874000 words at 1828.4 wps, loss = 1.551\n",
      "[batch 3482]: seen 20898000 words at 1828.5 wps, loss = 1.551\n",
      "[batch 3486]: seen 20922000 words at 1828.5 wps, loss = 1.551\n",
      "[batch 3490]: seen 20946000 words at 1828.5 wps, loss = 1.550\n",
      "[batch 3494]: seen 20970000 words at 1828.5 wps, loss = 1.550\n",
      "[batch 3498]: seen 20994000 words at 1828.5 wps, loss = 1.550\n",
      "[batch 3502]: seen 21018000 words at 1828.5 wps, loss = 1.549\n",
      "[batch 3506]: seen 21042000 words at 1828.5 wps, loss = 1.549\n",
      "[batch 3510]: seen 21066000 words at 1828.5 wps, loss = 1.549\n",
      "[batch 3514]: seen 21090000 words at 1828.5 wps, loss = 1.549\n",
      "[batch 3518]: seen 21114000 words at 1828.5 wps, loss = 1.548\n",
      "[batch 3522]: seen 21138000 words at 1828.5 wps, loss = 1.548\n",
      "[batch 3526]: seen 21162000 words at 1828.5 wps, loss = 1.548\n",
      "[batch 3530]: seen 21186000 words at 1828.5 wps, loss = 1.548\n",
      "[batch 3534]: seen 21210000 words at 1828.5 wps, loss = 1.547\n",
      "[batch 3538]: seen 21234000 words at 1828.5 wps, loss = 1.547\n",
      "[batch 3542]: seen 21258000 words at 1828.5 wps, loss = 1.547\n",
      "[batch 3546]: seen 21282000 words at 1828.5 wps, loss = 1.546\n",
      "[batch 3550]: seen 21306000 words at 1828.5 wps, loss = 1.546\n",
      "[batch 3554]: seen 21330000 words at 1828.5 wps, loss = 1.546\n",
      "[batch 3558]: seen 21354000 words at 1828.5 wps, loss = 1.546\n",
      "[batch 3562]: seen 21378000 words at 1828.5 wps, loss = 1.545\n",
      "[batch 3566]: seen 21402000 words at 1828.5 wps, loss = 1.545\n",
      "[batch 3570]: seen 21426000 words at 1828.5 wps, loss = 1.545\n",
      "[batch 3574]: seen 21450000 words at 1828.5 wps, loss = 1.544\n",
      "[batch 3578]: seen 21474000 words at 1828.5 wps, loss = 1.544\n",
      "[batch 3582]: seen 21498000 words at 1828.6 wps, loss = 1.544\n",
      "[batch 3586]: seen 21522000 words at 1828.6 wps, loss = 1.544\n",
      "[batch 3590]: seen 21546000 words at 1828.6 wps, loss = 1.543\n",
      "[batch 3594]: seen 21570000 words at 1828.6 wps, loss = 1.543\n",
      "[batch 3598]: seen 21594000 words at 1828.6 wps, loss = 1.543\n",
      "[batch 3602]: seen 21618000 words at 1828.6 wps, loss = 1.543\n",
      "[batch 3606]: seen 21642000 words at 1828.6 wps, loss = 1.542\n",
      "[batch 3610]: seen 21666000 words at 1828.6 wps, loss = 1.542\n",
      "[batch 3614]: seen 21690000 words at 1828.6 wps, loss = 1.542\n",
      "[batch 3618]: seen 21714000 words at 1828.7 wps, loss = 1.542\n",
      "[batch 3622]: seen 21738000 words at 1828.6 wps, loss = 1.541\n",
      "[batch 3626]: seen 21762000 words at 1828.6 wps, loss = 1.541\n",
      "[batch 3630]: seen 21786000 words at 1828.6 wps, loss = 1.541\n",
      "[batch 3634]: seen 21810000 words at 1828.6 wps, loss = 1.541\n",
      "[batch 3638]: seen 21834000 words at 1828.6 wps, loss = 1.540\n",
      "[batch 3642]: seen 21858000 words at 1828.6 wps, loss = 1.540\n",
      "[batch 3646]: seen 21882000 words at 1828.6 wps, loss = 1.540\n",
      "[batch 3650]: seen 21906000 words at 1828.6 wps, loss = 1.540\n",
      "[batch 3654]: seen 21930000 words at 1828.5 wps, loss = 1.539\n",
      "[batch 3658]: seen 21954000 words at 1828.5 wps, loss = 1.539\n",
      "[batch 3662]: seen 21978000 words at 1828.5 wps, loss = 1.539\n",
      "[batch 3666]: seen 22002000 words at 1828.5 wps, loss = 1.539\n",
      "[batch 3670]: seen 22026000 words at 1828.5 wps, loss = 1.538\n",
      "[batch 3674]: seen 22050000 words at 1828.5 wps, loss = 1.538\n",
      "[batch 3678]: seen 22074000 words at 1828.5 wps, loss = 1.538\n",
      "[batch 3682]: seen 22098000 words at 1828.5 wps, loss = 1.538\n",
      "[batch 3686]: seen 22122000 words at 1828.6 wps, loss = 1.537\n",
      "[batch 3690]: seen 22146000 words at 1828.6 wps, loss = 1.537\n",
      "[batch 3694]: seen 22170000 words at 1828.6 wps, loss = 1.537\n",
      "[batch 3698]: seen 22194000 words at 1828.6 wps, loss = 1.537\n",
      "[batch 3702]: seen 22218000 words at 1828.6 wps, loss = 1.536\n",
      "[batch 3706]: seen 22242000 words at 1828.6 wps, loss = 1.536\n",
      "[batch 3710]: seen 22266000 words at 1828.6 wps, loss = 1.536\n",
      "[batch 3714]: seen 22290000 words at 1828.6 wps, loss = 1.536\n",
      "[batch 3718]: seen 22314000 words at 1828.6 wps, loss = 1.535\n",
      "[batch 3722]: seen 22338000 words at 1828.6 wps, loss = 1.535\n",
      "[batch 3726]: seen 22362000 words at 1828.6 wps, loss = 1.535\n",
      "[batch 3730]: seen 22386000 words at 1828.6 wps, loss = 1.535\n",
      "[batch 3734]: seen 22410000 words at 1828.6 wps, loss = 1.534\n",
      "[batch 3738]: seen 22434000 words at 1828.6 wps, loss = 1.534\n",
      "[batch 3742]: seen 22458000 words at 1828.6 wps, loss = 1.534\n",
      "[batch 3746]: seen 22482000 words at 1828.6 wps, loss = 1.534\n",
      "[batch 3750]: seen 22506000 words at 1828.6 wps, loss = 1.533\n",
      "[batch 3754]: seen 22530000 words at 1828.6 wps, loss = 1.533\n",
      "[batch 3758]: seen 22554000 words at 1828.6 wps, loss = 1.533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 3762]: seen 22578000 words at 1828.6 wps, loss = 1.533\n",
      "[batch 3766]: seen 22602000 words at 1828.6 wps, loss = 1.533\n",
      "[batch 3770]: seen 22626000 words at 1828.7 wps, loss = 1.532\n",
      "[batch 3774]: seen 22650000 words at 1828.7 wps, loss = 1.532\n",
      "[batch 3778]: seen 22674000 words at 1828.7 wps, loss = 1.532\n",
      "[batch 3782]: seen 22698000 words at 1828.7 wps, loss = 1.532\n",
      "[batch 3786]: seen 22722000 words at 1828.7 wps, loss = 1.531\n",
      "[batch 3790]: seen 22746000 words at 1828.7 wps, loss = 1.531\n",
      "[batch 3794]: seen 22770000 words at 1828.7 wps, loss = 1.531\n",
      "[batch 3798]: seen 22794000 words at 1828.7 wps, loss = 1.531\n",
      "[batch 3802]: seen 22818000 words at 1828.7 wps, loss = 1.530\n",
      "[batch 3806]: seen 22842000 words at 1828.8 wps, loss = 1.530\n",
      "[batch 3810]: seen 22866000 words at 1828.8 wps, loss = 1.530\n",
      "[batch 3814]: seen 22890000 words at 1828.8 wps, loss = 1.530\n",
      "[batch 3818]: seen 22914000 words at 1828.8 wps, loss = 1.529\n",
      "[batch 3822]: seen 22938000 words at 1828.8 wps, loss = 1.529\n",
      "[batch 3826]: seen 22962000 words at 1828.8 wps, loss = 1.529\n",
      "[batch 3830]: seen 22986000 words at 1828.8 wps, loss = 1.529\n",
      "[batch 3834]: seen 23010000 words at 1828.8 wps, loss = 1.529\n",
      "[batch 3838]: seen 23034000 words at 1828.8 wps, loss = 1.528\n",
      "[batch 3842]: seen 23058000 words at 1828.8 wps, loss = 1.528\n",
      "[batch 3846]: seen 23082000 words at 1828.8 wps, loss = 1.528\n",
      "[batch 3850]: seen 23106000 words at 1828.8 wps, loss = 1.528\n",
      "[batch 3854]: seen 23130000 words at 1828.8 wps, loss = 1.527\n",
      "[batch 3858]: seen 23154000 words at 1828.8 wps, loss = 1.527\n",
      "[batch 3862]: seen 23178000 words at 1828.8 wps, loss = 1.527\n",
      "[batch 3866]: seen 23202000 words at 1828.8 wps, loss = 1.527\n",
      "[batch 3870]: seen 23226000 words at 1828.8 wps, loss = 1.526\n",
      "[batch 3874]: seen 23250000 words at 1828.8 wps, loss = 1.526\n",
      "[batch 3878]: seen 23274000 words at 1828.8 wps, loss = 1.526\n",
      "[batch 3882]: seen 23298000 words at 1828.8 wps, loss = 1.526\n",
      "[batch 3886]: seen 23322000 words at 1828.8 wps, loss = 1.526\n",
      "[batch 3890]: seen 23346000 words at 1828.8 wps, loss = 1.525\n",
      "[batch 3893]: seen 23364000 words at 1828.7 wps, loss = 1.525\n",
      "[batch 3896]: seen 23382000 words at 1828.7 wps, loss = 1.525\n",
      "[batch 3900]: seen 23406000 words at 1828.7 wps, loss = 1.525\n",
      "[batch 3904]: seen 23430000 words at 1828.7 wps, loss = 1.525\n",
      "[batch 3908]: seen 23454000 words at 1828.7 wps, loss = 1.524\n",
      "[batch 3912]: seen 23478000 words at 1828.7 wps, loss = 1.524\n",
      "[batch 3916]: seen 23502000 words at 1828.7 wps, loss = 1.524\n",
      "[batch 3920]: seen 23526000 words at 1828.7 wps, loss = 1.524\n",
      "[batch 3924]: seen 23550000 words at 1828.7 wps, loss = 1.524\n",
      "[batch 3928]: seen 23574000 words at 1828.7 wps, loss = 1.523\n",
      "[batch 3932]: seen 23598000 words at 1828.7 wps, loss = 1.523\n",
      "[batch 3936]: seen 23622000 words at 1828.7 wps, loss = 1.523\n",
      "[batch 3940]: seen 23646000 words at 1828.7 wps, loss = 1.523\n",
      "[batch 3944]: seen 23670000 words at 1828.7 wps, loss = 1.523\n",
      "[batch 3948]: seen 23694000 words at 1828.7 wps, loss = 1.522\n",
      "[batch 3952]: seen 23718000 words at 1828.7 wps, loss = 1.522\n",
      "[batch 3955]: seen 23736000 words at 1828.6 wps, loss = 1.522\n",
      "[batch 3959]: seen 23760000 words at 1828.6 wps, loss = 1.522\n",
      "[batch 3963]: seen 23784000 words at 1828.6 wps, loss = 1.521\n",
      "[batch 3967]: seen 23808000 words at 1828.6 wps, loss = 1.521\n",
      "[batch 3971]: seen 23832000 words at 1828.6 wps, loss = 1.521\n",
      "[batch 3975]: seen 23856000 words at 1828.6 wps, loss = 1.521\n",
      "[batch 3979]: seen 23880000 words at 1828.6 wps, loss = 1.521\n",
      "[batch 3983]: seen 23904000 words at 1828.6 wps, loss = 1.520\n",
      "[batch 3987]: seen 23928000 words at 1828.6 wps, loss = 1.520\n",
      "[batch 3991]: seen 23952000 words at 1828.6 wps, loss = 1.520\n",
      "[batch 3995]: seen 23976000 words at 1828.6 wps, loss = 1.520\n",
      "[batch 3998]: seen 23994000 words at 1828.5 wps, loss = 1.520\n",
      "[batch 4002]: seen 24018000 words at 1828.5 wps, loss = 1.520\n",
      "[batch 4006]: seen 24042000 words at 1828.5 wps, loss = 1.519\n",
      "[batch 4010]: seen 24066000 words at 1828.5 wps, loss = 1.519\n",
      "[batch 4014]: seen 24090000 words at 1828.5 wps, loss = 1.519\n",
      "[batch 4018]: seen 24114000 words at 1828.5 wps, loss = 1.519\n",
      "[batch 4022]: seen 24138000 words at 1828.6 wps, loss = 1.518\n",
      "[batch 4026]: seen 24162000 words at 1828.5 wps, loss = 1.518\n",
      "[batch 4030]: seen 24186000 words at 1828.5 wps, loss = 1.518\n",
      "[batch 4034]: seen 24210000 words at 1828.5 wps, loss = 1.518\n",
      "[batch 4038]: seen 24234000 words at 1828.5 wps, loss = 1.518\n",
      "[batch 4042]: seen 24258000 words at 1828.5 wps, loss = 1.517\n",
      "[batch 4046]: seen 24282000 words at 1828.5 wps, loss = 1.517\n",
      "[batch 4050]: seen 24306000 words at 1828.5 wps, loss = 1.517\n",
      "[batch 4054]: seen 24330000 words at 1828.5 wps, loss = 1.517\n",
      "[batch 4058]: seen 24354000 words at 1828.5 wps, loss = 1.517\n",
      "[batch 4062]: seen 24378000 words at 1828.5 wps, loss = 1.516\n",
      "[batch 4066]: seen 24402000 words at 1828.5 wps, loss = 1.516\n",
      "[batch 4070]: seen 24426000 words at 1828.5 wps, loss = 1.516\n",
      "[batch 4074]: seen 24450000 words at 1828.5 wps, loss = 1.516\n",
      "[batch 4078]: seen 24474000 words at 1828.5 wps, loss = 1.516\n",
      "[batch 4082]: seen 24498000 words at 1828.5 wps, loss = 1.515\n",
      "[batch 4086]: seen 24522000 words at 1828.5 wps, loss = 1.515\n",
      "[batch 4090]: seen 24546000 words at 1828.5 wps, loss = 1.515\n",
      "[batch 4094]: seen 24570000 words at 1828.5 wps, loss = 1.515\n",
      "[batch 4098]: seen 24594000 words at 1828.5 wps, loss = 1.515\n",
      "[epoch 1] Completed in 3:44:17\n",
      "[epoch 1] Train set: avg. loss: 1.220  (perplexity: 3.39)\n",
      "[epoch 1] Test set: avg. loss: 1.210  (perplexity: 3.35)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "    \n",
    "    #check trainable variables\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n",
    "    \n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "    #lm.BuildSamplerGraph()\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "<SOR>getled (but i was happy would get while you grow o\n",
      "<SOR>5th hot clarl with because. we didn't lip the mill\n",
      "<SOR>briking a most happy i should in the most, a life \n",
      "<SOR>a nice one which kerp (conesteagily to long and wh\n",
      "<SOR>ryly experience to make the bar requested kee busi\n",
      "<SOR>were alla lots of spot for juices me off the cuper\n",
      "<SOR>hopes we got only but if she would have never been\n",
      "<SOR>friendly and constructors/good!<EOR>\n",
      "<SOR>down again after a shops that was nice and stop to\n",
      "<SOR>britted as well. appetizers indian food tacos he n\n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "#max_steps = 20\n",
    "max_steps = 50\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    [char_dict.get(token) for token in test_review_list]\n",
    "    w = np.repeat([[char_dict.get('<SOR>')]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            #print(vocab.id_to_word[word_id], end=\" \")\n",
    "            print(ids_to_words[word_id], end=\"\")\n",
    "            #if (i != 0) and (word_id == vocab.START_ID):\n",
    "            if (i != 0) and (word_id == char_dict.get(\"<EOR>\")):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linguistic Properties\n",
    "\n",
    "Given two or more test sentences, the model should score the more plausible (or more correct) sentence with a higher log-probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"once upon a time\" : -8.74\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.83\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"once upon a time\",\n",
    "         #\"the quick brown fox jumps over the lazy dog\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = [\"is sentence nonsense this\", \"i drive a car\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -6.03\n",
      "\"the boy and the girl is\" : -5.90\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"the boy and the girl are\", \"the boy and the girl is\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -7.46\n",
      "\"peanuts are my favorite kind of vegetable\" : -7.29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"when I'm hungry I really prefer to eat\" : -7.49\n",
      "\"when I'm hungry I really prefer to drink\" : -7.52\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"peanuts are my favorite kind of nut\",\n",
    "         #\"peanuts are my favorite kind of vegetable\"]\n",
    "#load_and_score([s.split() for s in sents])\n",
    "\n",
    "#sents = [\"when I'm hungry I really prefer to eat\",\n",
    "         #\"when I'm hungry I really prefer to drink\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"I have lots of plastic green square toys\" : -8.51\n",
      "\"I have lots of green square plastic toys\" : -8.56\n",
      "\"I have lots of green plastic square toys\" : -8.57\n",
      "\"I have lots of plastic square green toys\" : -8.64\n",
      "\"I have lots of square green plastic toys\" : -8.69\n",
      "\"I have lots of square plastic green toys\" : -8.71\n"
     ]
    }
   ],
   "source": [
    "#prefix = \"I have lots of\".split()\n",
    "#noun = \"toys\"\n",
    "#adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "#inputs = []\n",
    "#for adjs in itertools.permutations(adjectives):\n",
    "    #words = prefix + list(adjs) + [noun]\n",
    "    #inputs.append(words)\n",
    "    \n",
    "#load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
