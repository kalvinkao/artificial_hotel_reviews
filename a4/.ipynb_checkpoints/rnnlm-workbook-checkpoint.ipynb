{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Recurrent Neural Network Language Model\n",
    "\n",
    "This is the \"working notebook\", with skeleton code to load and train your model, as well as run unit tests. See [rnnlm-instructions.ipynb](rnnlm-instructions.ipynb) for the main writeup.\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters  \n",
    "\n",
    "### Questions for Part (a)\n",
    "You should use big-O notation when appropriate (i.e. computing $\\exp(\\mathbf{v})$ for a vector $\\mathbf{v}$ of length $n$ is $O(n)$ operations).  Assume for problems a(1-5) that:   \n",
    "> -- Cell is one layer,  \n",
    "> -- the embedding feature length and hidden-layer feature lengths are both H, and   \n",
    "> -- ignore at the moment batch and max_time dimensions.  \n",
    "\n",
    "1. Let $\\text{CellFunc}$ be a simple RNN cell (see async Section 5.8). Write the cell equation in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
    "<p>\n",
    "2. How many parameters are in the embedding layer? In the output layer? (By parameters, we mean total number of matrix elements across all train-able tensors. A $m \\times n$ matrix has $mn$ elements.)\n",
    "<p>\n",
    "3. How many calculations (floating point operations) are required to compute $\\hat{P}(w^{(i+1)})$ for a given *single* target word $w^{(i+1)}$, assuming $w^{(i)}$ given and $h^{(i-1)}$ already computed? How about for *all* target words?\n",
    "<p>\n",
    "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax? (*Recall that hierarchical softmax makes a series of left/right decisions using a binary classifier $P_s(\\text{right}) = \\sigma(u_s \\cdot o^{(i)} + b_s)$ at each split $s$ in the tree.*)\n",
    "<p>\n",
    "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training? (*Choose \"embedding layer\", \"recurrent layer\", or \"output layer\"*.)\n",
    "\n",
    "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers for Part (a)\n",
    "\n",
    "####  [Use the Google forms to answer a1) - a5)!]( https://docs.google.com/forms/d/e/1FAIpQLSc7kpuOzErVE_H0vMfsDvFcBHz9dkwXGSzqHUGa70QsTIC1ow/viewform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM\n",
    "\n",
    "### Aside: Shapes Review\n",
    "\n",
    "Before we start, let's review our understanding of the shapes involved in this assignment and how they manifest themselves in the TF API.\n",
    "\n",
    "As in the [instructions](rnnlm-instructions.ipynb) notebook, $w$ is a matrix of wordids with shape batch_size x max_time.  Passing this through the embedding layer, we retrieve the word embedding for each, resulting in $x$ having shape batch_size x max_time x embedding_dim.  I find it useful to draw this out on a piece of paper.  When you do, you should end up with a rectangular prism with batch_size height, max_time width and embedding_dim depth.  Many tensors in this assignment share this shape (e.g. $o$, the output from the LSTM, which represents the hidden layer going into the softmax to make a prediction at every time step in every batch).\n",
    "\n",
    "Since batch size and sentence length are only resolved when we run the graph, we construct the placeholder using \"None\" in the dimensions we don't know.  The .shape property renders these as ?s.  This should be familiar to you from batch size handling in earlier assignments, only now there are two dimensions of variable length.\n",
    "\n",
    "See the next cell for a concrete example (though in practice, we'd use a TensorFlow variable that we can train for the embeddings rather than a static array).  Notice how the shape of x_val matches the shape described earlier in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "print('wordid placeholder shape:', wordid_ph.shape)\n",
    "print('x shape:', x.shape)\n",
    "\n",
    "sess = tf.Session()\n",
    "# Two sentences, each with four words.\n",
    "wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implmenting the RNNLM\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/artificial_hotel_reviews/a4_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "cd assignment/a4\n",
    "tensorboard --logdir /tmp/w266/a4_graph --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/ and visit the \"Graphs\" tab to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  W_in:0\n",
      "Shape:  (10000, 200)\n",
      "[[-0.78748155 -0.05848813 -0.64172769 ...,  0.84141541 -0.94063973\n",
      "   0.6318059 ]\n",
      " [ 0.73271012  0.9191277   0.16166019 ..., -0.77998519  0.349545\n",
      "  -0.45933056]\n",
      " [ 0.56432152 -0.03329587  0.20995927 ..., -0.3828721   0.12824035\n",
      "  -0.13679957]\n",
      " ..., \n",
      " [-0.12901855 -0.29448342 -0.08786106 ..., -0.73254704 -0.37692833\n",
      "  -0.20111871]\n",
      " [-0.62848496 -0.50970197  0.9452734  ...,  0.99917698 -0.81992722\n",
      "  -0.37649083]\n",
      " [-0.09145284  0.00680947  0.67164588 ..., -0.92668557  0.46153927\n",
      "   0.74277234]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.02885439 -0.00585808 -0.04069098 ...,  0.00699186 -0.05404399\n",
      "   0.02202466]\n",
      " [ 0.06509807 -0.02223223  0.02077565 ..., -0.00037639  0.04783005\n",
      "  -0.00804034]\n",
      " [ 0.02742642  0.05705827 -0.03898749 ...,  0.00316557  0.06507974\n",
      "  -0.01836298]\n",
      " ..., \n",
      " [-0.00754741 -0.04928695  0.06990113 ..., -0.06414328 -0.06926721\n",
      "  -0.05963309]\n",
      " [-0.06505661 -0.0187963   0.05553678 ..., -0.00105456 -0.04487306\n",
      "   0.05926735]\n",
      " [ 0.05089028  0.03582108 -0.05539448 ...,  0.00139427  0.06008063\n",
      "   0.05134535]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.06411717 -0.059903    0.05128169 ...,  0.03041378  0.05353859\n",
      "   0.0544473 ]\n",
      " [ 0.06030189  0.02777085  0.0119326  ..., -0.00028028 -0.04805488\n",
      "  -0.03846691]\n",
      " [-0.02244554  0.04396319  0.06136966 ...,  0.02755227  0.01427022\n",
      "   0.0558702 ]\n",
      " ..., \n",
      " [-0.0622274  -0.03975505 -0.03073074 ...,  0.0241474  -0.04965901\n",
      "  -0.00885601]\n",
      " [ 0.0227494  -0.0019692  -0.05720998 ..., -0.03900233 -0.02878788\n",
      "  -0.05673516]\n",
      " [-0.04740062  0.00044262  0.0118935  ...,  0.02134912  0.04072783\n",
      "   0.00831893]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  W_out:0\n",
      "Shape:  (200, 10000)\n",
      "[[ 0.22999096 -0.98074627  0.2485311  ...,  0.31636119  0.64180779\n",
      "   0.48168755]\n",
      " [ 0.02860689  0.14025211  0.73703051 ...,  0.55039978 -0.11227965\n",
      "   0.06153202]\n",
      " [-0.74636269 -0.50103402  0.13248014 ..., -0.45557976  0.15847898\n",
      "   0.09724474]\n",
      " ..., \n",
      " [-0.21404767  0.9171977  -0.29322004 ..., -0.82013679  0.63434672\n",
      "  -0.56355882]\n",
      " [ 0.83722639 -0.26542854 -0.89726591 ..., -0.59170508  0.18550444\n",
      "   0.48118067]\n",
      " [ 0.55475283 -0.74218535 -0.24593616 ..., -0.88373137 -0.86611366\n",
      "  -0.00220013]]\n",
      "Variable:  b_out:0\n",
      "Shape:  (10000,)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    session.run(initializer)\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = session.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(\"Variable: \", k)\n",
    "        print(\"Shape: \", v.shape)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a few unit tests below to verify some *very* basic properties of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 3.315s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error messages are intentionally somewhat spare, and that passing tests are no guarantee of model correctness! Your best chance of success is through careful coding and understanding of how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 5 notebook.\n",
    "\n",
    "Particularly, `utils.rnnlm_batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:  \n",
    "*(Ignore the ugly formatter code.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to verify your implementation of `run_epoch`, and to test your RNN on a (very simple) toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 0]: seen 50 words at 43.1 wps, loss = 2.028\n",
      "[batch 98]: seen 4950 words at 2284.2 wps, loss = 1.114\n",
      "[batch 210]: seen 10550 words at 3329.2 wps, loss = 0.959\n",
      "[batch 346]: seen 17350 words at 4155.3 wps, loss = 1.024\n",
      "[batch 481]: seen 24100 words at 4656.4 wps, loss = 0.983\n",
      "[batch 615]: seen 30800 words at 4987.1 wps, loss = 0.943\n",
      "[batch 751]: seen 37600 words at 5237.5 wps, loss = 0.954\n",
      "[batch 887]: seen 44400 words at 5425.6 wps, loss = 0.964\n",
      "[batch 1024]: seen 51250 words at 5579.2 wps, loss = 0.954\n",
      "[batch 1160]: seen 58050 words at 5697.4 wps, loss = 0.943\n",
      "[batch 1296]: seen 64850 words at 5794.1 wps, loss = 0.930\n",
      "[batch 1431]: seen 71600 words at 5872.2 wps, loss = 0.921\n",
      "[batch 1567]: seen 78400 words at 5940.1 wps, loss = 0.917\n",
      "[batch 1702]: seen 85150 words at 5995.7 wps, loss = 0.905\n",
      "[batch 1839]: seen 92000 words at 6050.8 wps, loss = 0.909\n",
      "[batch 1976]: seen 98850 words at 6098.0 wps, loss = 0.914\n",
      "[batch 2112]: seen 105650 words at 6138.3 wps, loss = 0.906\n",
      "[batch 2245]: seen 112300 words at 6164.1 wps, loss = 0.893\n",
      "[batch 2379]: seen 119000 words at 6191.6 wps, loss = 0.883\n",
      "[batch 2516]: seen 125850 words at 6222.1 wps, loss = 0.881\n",
      "[batch 2650]: seen 132550 words at 6243.5 wps, loss = 0.886\n",
      "[batch 2786]: seen 139350 words at 6268.4 wps, loss = 0.880\n",
      "[batch 2919]: seen 146000 words at 6283.9 wps, loss = 0.875\n",
      "[batch 3055]: seen 152800 words at 6303.9 wps, loss = 0.870\n",
      "[batch 3189]: seen 159500 words at 6319.6 wps, loss = 0.874\n",
      "[batch 3323]: seen 166200 words at 6334.0 wps, loss = 0.878\n",
      "[batch 3456]: seen 172850 words at 6344.5 wps, loss = 0.889\n",
      "[batch 3587]: seen 179400 words at 6350.9 wps, loss = 0.886\n",
      "[batch 3717]: seen 185900 words at 6355.3 wps, loss = 0.880\n",
      "[batch 3844]: seen 192250 words at 6353.9 wps, loss = 0.873\n",
      "[batch 3970]: seen 198550 words at 6351.4 wps, loss = 0.870\n",
      "Train set: avg. loss: 0.018  (perplexity: 1.02)\n",
      "Test set: avg. loss: 0.038  (perplexity: 1.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 33.626s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as above, this is a *very* simple test case that does not guarantee model correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `/tmp/w266/a4_model/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in around 15 minutes on a single-core GCE instance, and reach a training set perplexity between 120-140.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Notes on Speed\n",
    "\n",
    "To speed things up, you may want to re-start your GCE instance with more CPUs. Using a 16-core machine will train *very* quickly if using a sampled softmax lost, almost as fast as a GPU. (Because of the sequential nature of the model, GPUs aren't actually much faster than CPUs for training and running RNNs.) The training code will print the words-per-second processed; with the default settings on a single core, you can expect around 8000 WPS, or up to more than 25000 WPS on a fast multi-core machine.\n",
    "\n",
    "You might also want to modify the code below to only run score_dataset at the very end, after all epochs are completed. This will speed things up significantly, since `score_dataset` uses the full softmax loss - and so often can take longer than a whole training epoch!\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "cp /tmp/w266/a4_model/rnnlm_trained* .\n",
    "git add rnnlm_trained*\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle. If you do also train a large model, please only submit the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/kalvin_kao/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv(review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_csv(business_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_review_df = review_df[review_df['stars']==5]\n",
    "five_star_review_series = five_star_review_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in five_star_review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "        #print(final_review)\n",
    "        review_list.append(final_review)\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed reviews\n",
    "#review_list = preprocess_review_series(five_star_review_series)\n",
    "review_list = preprocess_review_series(five_star_review_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10406707\n"
     ]
    }
   ],
   "source": [
    "#combined_review_list = [item for sublist in review_list for item in sublist]\n",
    "training_review_list = [item for sublist in review_list[:20000] for item in sublist]\n",
    "print(len(training_review_list))\n",
    "test_review_list = [item for sublist in review_list[20000:21000] for item in sublist]\n",
    "#unique_characters = list(set(combined_review_list))\n",
    "unique_characters = list(set(training_review_list + test_review_list))\n",
    "#len(unique_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'h', 1: '.', 2: '4', 3: '*', 4: '_', 5: '`', 6: 'p', 7: 'j', 8: 'n', 9: 'w', 10: ';', 11: 'z', 12: '#', 13: 'f', 14: 'r', 15: 'b', 16: 'q', 17: '^', 18: '<EOR>', 19: 'x', 20: 'y', 21: '@', 22: '5', 23: '$', 24: \"'\", 25: 't', 26: 'k', 27: '\"', 28: '7', 29: '8', 30: 'c', 31: '|', 32: 'a', 33: 'e', 34: 'o', 35: '}', 36: '(', 37: 'l', 38: ' ', 39: '?', 40: 'g', 41: '-', 42: '%', 43: '[', 44: '2', 45: '+', 46: '9', 47: '~', 48: 'd', 49: '6', 50: ',', 51: ')', 52: ':', 53: ']', 54: 's', 55: '0', 56: 'u', 57: '/', 58: '=', 59: '1', 60: '<SOR>', 61: '3', 62: '&', 63: 'm', 64: '!', 65: 'v', 66: 'i'}\n"
     ]
    }
   ],
   "source": [
    "#vocabulary\n",
    "char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#print(char_dict)\n",
    "#print(ids_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to flat (1D) np.array(int) of ids\n",
    "#add memory to VM and remove 1000 slice\n",
    "#combined_review_ids = [char_dict.get(token) for token in combined_review_list[:1000]]\n",
    "training_review_ids = [char_dict.get(token) for token in training_review_list]\n",
    "test_review_ids = [char_dict.get(token) for token in test_review_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array(training_review_ids)\n",
    "test_ids = np.array(test_review_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "#max_time = 25\n",
    "max_time = 35\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "#num_epochs = 10\n",
    "num_epochs = 1\n",
    "\n",
    "# Model parameters\n",
    "#model_params = dict(V=vocab.size, \n",
    "                    #H=200, \n",
    "                    #softmax_ns=200,\n",
    "                    #num_layers=2)\n",
    "model_params = dict(V=len(unique_characters), \n",
    "                    H=200, \n",
    "                    softmax_ns=len(unique_characters),\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 31]: seen 112000 words at 10971.4 wps, loss = 2.963\n",
      "[batch 63]: seen 224000 words at 10945.5 wps, loss = 2.698\n",
      "[batch 95]: seen 336000 words at 10933.6 wps, loss = 2.564\n",
      "[batch 127]: seen 448000 words at 10945.1 wps, loss = 2.469\n",
      "[batch 159]: seen 560000 words at 10957.2 wps, loss = 2.405\n",
      "[batch 191]: seen 672000 words at 10964.3 wps, loss = 2.355\n",
      "[batch 223]: seen 784000 words at 10982.7 wps, loss = 2.312\n",
      "[batch 255]: seen 896000 words at 10978.4 wps, loss = 2.275\n",
      "[batch 287]: seen 1008000 words at 10970.7 wps, loss = 2.243\n",
      "[batch 319]: seen 1120000 words at 10964.2 wps, loss = 2.214\n",
      "[batch 351]: seen 1232000 words at 10956.0 wps, loss = 2.189\n",
      "[batch 382]: seen 1340500 words at 10943.4 wps, loss = 2.167\n",
      "[batch 413]: seen 1449000 words at 10935.9 wps, loss = 2.148\n",
      "[batch 444]: seen 1557500 words at 10929.5 wps, loss = 2.131\n",
      "[batch 476]: seen 1669500 words at 10931.1 wps, loss = 2.114\n",
      "[batch 508]: seen 1781500 words at 10932.9 wps, loss = 2.098\n",
      "[batch 540]: seen 1893500 words at 10933.7 wps, loss = 2.086\n",
      "[batch 572]: seen 2005500 words at 10938.7 wps, loss = 2.074\n",
      "[batch 604]: seen 2117500 words at 10939.5 wps, loss = 2.063\n",
      "[batch 636]: seen 2229500 words at 10935.3 wps, loss = 2.052\n",
      "[batch 667]: seen 2338000 words at 10928.5 wps, loss = 2.041\n",
      "[batch 698]: seen 2446500 words at 10924.0 wps, loss = 2.032\n",
      "[batch 729]: seen 2555000 words at 10919.4 wps, loss = 2.023\n",
      "[batch 761]: seen 2667000 words at 10918.5 wps, loss = 2.013\n",
      "[batch 792]: seen 2775500 words at 10913.9 wps, loss = 2.004\n",
      "[batch 823]: seen 2884000 words at 10911.0 wps, loss = 1.995\n",
      "[batch 854]: seen 2992500 words at 10908.5 wps, loss = 1.987\n",
      "[batch 886]: seen 3104500 words at 10907.8 wps, loss = 1.979\n",
      "[batch 918]: seen 3216500 words at 10906.7 wps, loss = 1.972\n",
      "[batch 950]: seen 3328500 words at 10906.6 wps, loss = 1.965\n",
      "[batch 982]: seen 3440500 words at 10908.7 wps, loss = 1.959\n",
      "[batch 1013]: seen 3549000 words at 10906.6 wps, loss = 1.953\n",
      "[batch 1045]: seen 3661000 words at 10908.8 wps, loss = 1.947\n",
      "[batch 1076]: seen 3769500 words at 10905.5 wps, loss = 1.942\n",
      "[batch 1108]: seen 3881500 words at 10904.1 wps, loss = 1.936\n",
      "[batch 1139]: seen 3990000 words at 10901.7 wps, loss = 1.931\n",
      "[batch 1171]: seen 4102000 words at 10901.3 wps, loss = 1.925\n",
      "[batch 1203]: seen 4214000 words at 10904.1 wps, loss = 1.920\n",
      "[batch 1235]: seen 4326000 words at 10905.9 wps, loss = 1.915\n",
      "[batch 1267]: seen 4438000 words at 10906.1 wps, loss = 1.912\n",
      "[batch 1299]: seen 4550000 words at 10908.1 wps, loss = 1.908\n",
      "[batch 1331]: seen 4662000 words at 10910.7 wps, loss = 1.904\n",
      "[batch 1363]: seen 4774000 words at 10910.2 wps, loss = 1.900\n",
      "[batch 1395]: seen 4886000 words at 10909.8 wps, loss = 1.896\n",
      "[batch 1427]: seen 4998000 words at 10911.8 wps, loss = 1.892\n",
      "[batch 1459]: seen 5110000 words at 10913.2 wps, loss = 1.889\n",
      "[batch 1491]: seen 5222000 words at 10912.2 wps, loss = 1.885\n",
      "[batch 1523]: seen 5334000 words at 10913.0 wps, loss = 1.882\n",
      "[batch 1554]: seen 5442500 words at 10910.6 wps, loss = 1.878\n",
      "[batch 1586]: seen 5554500 words at 10911.8 wps, loss = 1.874\n",
      "[batch 1618]: seen 5666500 words at 10913.1 wps, loss = 1.871\n",
      "[batch 1650]: seen 5778500 words at 10912.0 wps, loss = 1.868\n",
      "[batch 1682]: seen 5890500 words at 10914.0 wps, loss = 1.865\n",
      "[batch 1714]: seen 6002500 words at 10915.1 wps, loss = 1.862\n",
      "[batch 1746]: seen 6114500 words at 10916.0 wps, loss = 1.859\n",
      "[batch 1778]: seen 6226500 words at 10915.5 wps, loss = 1.856\n",
      "[batch 1810]: seen 6338500 words at 10914.8 wps, loss = 1.854\n",
      "[batch 1841]: seen 6447000 words at 10912.6 wps, loss = 1.851\n",
      "[batch 1873]: seen 6559000 words at 10912.9 wps, loss = 1.848\n",
      "[batch 1905]: seen 6671000 words at 10913.3 wps, loss = 1.845\n",
      "[batch 1937]: seen 6783000 words at 10913.5 wps, loss = 1.843\n",
      "[batch 1969]: seen 6895000 words at 10912.8 wps, loss = 1.841\n",
      "[batch 2001]: seen 7007000 words at 10915.2 wps, loss = 1.838\n",
      "[batch 2033]: seen 7119000 words at 10916.1 wps, loss = 1.836\n",
      "[batch 2065]: seen 7231000 words at 10915.2 wps, loss = 1.834\n",
      "[batch 2097]: seen 7343000 words at 10916.5 wps, loss = 1.832\n",
      "[batch 2129]: seen 7455000 words at 10919.4 wps, loss = 1.830\n",
      "[batch 2161]: seen 7567000 words at 10919.7 wps, loss = 1.827\n",
      "[batch 2193]: seen 7679000 words at 10921.2 wps, loss = 1.825\n",
      "[batch 2225]: seen 7791000 words at 10921.0 wps, loss = 1.823\n",
      "[batch 2257]: seen 7903000 words at 10922.2 wps, loss = 1.821\n",
      "[batch 2289]: seen 8015000 words at 10923.1 wps, loss = 1.820\n",
      "[batch 2321]: seen 8127000 words at 10923.4 wps, loss = 1.818\n",
      "[batch 2353]: seen 8239000 words at 10925.2 wps, loss = 1.816\n",
      "[batch 2385]: seen 8351000 words at 10928.5 wps, loss = 1.815\n",
      "[batch 2417]: seen 8463000 words at 10928.9 wps, loss = 1.813\n",
      "[batch 2449]: seen 8575000 words at 10929.4 wps, loss = 1.811\n",
      "[batch 2481]: seen 8687000 words at 10929.0 wps, loss = 1.810\n",
      "[batch 2513]: seen 8799000 words at 10929.5 wps, loss = 1.809\n",
      "[batch 2545]: seen 8911000 words at 10930.1 wps, loss = 1.807\n",
      "[batch 2577]: seen 9023000 words at 10929.5 wps, loss = 1.805\n",
      "[batch 2609]: seen 9135000 words at 10930.1 wps, loss = 1.804\n",
      "[batch 2641]: seen 9247000 words at 10930.5 wps, loss = 1.802\n",
      "[batch 2673]: seen 9359000 words at 10929.8 wps, loss = 1.801\n",
      "[batch 2705]: seen 9471000 words at 10929.8 wps, loss = 1.799\n",
      "[batch 2737]: seen 9583000 words at 10930.5 wps, loss = 1.798\n",
      "[batch 2769]: seen 9695000 words at 10930.8 wps, loss = 1.797\n",
      "[batch 2801]: seen 9807000 words at 10930.2 wps, loss = 1.795\n",
      "[batch 2833]: seen 9919000 words at 10931.4 wps, loss = 1.794\n",
      "[batch 2864]: seen 10027500 words at 10930.2 wps, loss = 1.792\n",
      "[batch 2896]: seen 10139500 words at 10929.5 wps, loss = 1.791\n",
      "[batch 2927]: seen 10248000 words at 10928.2 wps, loss = 1.790\n",
      "[batch 2959]: seen 10360000 words at 10927.6 wps, loss = 1.789\n",
      "[epoch 1] Completed in 0:15:52\n",
      "[epoch 1] Train set: avg. loss: 1.476  (perplexity: 4.38)\n",
      "[epoch 1] Test set: avg. loss: 1.478  (perplexity: 4.38)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "    \n",
    "    #check trainable variables\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n",
    "    \n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,1]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "    #lm.BuildSamplerGraph()\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "<SOR>with some place for upy in this flome the wonderfu\n",
      "<SOR>works were knowledgeable. steeku<EOR>\n",
      "<SOR>to looks out the service tited. what i could ever \n",
      "<SOR>of me food was fresh. and views and it was not was\n",
      "<SOR>in the special which with my night goodne yelps. f\n",
      "<SOR>and they recently helping to the stoff back to you\n",
      "<SOR>besizes was the unextantly for a wackly could be b\n",
      "<SOR>and meats!<EOR>\n",
      "<SOR>amazing asson of cramichication and came to the go\n",
      "<SOR>before to love here. which was can did fries and s\n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "#max_steps = 20\n",
    "max_steps = 50\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    [char_dict.get(token) for token in test_review_list]\n",
    "    w = np.repeat([[char_dict.get('<SOR>')]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            #print(vocab.id_to_word[word_id], end=\" \")\n",
    "            print(ids_to_words[word_id], end=\"\")\n",
    "            #if (i != 0) and (word_id == vocab.START_ID):\n",
    "            if (i != 0) and (word_id == char_dict.get(\"<EOR>\")):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"once upon a time\" : -8.74\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.83\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = [\"is sentence nonsense this\", \"i drive a car\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -6.03\n",
      "\"the boy and the girl is\" : -5.90\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"the boy and the girl are\", \"the boy and the girl is\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 1. question(s)\n",
    "\n",
    "*According to the model, \"the boy and the girl are\" is more plausible, but the log probabilities of the two phrases are very close.  This is likely because \"is\" and \"are\" are used so similarly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -7.46\n",
      "\"peanuts are my favorite kind of vegetable\" : -7.29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"when I'm hungry I really prefer to eat\" : -7.49\n",
      "\"when I'm hungry I really prefer to drink\" : -7.52\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"peanuts are my favorite kind of nut\",\n",
    "         \"peanuts are my favorite kind of vegetable\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "sents = [\"when I'm hungry I really prefer to eat\",\n",
    "         \"when I'm hungry I really prefer to drink\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 2. question(s)\n",
    "\n",
    "*Of each pair, the sentences \"peanuts are my favorite kind of vegetable\" and \"when I'm hungry I really prefer to eat\" are more plausible according to the model.*\n",
    "\n",
    "*For the differing word in each pair of sentences, a 3-gram language model would use as context \"favorite kind of\" and \"really prefer to\", and it thus would not have enough context to correctly score the likely final word.*\n",
    "\n",
    "*For the differing word in each pair of sentences, a 5-gram language model would use as context \"are my favorite kind of\" and \"hungry I really prefer to\".  In the first pair of sentences, there is still not enough context to correctly score the next expected word.  However, in the second pair of sentences, a 5-gram language model would capture \"hungry\" in the context, and this may help it to score \"eat\" higher than \"drink\" as the next expected word, similarly to the RNN model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from Week 2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"I have lots of plastic green square toys\" : -8.51\n",
      "\"I have lots of green square plastic toys\" : -8.56\n",
      "\"I have lots of green plastic square toys\" : -8.57\n",
      "\"I have lots of plastic square green toys\" : -8.64\n",
      "\"I have lots of square green plastic toys\" : -8.69\n",
      "\"I have lots of square plastic green toys\" : -8.71\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = prefix + list(adjs) + [noun]\n",
    "    inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
