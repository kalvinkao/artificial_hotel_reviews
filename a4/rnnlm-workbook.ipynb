{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Recurrent Neural Network Language Model\n",
    "\n",
    "This is the \"working notebook\", with skeleton code to load and train your model, as well as run unit tests. See [rnnlm-instructions.ipynb](rnnlm-instructions.ipynb) for the main writeup.\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm_test' from '/home/kalvin_kao/artificial_hotel_reviews/a4/rnnlm_test.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters  \n",
    "\n",
    "### Questions for Part (a)\n",
    "You should use big-O notation when appropriate (i.e. computing $\\exp(\\mathbf{v})$ for a vector $\\mathbf{v}$ of length $n$ is $O(n)$ operations).  Assume for problems a(1-5) that:   \n",
    "> -- Cell is one layer,  \n",
    "> -- the embedding feature length and hidden-layer feature lengths are both H, and   \n",
    "> -- ignore at the moment batch and max_time dimensions.  \n",
    "\n",
    "1. Let $\\text{CellFunc}$ be a simple RNN cell (see async Section 5.8). Write the cell equation in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
    "<p>\n",
    "2. How many parameters are in the embedding layer? In the output layer? (By parameters, we mean total number of matrix elements across all train-able tensors. A $m \\times n$ matrix has $mn$ elements.)\n",
    "<p>\n",
    "3. How many calculations (floating point operations) are required to compute $\\hat{P}(w^{(i+1)})$ for a given *single* target word $w^{(i+1)}$, assuming $w^{(i)}$ given and $h^{(i-1)}$ already computed? How about for *all* target words?\n",
    "<p>\n",
    "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax? (*Recall that hierarchical softmax makes a series of left/right decisions using a binary classifier $P_s(\\text{right}) = \\sigma(u_s \\cdot o^{(i)} + b_s)$ at each split $s$ in the tree.*)\n",
    "<p>\n",
    "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training? (*Choose \"embedding layer\", \"recurrent layer\", or \"output layer\"*.)\n",
    "\n",
    "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers for Part (a)\n",
    "\n",
    "####  [Use the Google forms to answer a1) - a5)!]( https://docs.google.com/forms/d/e/1FAIpQLSc7kpuOzErVE_H0vMfsDvFcBHz9dkwXGSzqHUGa70QsTIC1ow/viewform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM\n",
    "\n",
    "### Aside: Shapes Review\n",
    "\n",
    "Before we start, let's review our understanding of the shapes involved in this assignment and how they manifest themselves in the TF API.\n",
    "\n",
    "As in the [instructions](rnnlm-instructions.ipynb) notebook, $w$ is a matrix of wordids with shape batch_size x max_time.  Passing this through the embedding layer, we retrieve the word embedding for each, resulting in $x$ having shape batch_size x max_time x embedding_dim.  I find it useful to draw this out on a piece of paper.  When you do, you should end up with a rectangular prism with batch_size height, max_time width and embedding_dim depth.  Many tensors in this assignment share this shape (e.g. $o$, the output from the LSTM, which represents the hidden layer going into the softmax to make a prediction at every time step in every batch).\n",
    "\n",
    "Since batch size and sentence length are only resolved when we run the graph, we construct the placeholder using \"None\" in the dimensions we don't know.  The .shape property renders these as ?s.  This should be familiar to you from batch size handling in earlier assignments, only now there are two dimensions of variable length.\n",
    "\n",
    "See the next cell for a concrete example (though in practice, we'd use a TensorFlow variable that we can train for the embeddings rather than a static array).  Notice how the shape of x_val matches the shape described earlier in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "print('wordid placeholder shape:', wordid_ph.shape)\n",
    "print('x shape:', x.shape)\n",
    "\n",
    "sess = tf.Session()\n",
    "# Two sentences, each with four words.\n",
    "wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implmenting the RNNLM\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/artificial_hotel_reviews/a4_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "cd assignment/a4\n",
    "tensorboard --logdir /tmp/w266/a4_graph --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/ and visit the \"Graphs\" tab to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  W_in:0\n",
      "Shape:  (10000, 200)\n",
      "[[-0.78748155 -0.05848813 -0.64172769 ...,  0.84141541 -0.94063973\n",
      "   0.6318059 ]\n",
      " [ 0.73271012  0.9191277   0.16166019 ..., -0.77998519  0.349545\n",
      "  -0.45933056]\n",
      " [ 0.56432152 -0.03329587  0.20995927 ..., -0.3828721   0.12824035\n",
      "  -0.13679957]\n",
      " ..., \n",
      " [-0.12901855 -0.29448342 -0.08786106 ..., -0.73254704 -0.37692833\n",
      "  -0.20111871]\n",
      " [-0.62848496 -0.50970197  0.9452734  ...,  0.99917698 -0.81992722\n",
      "  -0.37649083]\n",
      " [-0.09145284  0.00680947  0.67164588 ..., -0.92668557  0.46153927\n",
      "   0.74277234]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.02885439 -0.00585808 -0.04069098 ...,  0.00699186 -0.05404399\n",
      "   0.02202466]\n",
      " [ 0.06509807 -0.02223223  0.02077565 ..., -0.00037639  0.04783005\n",
      "  -0.00804034]\n",
      " [ 0.02742642  0.05705827 -0.03898749 ...,  0.00316557  0.06507974\n",
      "  -0.01836298]\n",
      " ..., \n",
      " [-0.00754741 -0.04928695  0.06990113 ..., -0.06414328 -0.06926721\n",
      "  -0.05963309]\n",
      " [-0.06505661 -0.0187963   0.05553678 ..., -0.00105456 -0.04487306\n",
      "   0.05926735]\n",
      " [ 0.05089028  0.03582108 -0.05539448 ...,  0.00139427  0.06008063\n",
      "   0.05134535]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.06411717 -0.059903    0.05128169 ...,  0.03041378  0.05353859\n",
      "   0.0544473 ]\n",
      " [ 0.06030189  0.02777085  0.0119326  ..., -0.00028028 -0.04805488\n",
      "  -0.03846691]\n",
      " [-0.02244554  0.04396319  0.06136966 ...,  0.02755227  0.01427022\n",
      "   0.0558702 ]\n",
      " ..., \n",
      " [-0.0622274  -0.03975505 -0.03073074 ...,  0.0241474  -0.04965901\n",
      "  -0.00885601]\n",
      " [ 0.0227494  -0.0019692  -0.05720998 ..., -0.03900233 -0.02878788\n",
      "  -0.05673516]\n",
      " [-0.04740062  0.00044262  0.0118935  ...,  0.02134912  0.04072783\n",
      "   0.00831893]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  W_out:0\n",
      "Shape:  (200, 10000)\n",
      "[[ 0.22999096 -0.98074627  0.2485311  ...,  0.31636119  0.64180779\n",
      "   0.48168755]\n",
      " [ 0.02860689  0.14025211  0.73703051 ...,  0.55039978 -0.11227965\n",
      "   0.06153202]\n",
      " [-0.74636269 -0.50103402  0.13248014 ..., -0.45557976  0.15847898\n",
      "   0.09724474]\n",
      " ..., \n",
      " [-0.21404767  0.9171977  -0.29322004 ..., -0.82013679  0.63434672\n",
      "  -0.56355882]\n",
      " [ 0.83722639 -0.26542854 -0.89726591 ..., -0.59170508  0.18550444\n",
      "   0.48118067]\n",
      " [ 0.55475283 -0.74218535 -0.24593616 ..., -0.88373137 -0.86611366\n",
      "  -0.00220013]]\n",
      "Variable:  b_out:0\n",
      "Shape:  (10000,)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    session.run(initializer)\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = session.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(\"Variable: \", k)\n",
    "        print(\"Shape: \", v.shape)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a few unit tests below to verify some *very* basic properties of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 2.256s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error messages are intentionally somewhat spare, and that passing tests are no guarantee of model correctness! Your best chance of success is through careful coding and understanding of how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 5 notebook.\n",
    "\n",
    "Particularly, `utils.rnnlm_batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:  \n",
    "*(Ignore the ugly formatter code.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to verify your implementation of `run_epoch`, and to test your RNN on a (very simple) toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 129]: seen 6500 words at 6475.5 wps, loss = 1.119\n",
      "[batch 263]: seen 13200 words at 6568.6 wps, loss = 0.815\n",
      "[batch 409]: seen 20500 words at 6811.2 wps, loss = 0.736\n",
      "[batch 557]: seen 27900 words at 6955.7 wps, loss = 0.775\n",
      "[batch 706]: seen 35350 words at 7044.9 wps, loss = 0.939\n",
      "[batch 855]: seen 42800 words at 7105.2 wps, loss = 0.973\n",
      "[batch 1001]: seen 50100 words at 7126.5 wps, loss = 0.951\n",
      "[batch 1148]: seen 57450 words at 7153.1 wps, loss = 0.937\n",
      "[batch 1294]: seen 64750 words at 7164.5 wps, loss = 0.939\n",
      "[batch 1440]: seen 72050 words at 7177.4 wps, loss = 0.933\n",
      "[batch 1584]: seen 79250 words at 7176.5 wps, loss = 0.924\n",
      "[batch 1728]: seen 86450 words at 7177.4 wps, loss = 0.911\n",
      "[batch 1874]: seen 93750 words at 7183.3 wps, loss = 0.920\n",
      "[batch 2020]: seen 101050 words at 7191.5 wps, loss = 0.931\n",
      "[batch 2166]: seen 108350 words at 7196.0 wps, loss = 0.947\n",
      "[batch 2311]: seen 115600 words at 7198.0 wps, loss = 0.953\n",
      "[batch 2454]: seen 122750 words at 7194.3 wps, loss = 0.962\n",
      "[batch 2598]: seen 129950 words at 7193.2 wps, loss = 0.966\n",
      "[batch 2737]: seen 136900 words at 7179.0 wps, loss = 0.973\n",
      "[batch 2875]: seen 143800 words at 7164.6 wps, loss = 0.990\n",
      "[batch 3023]: seen 151200 words at 7175.2 wps, loss = 0.996\n",
      "[batch 3169]: seen 158500 words at 7179.6 wps, loss = 1.000\n",
      "[batch 3315]: seen 165800 words at 7182.8 wps, loss = 1.008\n",
      "[batch 3460]: seen 173050 words at 7184.9 wps, loss = 1.023\n",
      "[batch 3606]: seen 180350 words at 7189.1 wps, loss = 1.029\n",
      "[batch 3751]: seen 187600 words at 7190.8 wps, loss = 1.034\n",
      "[batch 3892]: seen 194650 words at 7184.4 wps, loss = 1.030\n",
      "Train set: avg. loss: 0.074  (perplexity: 1.08)\n",
      "Test set: avg. loss: 0.078  (perplexity: 1.08)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 29.487s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as above, this is a *very* simple test case that does not guarantee model correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `/tmp/w266/a4_model/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in around 15 minutes on a single-core GCE instance, and reach a training set perplexity between 120-140.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Notes on Speed\n",
    "\n",
    "To speed things up, you may want to re-start your GCE instance with more CPUs. Using a 16-core machine will train *very* quickly if using a sampled softmax lost, almost as fast as a GPU. (Because of the sequential nature of the model, GPUs aren't actually much faster than CPUs for training and running RNNs.) The training code will print the words-per-second processed; with the default settings on a single core, you can expect around 8000 WPS, or up to more than 25000 WPS on a fast multi-core machine.\n",
    "\n",
    "You might also want to modify the code below to only run score_dataset at the very end, after all epochs are completed. This will speed things up significantly, since `score_dataset` uses the full softmax loss - and so often can take longer than a whole training epoch!\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "cp /tmp/w266/a4_model/rnnlm_trained* .\n",
    "git add rnnlm_trained*\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle. If you do also train a large model, please only submit the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/kalvin_kao/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  W_in:0\n",
      "Shape:  (10000, 200)\n",
      "[[-0.80276132  0.5878582   0.0499115  ...,  0.80741596 -0.88690448\n",
      "  -0.60992002]\n",
      " [-0.32976007  0.45203614 -0.51942873 ..., -0.45567822  0.36616588\n",
      "   0.39991093]\n",
      " [-0.01927447 -0.08796191  0.1803503  ..., -0.3464644   0.20432448\n",
      "  -0.54784822]\n",
      " ..., \n",
      " [-0.10561752 -0.07984304 -0.03611135 ...,  0.11877084 -0.70614171\n",
      "  -0.16387248]\n",
      " [ 0.16742086  0.63805413  0.33990169 ..., -0.28264141 -0.7318213\n",
      "  -0.65414596]\n",
      " [-0.71266985 -0.14230752  0.531353   ...,  0.68915963 -0.28666854\n",
      "  -0.91309953]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[-0.03057739 -0.05564748 -0.0686468  ..., -0.05256543 -0.06882788\n",
      "   0.01906256]\n",
      " [ 0.02572244  0.05407195  0.01415776 ...,  0.0172588  -0.02449413\n",
      "   0.06690901]\n",
      " [ 0.06485838 -0.03484184 -0.0328573  ..., -0.05020545  0.02072479\n",
      "   0.02892677]\n",
      " ..., \n",
      " [ 0.06645449 -0.0235068  -0.0297286  ..., -0.02072462  0.01551989\n",
      "  -0.03491803]\n",
      " [ 0.06673931 -0.03690427 -0.03047632 ...,  0.02812143  0.02678348\n",
      "   0.04650865]\n",
      " [ 0.03077049 -0.00355357 -0.01502657 ..., -0.0250939   0.00369895\n",
      "  -0.03867999]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[-0.02233397  0.06666537  0.06643967 ...,  0.05720081 -0.05832536\n",
      "  -0.06193139]\n",
      " [ 0.00059149  0.01443295  0.03063651 ...,  0.0543706  -0.04696034\n",
      "   0.00061975]\n",
      " [ 0.02892999  0.00332129  0.04947492 ...,  0.06584067 -0.01469939\n",
      "   0.01303742]\n",
      " ..., \n",
      " [-0.03728069 -0.04034716 -0.03407276 ...,  0.04239164  0.06663091\n",
      "  -0.05011962]\n",
      " [ 0.03159871 -0.04921903 -0.05629423 ...,  0.04925045 -0.06809358\n",
      "  -0.02097007]\n",
      " [-0.00746205 -0.05059271 -0.05653658 ...,  0.01360111 -0.06187578\n",
      "  -0.05268805]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  W_out:0\n",
      "Shape:  (200, 10000)\n",
      "[[ 0.73397827  0.13973212  0.64820647 ...,  0.57642031 -0.89331317\n",
      "   0.73256826]\n",
      " [-0.97232318  0.08740997 -0.26685905 ...,  0.69170856  0.66908383\n",
      "  -0.22922945]\n",
      " [ 0.87868953 -0.23716259 -0.51524329 ..., -0.23026848 -0.91835308\n",
      "  -0.68138051]\n",
      " ..., \n",
      " [ 0.23939443 -0.14319038 -0.7679162  ...,  0.4115088   0.38104582\n",
      "   0.61053014]\n",
      " [-0.51741791  0.03972912  0.69818926 ..., -0.59618926  0.78314662\n",
      "   0.0913403 ]\n",
      " [ 0.50618792  0.86913562 -0.82098985 ...,  0.11638713 -0.95931411\n",
      "   0.83986473]]\n",
      "Variable:  b_out:0\n",
      "Shape:  (10000,)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[epoch 1] Starting epoch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 43]: seen 110000 words at 10755.7 wps, loss = 5.849\n",
      "[batch 84]: seen 212500 words at 10490.6 wps, loss = 5.334\n",
      "[batch 126]: seen 317500 words at 10441.1 wps, loss = 5.106\n",
      "[batch 168]: seen 422500 words at 10408.6 wps, loss = 4.963\n",
      "[batch 210]: seen 527500 words at 10381.5 wps, loss = 4.858\n",
      "[batch 252]: seen 632500 words at 10365.9 wps, loss = 4.776\n",
      "[batch 293]: seen 735000 words at 10330.4 wps, loss = 4.713\n",
      "[batch 334]: seen 837500 words at 10314.6 wps, loss = 4.658\n",
      "[batch 375]: seen 940000 words at 10300.4 wps, loss = 4.613\n",
      "[epoch 1] Completed in 0:01:34\n",
      "[epoch 1] Train set: avg. loss: 5.407  (perplexity: 222.94)\n",
      "[epoch 1] Test set: avg. loss: 5.447  (perplexity: 232.00)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 40]: seen 102500 words at 10156.4 wps, loss = 4.209\n",
      "[batch 81]: seen 205000 words at 10142.1 wps, loss = 4.177\n",
      "[batch 122]: seen 307500 words at 10122.2 wps, loss = 4.156\n",
      "[batch 163]: seen 410000 words at 10126.0 wps, loss = 4.140\n",
      "[batch 204]: seen 512500 words at 10122.1 wps, loss = 4.126\n",
      "[batch 245]: seen 615000 words at 10137.7 wps, loss = 4.114\n",
      "[batch 286]: seen 717500 words at 10145.1 wps, loss = 4.102\n",
      "[batch 328]: seen 822500 words at 10163.6 wps, loss = 4.094\n",
      "[batch 370]: seen 927500 words at 10197.8 wps, loss = 4.084\n",
      "[epoch 2] Completed in 0:01:35\n",
      "[epoch 2] Train set: avg. loss: 5.210  (perplexity: 183.13)\n",
      "[epoch 2] Test set: avg. loss: 5.267  (perplexity: 193.90)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 37]: seen 95000 words at 9385.4 wps, loss = 4.012\n",
      "[batch 75]: seen 190000 words at 9360.6 wps, loss = 3.989\n",
      "[batch 114]: seen 287500 words at 9463.5 wps, loss = 3.981\n",
      "[batch 153]: seen 385000 words at 9505.1 wps, loss = 3.971\n",
      "[batch 191]: seen 480000 words at 9493.8 wps, loss = 3.963\n",
      "[batch 230]: seen 577500 words at 9530.4 wps, loss = 3.955\n",
      "[batch 269]: seen 675000 words at 9551.4 wps, loss = 3.947\n",
      "[batch 308]: seen 772500 words at 9567.1 wps, loss = 3.943\n",
      "[batch 347]: seen 870000 words at 9585.9 wps, loss = 3.940\n",
      "[batch 386]: seen 967500 words at 9599.0 wps, loss = 3.935\n",
      "[epoch 3] Completed in 0:01:41\n",
      "[epoch 3] Train set: avg. loss: 5.111  (perplexity: 165.82)\n",
      "[epoch 3] Test set: avg. loss: 5.179  (perplexity: 177.50)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "[batch 39]: seen 100000 words at 9763.8 wps, loss = 3.903\n",
      "[batch 78]: seen 197500 words at 9739.6 wps, loss = 3.888\n",
      "[batch 118]: seen 297500 words at 9749.4 wps, loss = 3.880\n",
      "[batch 158]: seen 397500 words at 9754.9 wps, loss = 3.876\n",
      "[batch 197]: seen 495000 words at 9731.4 wps, loss = 3.871\n",
      "[batch 235]: seen 590000 words at 9671.6 wps, loss = 3.867\n",
      "[batch 274]: seen 687500 words at 9650.1 wps, loss = 3.862\n",
      "[batch 313]: seen 785000 words at 9655.5 wps, loss = 3.858\n",
      "[batch 352]: seen 882500 words at 9658.9 wps, loss = 3.856\n",
      "[epoch 4] Completed in 0:01:40\n",
      "[epoch 4] Train set: avg. loss: 5.037  (perplexity: 154.08)\n",
      "[epoch 4] Test set: avg. loss: 5.119  (perplexity: 167.23)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "[batch 38]: seen 97500 words at 9680.0 wps, loss = 3.825\n",
      "[batch 77]: seen 195000 words at 9673.5 wps, loss = 3.820\n",
      "[batch 116]: seen 292500 words at 9676.3 wps, loss = 3.812\n",
      "[batch 155]: seen 390000 words at 9665.5 wps, loss = 3.809\n",
      "[batch 194]: seen 487500 words at 9667.2 wps, loss = 3.809\n",
      "[batch 233]: seen 585000 words at 9657.5 wps, loss = 3.806\n",
      "[batch 272]: seen 682500 words at 9652.8 wps, loss = 3.803\n",
      "[batch 311]: seen 780000 words at 9661.9 wps, loss = 3.801\n",
      "[batch 350]: seen 877500 words at 9658.7 wps, loss = 3.801\n",
      "[epoch 5] Completed in 0:01:40\n",
      "[epoch 5] Train set: avg. loss: 4.983  (perplexity: 145.89)\n",
      "[epoch 5] Test set: avg. loss: 5.074  (perplexity: 159.88)\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "[batch 37]: seen 95000 words at 9326.0 wps, loss = 3.767\n",
      "[batch 75]: seen 190000 words at 9412.0 wps, loss = 3.764\n",
      "[batch 114]: seen 287500 words at 9502.7 wps, loss = 3.760\n",
      "[batch 153]: seen 385000 words at 9532.3 wps, loss = 3.758\n",
      "[batch 192]: seen 482500 words at 9553.5 wps, loss = 3.756\n",
      "[batch 231]: seen 580000 words at 9571.3 wps, loss = 3.753\n",
      "[batch 270]: seen 677500 words at 9594.1 wps, loss = 3.752\n",
      "[batch 309]: seen 775000 words at 9612.5 wps, loss = 3.751\n",
      "[batch 348]: seen 872500 words at 9617.2 wps, loss = 3.751\n",
      "[batch 387]: seen 969900 words at 9626.0 wps, loss = 3.751\n",
      "[epoch 6] Completed in 0:01:40\n",
      "[epoch 6] Train set: avg. loss: 4.945  (perplexity: 140.50)\n",
      "[epoch 6] Test set: avg. loss: 5.045  (perplexity: 155.25)\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "[batch 38]: seen 97500 words at 9724.9 wps, loss = 3.743\n",
      "[batch 77]: seen 195000 words at 9698.1 wps, loss = 3.733\n",
      "[batch 116]: seen 292500 words at 9702.8 wps, loss = 3.728\n",
      "[batch 155]: seen 390000 words at 9699.7 wps, loss = 3.726\n",
      "[batch 194]: seen 487500 words at 9696.9 wps, loss = 3.724\n",
      "[batch 233]: seen 585000 words at 9696.5 wps, loss = 3.724\n",
      "[batch 272]: seen 682500 words at 9688.8 wps, loss = 3.721\n",
      "[batch 311]: seen 780000 words at 9688.7 wps, loss = 3.720\n",
      "[batch 350]: seen 877500 words at 9692.4 wps, loss = 3.719\n",
      "[epoch 7] Completed in 0:01:40\n",
      "[epoch 7] Train set: avg. loss: 4.899  (perplexity: 134.10)\n",
      "[epoch 7] Test set: avg. loss: 5.012  (perplexity: 150.18)\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "[batch 38]: seen 97500 words at 9570.1 wps, loss = 3.706\n",
      "[batch 76]: seen 192500 words at 9518.6 wps, loss = 3.700\n",
      "[batch 115]: seen 290000 words at 9535.6 wps, loss = 3.695\n",
      "[batch 153]: seen 385000 words at 9511.5 wps, loss = 3.694\n",
      "[batch 192]: seen 482500 words at 9525.5 wps, loss = 3.694\n",
      "[batch 231]: seen 580000 words at 9547.4 wps, loss = 3.691\n",
      "[batch 270]: seen 677500 words at 9558.1 wps, loss = 3.689\n",
      "[batch 309]: seen 775000 words at 9564.0 wps, loss = 3.688\n",
      "[batch 348]: seen 872500 words at 9568.9 wps, loss = 3.688\n",
      "[batch 387]: seen 969900 words at 9570.2 wps, loss = 3.688\n",
      "[epoch 8] Completed in 0:01:41\n",
      "[epoch 8] Train set: avg. loss: 4.873  (perplexity: 130.72)\n",
      "[epoch 8] Test set: avg. loss: 4.996  (perplexity: 147.83)\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "[batch 36]: seen 92500 words at 9247.6 wps, loss = 3.691\n",
      "[batch 73]: seen 185000 words at 9129.9 wps, loss = 3.681\n",
      "[batch 109]: seen 275000 words at 9047.6 wps, loss = 3.673\n",
      "[batch 145]: seen 365000 words at 9024.6 wps, loss = 3.674\n",
      "[batch 181]: seen 455000 words at 8995.5 wps, loss = 3.672\n",
      "[batch 217]: seen 545000 words at 8975.6 wps, loss = 3.669\n",
      "[batch 253]: seen 635000 words at 8973.8 wps, loss = 3.667\n",
      "[batch 289]: seen 725000 words at 8971.9 wps, loss = 3.667\n",
      "[batch 326]: seen 817500 words at 9002.2 wps, loss = 3.667\n",
      "[batch 364]: seen 912500 words at 9039.6 wps, loss = 3.667\n",
      "[epoch 9] Completed in 0:01:46\n",
      "[epoch 9] Train set: avg. loss: 4.843  (perplexity: 126.80)\n",
      "[epoch 9] Test set: avg. loss: 4.975  (perplexity: 144.78)\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "[batch 38]: seen 97500 words at 9721.8 wps, loss = 3.663\n",
      "[batch 78]: seen 197500 words at 9736.1 wps, loss = 3.655\n",
      "[batch 117]: seen 295000 words at 9718.1 wps, loss = 3.650\n",
      "[batch 156]: seen 392500 words at 9698.7 wps, loss = 3.647\n",
      "[batch 195]: seen 490000 words at 9700.1 wps, loss = 3.647\n",
      "[batch 234]: seen 587500 words at 9698.5 wps, loss = 3.646\n",
      "[batch 273]: seen 685000 words at 9698.0 wps, loss = 3.644\n",
      "[batch 312]: seen 782500 words at 9701.5 wps, loss = 3.645\n",
      "[batch 351]: seen 880000 words at 9700.6 wps, loss = 3.645\n",
      "[epoch 10] Completed in 0:01:40\n",
      "[epoch 10] Train set: avg. loss: 4.815  (perplexity: 123.36)\n",
      "[epoch 10] Test set: avg. loss: 4.960  (perplexity: 142.62)\n",
      "\n",
      "Variable:  W_in:0\n",
      "Shape:  (10000, 200)\n",
      "[[-0.20832227 -0.21165472  0.46515125 ...,  0.56737316 -1.05009151\n",
      "  -0.2466348 ]\n",
      " [-0.32976007  0.45203614 -0.51942873 ..., -0.45567822  0.36616588\n",
      "   0.39991093]\n",
      " [-0.171213    0.04363073  0.02342075 ..., -0.11068165  0.12092681\n",
      "   0.0336319 ]\n",
      " ..., \n",
      " [-0.39467013 -0.18399377  0.47717234 ...,  0.52532274  0.49662998\n",
      "  -0.06885928]\n",
      " [ 0.51094437  0.76494408  1.10761082 ..., -0.424494   -0.98394305\n",
      "  -0.27540061]\n",
      " [-0.20641214  0.8980689   0.52067041 ...,  1.21862876  1.05035925\n",
      "  -0.58915347]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.25529417 -0.74134457 -0.19717969 ..., -0.34318525  0.69871861\n",
      "   0.58489573]\n",
      " [-0.1437117  -0.13189381 -0.67979717 ..., -0.00304972  0.13299643\n",
      "   0.14801437]\n",
      " [ 0.28190678  0.06527971  0.76296771 ..., -0.14434956  0.33360025\n",
      "  -0.19380529]\n",
      " ..., \n",
      " [ 0.09265636 -0.18768583 -1.13955319 ..., -0.49444982 -0.35670626\n",
      "   0.23523869]\n",
      " [ 0.5876413   0.51756346  0.99408996 ...,  0.79483497  0.00549489\n",
      "  -0.31256106]\n",
      " [ 1.22581327  1.06723416  0.4303692  ...,  0.77797443  0.42874205\n",
      "  -0.50756311]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.47235346e+00  -1.90241468e+00  -3.03158498e+00  -4.60217047e+00\n",
      "  -2.09379625e+00  -3.55579782e+00  -3.04777741e+00  -2.81217718e+00\n",
      "  -2.03463316e+00  -2.68180132e+00  -2.64531159e+00  -2.72717929e+00\n",
      "  -2.19901443e+00  -2.38726163e+00  -2.85214496e+00  -1.72429049e+00\n",
      "  -3.21454215e+00  -3.05422592e+00  -2.51715875e+00  -2.90490460e+00\n",
      "  -1.31377256e+00  -3.06525183e+00  -1.06785023e+00  -2.25877833e+00\n",
      "  -2.48843241e+00  -2.58417344e+00  -2.22993159e+00  -2.60263419e+00\n",
      "  -1.68799317e+00  -3.42655706e+00  -2.78653145e+00  -3.29856420e+00\n",
      "  -2.39915442e+00  -2.46503139e+00  -3.10096574e+00  -2.21951675e+00\n",
      "  -2.75574660e+00  -3.15038824e+00  -2.95044398e+00  -2.83441067e+00\n",
      "  -3.08357096e+00  -2.55567169e+00  -2.57328963e+00  -1.63017058e+00\n",
      "  -2.30995011e+00  -2.22349453e+00  -1.25014174e+00  -2.66954207e+00\n",
      "  -2.27409387e+00  -2.38635492e+00  -1.48846328e+00  -2.11288500e+00\n",
      "  -2.43061996e+00  -3.08872795e+00  -2.68165207e+00  -2.34161377e+00\n",
      "  -2.00905347e+00  -4.00311089e+00  -2.20975780e+00  -1.90340209e+00\n",
      "  -2.27403164e+00  -2.88552213e+00  -1.69739366e+00  -3.40792561e+00\n",
      "  -2.17740226e+00  -1.58883464e+00  -3.18201947e+00  -1.66696227e+00\n",
      "  -2.10738564e+00  -2.50783658e+00  -3.38958788e+00  -2.91872001e+00\n",
      "  -3.31322694e+00  -1.86979377e+00  -3.43482733e+00  -1.95652032e+00\n",
      "  -2.67367768e+00  -2.10787654e+00  -2.67853403e+00  -2.19423175e+00\n",
      "  -2.64751244e+00  -2.18494773e+00  -2.54319668e+00  -2.61251688e+00\n",
      "  -2.44737196e+00  -2.55188251e+00  -2.26702857e+00  -2.47797871e+00\n",
      "  -2.42606044e+00  -2.59057164e+00  -9.99668777e-01  -2.46475887e+00\n",
      "  -2.71020555e+00  -1.80783975e+00  -2.74350882e+00  -2.34330106e+00\n",
      "   4.40800071e-01  -1.45177972e+00  -2.97359300e+00  -2.17744923e+00\n",
      "  -2.78105021e+00  -2.78698850e+00  -2.95265365e+00  -2.51969671e+00\n",
      "  -1.61813426e+00  -2.74372625e+00  -3.16058731e+00  -2.33404422e+00\n",
      "  -2.82172942e+00  -2.02492595e+00  -2.79361796e+00  -2.92575002e+00\n",
      "  -2.44248605e+00  -2.38587141e+00  -1.29715466e+00  -2.90204644e+00\n",
      "  -2.23152304e+00  -1.81583333e+00  -1.42844629e+00  -2.85600090e+00\n",
      "  -2.41010761e+00  -2.85984516e+00  -1.20995438e+00  -3.04415894e+00\n",
      "  -1.46152020e+00   6.08216763e-01  -2.68006778e+00  -2.49353242e+00\n",
      "  -2.82149649e+00  -2.55564690e+00  -1.38232589e+00  -1.34179890e+00\n",
      "  -2.81227922e+00  -1.50850165e+00  -2.81931376e+00  -4.61126947e+00\n",
      "  -1.78143954e+00  -1.75366008e+00  -3.29290032e+00  -2.85296345e+00\n",
      "  -1.69495654e+00  -2.61994123e+00  -2.36406541e+00  -2.40572906e+00\n",
      "  -2.39363027e+00  -2.58342981e+00  -2.97333217e+00  -4.28018570e+00\n",
      "  -8.67613912e-01  -1.72002423e+00  -2.70195150e+00  -1.78627646e+00\n",
      "  -2.43906093e+00  -2.21582317e+00  -1.76058531e+00  -3.05567741e+00\n",
      "  -2.57355094e+00  -2.11422729e+00  -1.81093872e+00  -2.15012312e+00\n",
      "  -2.72276044e+00  -4.64293432e+00  -2.68250060e+00  -8.54402542e-01\n",
      "  -2.85474420e+00  -3.43853736e+00  -3.24213505e+00  -1.64930940e+00\n",
      "  -2.07252860e+00  -2.54951000e+00  -2.04908538e+00  -2.48466873e+00\n",
      "  -2.02443957e+00  -2.92975593e+00  -1.97574794e+00  -3.30379176e+00\n",
      "  -2.82248998e+00  -2.71938252e+00  -2.30903172e+00  -2.51939154e+00\n",
      "  -1.46837103e+00  -2.65867567e+00  -2.53982854e+00  -2.62559605e+00\n",
      "  -2.31790042e+00  -1.08911932e+00  -3.39266896e+00  -2.65215635e+00\n",
      "  -2.84676838e+00  -2.35444832e+00  -3.89572239e+00  -2.36444640e+00\n",
      "  -2.07661033e+00  -4.48984337e+00  -2.64450479e+00  -1.59427023e+00\n",
      "  -2.31218314e+00  -2.38702011e+00  -2.16171932e+00  -3.33466244e+00\n",
      "   2.08045691e-02  -5.70935547e-01   3.81610274e-01  -3.44480475e-04\n",
      "  -5.56449652e-01  -2.40195602e-01   1.15531594e-01   2.60532588e-01\n",
      "  -2.81801410e-02   4.71033245e-01   6.61972880e-01  -2.68950045e-01\n",
      "   5.50933659e-01   7.58187354e-01   1.28745005e-01   1.92368954e-01\n",
      "   5.48773527e-01   1.89029098e-01   4.34736609e-01   1.60928711e-01\n",
      "  -9.05690014e-01  -6.51669562e-01  -1.04207957e+00   4.54336196e-01\n",
      "  -6.03394389e-01   1.92437068e-01  -2.61361860e-02  -4.75029111e-01\n",
      "   1.04491949e+00   1.32195905e-01   4.30601329e-01   4.21742797e-02\n",
      "  -1.43908024e-01   3.25256556e-01   2.18922824e-01  -4.59062308e-01\n",
      "  -2.53629535e-01   1.38518721e-01  -7.54388690e-01   2.82628238e-01\n",
      "   5.36809266e-01  -2.39927575e-01   1.61777362e-01  -1.37867108e-02\n",
      "   2.30582759e-01  -3.48521382e-01   1.02014494e+00  -2.48758435e-01\n",
      "  -1.51701510e-01   1.91410944e-01   7.75827408e-01   2.35589221e-01\n",
      "  -2.97896057e-01  -1.51757568e-01   4.26139414e-01  -1.86533660e-01\n",
      "   1.90159813e-01   2.72496760e-01  -8.92106593e-02   2.06624672e-01\n",
      "   2.52058476e-01  -2.35430121e-01   3.72991621e-01  -1.90428942e-01\n",
      "   2.23885223e-01  -1.68745324e-01  -5.38844943e-01  -6.59410477e-01\n",
      "  -1.69561520e-01   2.09794715e-01   3.00196081e-01   6.15734339e-01\n",
      "   2.16402009e-01  -7.08609223e-01   5.42680919e-01  -3.25138003e-01\n",
      "  -3.08849871e-01  -3.30292374e-01  -2.86965430e-01  -2.49477461e-01\n",
      "   4.29605514e-01  -8.19194078e-01  -1.36367366e-01  -3.46850723e-01\n",
      "  -6.73444331e-01   6.56706467e-02   6.98205829e-01  -3.61302048e-01\n",
      "   2.28034511e-01  -1.39582038e-01  -2.44056106e-01  -4.03733224e-01\n",
      "   5.56030944e-02  -6.37837648e-01  -7.33012736e-01  -1.00239241e+00\n",
      "  -9.18428540e-01   1.07931936e+00  -5.78221306e-02  -1.58507213e-01\n",
      "   4.42426115e-01  -3.98466200e-01   6.07647479e-01   6.19168043e-01\n",
      "  -7.48107255e-01  -1.42739728e-01  -4.68259037e-01   2.96239704e-01\n",
      "  -8.68239775e-02   2.77112722e-01  -3.87607105e-02   4.39405769e-01\n",
      "   1.68584600e-01   3.49161237e-01  -1.25816023e+00   4.26949292e-01\n",
      "   6.65576532e-02   9.36568618e-01  -1.12746239e+00  -1.55692577e-01\n",
      "  -3.24376464e-01   2.06335261e-01  -1.01382995e+00  -4.86272633e-01\n",
      "   3.20244402e-01   1.82454562e+00  -1.15611076e-01  -8.11571002e-01\n",
      "  -3.15180123e-01   3.80177908e-02  -8.98338675e-01  -1.66241407e-01\n",
      "  -2.60387599e-01   4.60112207e-02   4.85912532e-01  -1.13644972e-01\n",
      "  -1.08525133e+00   5.83240807e-01  -2.42784008e-01   7.48974442e-01\n",
      "   7.75938809e-01   6.25028014e-02  -2.59313464e-01   4.04275745e-01\n",
      "   7.21383870e-01   5.67089319e-01  -1.15024090e-01   9.09775868e-02\n",
      "   4.29397672e-01   1.18849230e+00   3.62315625e-01  -4.93504047e-01\n",
      "  -8.34568679e-01  -1.05052553e-02  -8.29461813e-01  -1.84559956e-01\n",
      "  -7.65507936e-01   2.85398871e-01  -8.40879202e-01   5.48330188e-01\n",
      "   1.47013292e-01  -2.74958700e-01   3.15145940e-01  -5.46373487e-01\n",
      "  -4.47259694e-02   1.48882911e-01  -4.36989078e-03   8.49068224e-01\n",
      "   1.74752668e-01   6.24003649e-01   1.54286534e-01  -1.97949991e-01\n",
      "  -1.88017085e-01  -3.03591728e-01   4.29152846e-01  -2.04496592e-01\n",
      "   5.39900124e-01   1.47356600e-01  -3.56233835e-01  -5.87736368e-01\n",
      "   5.92067420e-01  -1.99673519e-01  -4.83431756e-01  -7.94298276e-02\n",
      "  -4.99628127e-01   4.14645165e-01  -2.62767851e-01  -3.46946925e-01\n",
      "  -7.19434842e-02  -1.92118227e-01  -3.24989334e-02   3.00027877e-01\n",
      "  -1.17712617e-01   2.65592843e-01   2.96948373e-01   7.61182010e-01\n",
      "   9.27448273e-02   3.65686625e-01  -8.48796844e-01  -3.51273745e-01\n",
      "  -1.41828835e+00  -1.33320034e+00  -1.08863151e+00   1.27353930e+00\n",
      "  -5.15111148e-01  -8.10439825e-01  -1.86901784e+00  -2.05639482e+00\n",
      "  -1.67048681e+00  -1.84239018e+00  -8.31925929e-01  -1.23892200e+00\n",
      "  -1.19797075e+00  -1.38880384e+00  -1.75384474e+00  -1.82799613e+00\n",
      "  -9.97468114e-01  -8.75514328e-01  -1.74301028e+00  -1.14542246e+00\n",
      "  -9.36556876e-01  -1.49113333e+00  -1.52666807e+00  -1.71257627e+00\n",
      "  -3.69878411e-01  -1.73707712e+00  -1.59866750e+00  -1.65846407e+00\n",
      "  -9.08418477e-01  -1.08271897e+00  -1.60151184e+00  -9.91763651e-01\n",
      "  -2.09813595e+00  -1.89121366e+00  -2.44821954e+00  -1.45675361e+00\n",
      "  -1.36451328e+00  -1.46191704e+00  -1.60023093e+00  -2.11159801e+00\n",
      "  -1.26222551e+00  -1.91996670e+00  -1.56909728e+00  -1.74989903e+00\n",
      "  -1.64778900e+00  -1.69446933e+00  -1.49509251e+00  -3.37163001e-01\n",
      "  -1.92246389e+00  -1.71280074e+00  -1.96555793e+00  -2.40065026e+00\n",
      "  -1.24215531e+00  -1.45209074e+00  -1.81226051e+00  -1.13492632e+00\n",
      "  -2.33307433e+00  -1.23047900e+00  -1.47385371e+00  -1.52204406e+00\n",
      "  -1.29910123e+00  -1.50713742e+00  -1.22550333e+00  -1.52402520e+00\n",
      "  -1.24629140e+00  -1.76014316e+00  -1.31843293e+00  -1.90105522e+00\n",
      "  -1.96525669e+00  -1.91244626e+00  -1.82104802e+00  -8.60768080e-01\n",
      "  -1.35324633e+00  -1.42693651e+00  -2.63611555e-01  -2.27688622e+00\n",
      "  -1.69311190e+00  -1.28175116e+00  -1.60618937e+00  -1.17325580e+00\n",
      "  -1.54995012e+00  -1.16995358e+00  -1.94244742e+00  -1.68264389e+00\n",
      "  -1.06046653e+00  -1.84607399e+00  -1.05757797e+00  -1.50747764e+00\n",
      "  -1.96701360e+00  -8.93847346e-01  -1.32578254e+00  -9.88362968e-01\n",
      "  -1.92367113e+00  -1.17951000e+00  -3.39954406e-01  -5.63958585e-01\n",
      "  -1.01612949e+00  -6.06012523e-01  -1.79125667e+00  -1.87648749e+00\n",
      "  -5.91725230e-01  -1.35754287e+00  -1.33094037e+00  -1.03878498e+00\n",
      "  -8.12394917e-01  -1.01825786e+00  -7.73330092e-01  -1.46593976e+00\n",
      "  -1.56139147e+00  -1.38648224e+00  -8.93979371e-01  -1.14462614e+00\n",
      "  -1.66014791e+00  -1.31378090e+00  -4.43590939e-01  -1.60769355e+00\n",
      "  -1.66186881e+00  -7.16079712e-01  -5.62373817e-01  -1.52249944e+00\n",
      "  -1.62863314e+00  -1.62823832e+00  -1.00275147e+00  -1.47406149e+00\n",
      "  -1.49944627e+00  -8.97830307e-01  -2.38218689e+00  -5.06794751e-01\n",
      "  -1.13009286e+00  -1.66612256e+00  -9.34168875e-01  -2.13496327e+00\n",
      "  -1.51020658e+00  -2.09747124e+00  -9.64151740e-01   5.89431703e-01\n",
      "  -9.52723145e-01  -1.53813994e+00  -2.36609149e+00   1.31635025e-01\n",
      "  -1.12303030e+00  -1.51484072e+00  -1.97514236e+00  -1.31753874e+00\n",
      "  -3.71032506e-01  -9.18388367e-01  -1.65951574e+00  -1.68630809e-01\n",
      "  -1.30385184e+00   1.89247832e-01  -1.51033235e+00  -1.66318190e+00\n",
      "  -9.06174004e-01  -2.43821692e+00  -1.28633082e+00  -1.46630871e+00\n",
      "  -8.20106685e-01  -1.64999807e+00  -1.58106375e+00  -1.37635005e+00\n",
      "  -1.34864438e+00   2.21141145e-01  -9.14696813e-01  -1.66822338e+00\n",
      "  -1.80604899e+00  -1.24680257e+00  -1.88558435e+00  -1.63377547e+00\n",
      "  -1.45606828e+00  -1.25249505e-01  -1.80083251e+00  -1.45348001e+00\n",
      "  -1.42969728e+00  -2.04593062e+00  -1.54606891e+00  -7.43859828e-01\n",
      "  -1.89489794e+00  -1.59469378e+00  -1.33681905e+00  -1.37244558e+00\n",
      "  -1.15804684e+00  -2.32544184e+00  -1.64340448e+00  -1.75944114e+00\n",
      "  -1.01716363e+00  -1.57255590e+00  -4.92816806e-01  -1.47059751e+00\n",
      "  -2.17847419e+00  -1.11758876e+00   3.03860188e-01  -1.91754293e+00\n",
      "  -1.72927666e+00   7.72563666e-02  -7.38504708e-01  -1.78736115e+00\n",
      "  -1.48747587e+00  -1.88458669e+00  -1.39410257e+00  -1.86321151e+00\n",
      "  -2.46106148e+00  -2.36139321e+00  -2.13692451e+00  -2.94909775e-01\n",
      "  -1.82715452e+00  -1.61537051e+00  -2.15772867e+00  -2.37257290e+00\n",
      "  -2.29761338e+00  -1.69632232e+00  -2.33065629e+00  -1.95860100e+00\n",
      "  -2.39289522e+00  -1.67477846e+00  -2.35803556e+00  -2.58432627e+00\n",
      "  -3.34969640e+00  -2.18571019e+00  -2.39873576e+00  -2.28591704e+00\n",
      "  -1.84808111e+00  -2.59694576e+00  -1.36062753e+00  -2.33242583e+00\n",
      "  -2.28657126e+00  -2.48156929e+00  -2.77386737e+00  -2.86462307e+00\n",
      "  -1.75437188e+00  -2.34490323e+00  -2.66017914e+00  -2.43083286e+00\n",
      "  -2.46847916e+00  -2.39063358e+00  -2.46282387e+00  -2.05647683e+00\n",
      "  -2.56108570e+00  -2.57945871e+00  -2.47988534e+00  -2.91200733e+00\n",
      "  -1.98889744e+00  -2.11009049e+00  -2.76515818e+00  -1.88227177e+00\n",
      "  -3.11019874e+00  -2.20663452e+00  -5.13567448e-01  -1.90426767e+00\n",
      "  -2.91182995e+00  -2.89198613e+00  -1.92201316e+00  -2.34189153e+00\n",
      "  -3.12465692e+00  -2.08985686e+00  -2.28001118e+00  -2.14140034e+00\n",
      "  -2.62878346e+00  -2.39797068e+00  -2.47842073e+00  -1.95605421e+00\n",
      "  -2.76367378e+00  -2.25392294e+00  -2.75047779e+00  -1.82768488e+00\n",
      "  -2.27793455e+00  -2.74242473e+00  -2.76432896e+00  -1.57641709e+00\n",
      "  -3.10891533e+00  -2.46470571e+00  -2.44749427e+00  -2.05968666e+00\n",
      "  -1.67098761e+00  -2.05032539e+00  -9.66237605e-01  -2.18091011e+00\n",
      "  -2.49824452e+00  -1.62029397e+00  -3.24328709e+00  -3.00673771e+00\n",
      "  -1.88484073e+00  -2.24869585e+00  -2.31298327e+00  -2.16155291e+00\n",
      "  -1.20756507e+00  -2.70381165e+00  -2.71077251e+00  -2.21024394e+00\n",
      "  -3.08636880e+00  -2.52220130e+00  -2.61437654e+00  -2.97796392e+00\n",
      "  -2.77238822e+00  -1.91270459e+00  -1.69957149e+00  -1.51159978e+00\n",
      "  -8.83536041e-01  -1.64814770e+00  -2.01589274e+00  -2.49564886e+00\n",
      "  -2.39660144e+00  -1.94299901e+00  -2.33361697e+00  -2.39413261e+00\n",
      "  -1.86470544e+00  -2.36139011e+00  -2.18864131e+00  -1.98679376e+00\n",
      "  -2.23961997e+00  -3.40008497e+00  -2.52123857e+00  -1.77510345e+00\n",
      "  -2.84349179e+00  -2.33087587e+00  -1.77846861e+00  -2.17770600e+00\n",
      "  -2.92091751e+00  -1.98366964e+00  -1.86376655e+00  -2.10642433e+00\n",
      "  -2.88577509e+00  -2.37125278e+00  -1.62064028e+00  -1.98299587e+00\n",
      "  -2.41824770e+00   3.10514599e-01  -2.62967658e+00  -2.26959252e+00\n",
      "  -1.77429128e+00  -2.87854981e+00  -1.09973955e+00  -2.59293795e+00\n",
      "  -2.64677620e+00  -2.53732729e+00  -2.49384785e+00  -3.80193233e-01\n",
      "  -2.05060673e+00  -2.13086486e+00  -2.38771319e+00  -8.00567448e-01\n",
      "  -2.31057358e+00  -2.58268499e+00  -2.35932946e+00  -2.30816865e+00\n",
      "  -2.07149673e+00  -1.76325524e+00  -2.62291598e+00  -8.87576997e-01\n",
      "  -2.36932254e+00  -1.82106733e+00  -2.23921013e+00  -2.37977529e+00\n",
      "  -1.21260989e+00  -2.33183694e+00  -2.26542735e+00  -2.25720501e+00\n",
      "  -2.87945271e+00  -2.47735453e+00  -2.21595335e+00  -3.02964926e+00\n",
      "  -2.61254883e+00  -1.52069592e+00  -1.91354728e+00  -1.88867867e+00\n",
      "  -2.88740754e+00  -2.50958514e+00  -2.42497396e+00  -1.58526468e+00\n",
      "  -3.07157421e+00  -1.15081418e+00  -2.11711025e+00  -2.87226701e+00\n",
      "  -2.35391283e+00  -2.91519213e+00  -2.66314220e+00  -1.81262124e+00\n",
      "  -2.79616928e+00  -2.76668406e+00  -2.15129781e+00  -2.18226027e+00\n",
      "  -1.27408588e+00  -2.45849895e+00  -2.22746158e+00  -2.45540595e+00\n",
      "  -2.56595111e+00  -2.55717039e+00  -1.33034742e+00  -2.28337598e+00\n",
      "  -2.37220764e+00  -2.15730929e+00  -7.35923409e-01  -2.29535365e+00\n",
      "  -2.13297772e+00  -2.38155156e-01  -2.32426524e+00  -2.07146811e+00\n",
      "  -2.43483233e+00  -3.02697754e+00  -1.75258183e+00  -2.07160640e+00]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[-0.03242516 -0.09097625  0.19739795 ...,  0.24999718 -0.12631275\n",
      "  -0.04201846]\n",
      " [ 0.4802303   0.13579495  0.57455164 ...,  0.14222631  0.06450776\n",
      "   0.10557505]\n",
      " [ 0.08086046 -0.24842496 -0.06691248 ..., -0.09734213 -0.0674372\n",
      "  -0.33212224]\n",
      " ..., \n",
      " [ 0.8013345   0.32409424  0.13307333 ...,  0.51160568  0.44547853\n",
      "   0.2158087 ]\n",
      " [-0.70946568 -0.53372216  0.28305349 ..., -0.04148354 -0.56221116\n",
      "  -0.13011432]\n",
      " [ 1.05582607  0.21959126  0.06517811 ...,  0.26711893 -0.00752871\n",
      "   0.25630891]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[-1.38885832 -1.17949617 -0.96485668 -1.3074007  -1.00466049 -0.85438526\n",
      " -0.82768869 -0.64782256 -1.22806251 -1.16284239 -0.56887555 -0.5544365\n",
      " -0.72855967 -0.71636027 -0.76582259 -1.04889822 -1.02695596 -0.61728114\n",
      " -1.27139068 -0.97110027 -1.04903376 -0.97551572 -1.07360005 -0.87961709\n",
      " -1.49909675 -0.73079813 -0.99246788 -0.84219521 -1.30249369 -0.37937447\n",
      " -0.74204701 -0.50576454 -1.16057348 -1.05268562 -1.52737868 -1.26373577\n",
      " -1.52701473 -0.67573863 -1.15154219 -1.43432236 -0.84098977 -1.15545535\n",
      " -1.16964555 -0.69582856 -0.82739979 -1.08470511 -0.8542577  -0.64905971\n",
      " -1.21297121 -0.83108938 -0.69188219 -0.89797378 -1.23341513 -1.45886838\n",
      " -0.55421716 -1.31236875 -0.41268629 -0.97898728 -0.68972725 -1.73017335\n",
      " -0.86754835 -0.95733362 -1.00188994 -1.10986531 -0.95482963 -0.54365432\n",
      " -0.83562732 -1.65668416 -0.99146193 -1.13830411 -0.55386907 -0.88509613\n",
      " -1.08738506 -1.00036633 -1.69394028 -1.03184283 -1.2721864  -1.48574901\n",
      " -1.43602729 -1.36949384 -1.28719807 -0.41253158 -1.75108278 -0.75320065\n",
      " -0.69447601 -1.21753073 -0.84466189 -1.24423838 -0.66468132 -0.64322865\n",
      " -0.884583   -1.19509685 -0.8585934  -1.10138834 -1.0403235  -1.64716017\n",
      " -0.97352725 -1.44979501 -0.34575143 -0.80013275 -0.46367082 -1.19013774\n",
      " -0.93750864 -0.3289859  -0.67270583 -0.99616587 -1.06791425 -0.83659947\n",
      " -0.71181047 -0.90112007 -1.20529342 -0.98827052 -1.31822479 -0.88645637\n",
      " -0.98427886 -1.52914894 -1.32987106 -1.00441849 -1.17324841 -0.93033564\n",
      " -0.77404153 -0.76693273 -1.24621212 -1.29554617 -1.35501277 -0.74218291\n",
      " -0.93616635 -1.70975232 -1.18639028 -0.81054634 -0.89595753 -0.9794479\n",
      " -1.0954529  -1.188959   -1.30883884 -1.24594378 -0.35864845 -1.12145174\n",
      " -0.61450374 -1.48134553 -0.4291485  -1.00221479 -1.05362511 -1.22528493\n",
      " -0.58124739 -0.81332082 -1.30863357 -0.67220783 -0.62678415 -1.11668527\n",
      " -0.8696025  -0.63204694 -0.78045499 -1.2738061  -1.17657614 -1.04567027\n",
      " -0.46498302 -0.97527736 -1.40683317 -0.8643775  -0.82944924 -0.94593132\n",
      " -1.58100665 -1.17761981 -0.8840754  -0.85197854 -0.45927939 -1.02470696\n",
      " -1.18874061 -1.1311599  -0.5709464  -0.21230623 -1.26446772 -0.32215783\n",
      " -0.208018   -0.87253988 -0.96691352 -1.40234351 -0.80975205 -1.01032805\n",
      " -0.65937114 -1.89057505 -1.0168494  -1.31873083 -1.33820283 -1.24145985\n",
      " -1.23296952 -1.38892806 -1.01527941 -0.64630586 -0.8848654  -1.55891037\n",
      " -1.22504187 -0.8489778  -0.53836668 -1.33466601 -0.84527886 -1.12553656\n",
      " -0.97938728 -0.804712   -0.47284546 -0.41255862  0.21558213 -0.49593228\n",
      " -0.21930093 -0.36824253 -0.08764596  0.11396907  0.17897245  0.30206946\n",
      " -0.20513777 -0.26105896 -0.25921857 -0.27512637  0.12853678 -0.42926055\n",
      "  0.17404318 -0.13462445  0.31913465 -0.07039332  0.06447887  0.00878719\n",
      " -0.20950115 -0.11692447  0.02865422  0.33928612 -0.0520496  -0.04123013\n",
      " -0.42525813  0.31142479 -0.24545324 -0.29450363 -0.37940633  0.00941213\n",
      " -0.20666346 -0.29010433 -0.0198283  -0.06322135  0.04779989 -0.21666013\n",
      "  0.26427031 -0.13778935 -0.16773401 -0.35066223  0.24475071 -0.14629975\n",
      "  0.3432661   0.12253846  0.13114338  0.19149691 -0.01939753 -0.14126629\n",
      " -0.31218627 -0.1984358  -0.49318847 -0.22223255 -0.05226941 -0.15514538\n",
      "  0.11156082 -0.20616929 -0.13923109 -0.07588762  0.21655683  0.30555397\n",
      "  0.05685889  0.20708346 -0.24402843 -0.05995354  0.01838849  0.11369556\n",
      "  0.27951178  0.19317251 -0.11645799 -0.15754662  0.26764849  0.36423966\n",
      " -0.38160548 -0.12431634  0.34947556  0.26037896  0.24719675 -0.21663207\n",
      "  0.07405283  0.30806643  0.10085044 -0.37564272 -0.10590003  0.07714954\n",
      " -0.34317437  0.04429065  0.05342088 -0.03595554  0.15537448  0.01655071\n",
      " -0.07207008 -0.1308555   0.16481514 -0.11407213  0.05371581  0.38451126\n",
      "  0.28240064 -0.41435757  0.60204458 -0.21615377  0.1535608   0.09831343\n",
      " -0.18504548  0.04249481 -0.47870144  0.22872546 -0.06310173  0.09791819\n",
      "  0.34444594 -0.2664665  -0.21616122 -0.041277   -0.27079341 -0.1514315\n",
      " -0.36659524 -0.16485594  0.21400116  0.16548222  0.40584818  0.33447012\n",
      " -0.27646866  0.17195621 -0.11337689  0.26114592  0.16725619 -0.23172998\n",
      "  0.33459336 -0.4442893  -0.22292082 -0.24248894  0.17561825  0.15150526\n",
      "  0.26579285  0.47748089  0.65698951  0.03557406  0.51365417  0.31008556\n",
      "  0.18317091 -0.14873023 -0.10651308  0.26940098  0.3019267  -0.03403223\n",
      "  0.227947   -0.1853312  -0.27943975  0.19992299 -0.48229539  0.39852554\n",
      "  0.26284266 -0.19975854 -0.58331889  0.11538835  0.07480371  0.02123258\n",
      " -0.25289708  0.02364382 -0.16333692 -0.25307208  0.14204994 -0.04806495\n",
      " -0.30738699 -0.01687431 -0.18247785  0.12983495  0.21761484 -0.14039074\n",
      " -0.21636817  0.45452079 -0.15039034  0.28887495  0.1986296   0.02644534\n",
      " -0.51308686 -0.18552893 -0.04472456  0.27717152 -0.0670101   0.26243782\n",
      " -0.02211045  0.06636734 -0.21527472  0.04497791 -0.28124395  0.09355179\n",
      " -0.22088195 -0.03174514 -0.2266928   0.10434103 -0.24530321 -0.17732373\n",
      "  0.10097005 -0.27224585  0.07065725 -0.20668578  0.49978203 -0.22160135\n",
      " -0.78769308 -0.23871133 -0.19496164 -0.41517848 -0.80961156 -0.34282428\n",
      " -0.62406301 -0.25187653  0.31175646 -1.21281242 -0.6559028  -0.58827275\n",
      " -0.75403345 -0.58548653  0.66391647 -1.12206542 -0.75167948 -0.81053132\n",
      " -0.39619377 -1.25106943 -0.33499321 -1.16049719 -0.06187676 -0.41320661\n",
      "  0.26254776 -1.00187635 -0.48609066 -0.97978514 -1.06316149 -1.06353867\n",
      " -0.2477043  -0.07814736  0.41977647 -1.11594057 -1.03222919 -0.7431795\n",
      " -0.22954053 -0.23104134 -0.33247638 -0.02244741  0.61472791 -1.61832082\n",
      " -0.46168745 -0.65481275 -0.51274139 -0.4312351  -0.29270557 -0.83941871\n",
      " -0.69901139 -0.34159306 -0.31520349 -0.7552281  -1.46113753 -0.36992776\n",
      " -1.54311073 -0.96306229  0.85340351 -0.79043102 -0.35208231 -0.38835609\n",
      " -0.10302607 -0.16447715 -0.42997941 -1.06737506 -1.06338298 -0.21228665\n",
      " -0.098262    0.46810508 -0.68598133 -0.54999864 -0.37923697 -0.5236634\n",
      "  0.23227285 -0.61094898 -1.10851991  0.61626929 -0.10921194 -0.10388382\n",
      " -0.11097398 -1.08766782  0.58427417 -1.47325146 -1.18728626 -0.38060868\n",
      " -0.42589462 -1.01009583  0.76991117 -0.0477961  -0.91320568 -0.63767374\n",
      " -0.98306996 -0.31313637 -0.69822299 -0.34630591 -1.17412198 -0.54706013\n",
      " -0.22522324 -0.57627338 -1.3250351  -0.26136386 -1.42946374 -1.0478977\n",
      " -1.31176877 -0.73726153 -0.53502679 -0.72382271 -1.07345724 -1.09226596\n",
      "  0.25490785 -0.41478503 -1.55593312 -0.88110757 -0.9380061   0.2110361\n",
      "  0.07034767  0.24534908 -1.18791103 -0.93096447 -0.74960172 -0.3615934\n",
      "  0.27900508 -0.1465617  -0.2654334  -0.78976488 -0.68474042  0.78824872\n",
      " -0.40606168 -0.99138916 -0.47346574 -0.63463098 -0.90745389 -0.73344237\n",
      " -0.29471818 -0.72132951 -1.33910501 -1.84247577 -0.93350506 -0.48260993\n",
      " -0.90629488 -0.86327261 -0.36246917  0.08096252 -0.50182438 -0.65449262\n",
      " -0.57235056 -0.80562711 -0.85172307 -1.03312862 -1.02071083 -0.45182592\n",
      " -1.35609519 -0.40441483 -0.65226334 -0.37349418 -1.68147898 -1.07002473\n",
      " -0.56461376 -0.99743962 -1.05683172 -1.66098619  0.19514951  0.04565418\n",
      " -0.5525412  -1.50989568 -1.38494146 -1.33653176  0.05514889 -0.46935314\n",
      " -1.03718567 -0.44088235 -0.56742704 -1.44888663 -1.82851839 -1.17218471\n",
      " -0.59974277  0.75763088 -0.49143812 -0.28153422 -0.86693305  0.52463937\n",
      " -1.2650367  -0.20260088 -0.42752743  0.05551466 -0.18913122 -0.78553873\n",
      " -1.01058006 -0.66452408 -0.64596057 -0.15287754 -1.62768328 -0.41630384\n",
      " -0.36360461 -0.36582187 -0.70004064 -0.22964467 -0.56476402 -0.21548954\n",
      " -1.07596374 -0.87310135 -1.16785789 -0.79760021 -1.10781431 -1.5738039\n",
      " -1.08682323 -1.43019915 -0.99616522 -1.04746401 -1.13118398 -1.56935608\n",
      " -1.25290596 -1.71057427 -1.71873581 -0.58217967 -1.20954835 -1.15984142\n",
      " -1.09918511 -1.46334255 -0.94292873 -1.0804882  -0.73849374 -1.67544079\n",
      " -0.47172901 -1.21074438 -1.13803709 -1.3972075  -0.86594611 -1.75600243\n",
      " -1.49376225 -1.20986092 -0.65632415 -1.11042869 -0.72084075 -1.5316509\n",
      " -0.93259495 -1.02088058 -0.81187123 -0.70081955 -0.92961663 -0.91162622\n",
      " -0.07085653 -1.66272831 -1.17152488 -1.20277619 -1.34503126 -1.62731385\n",
      " -0.76304227 -1.23506188 -1.13931131 -1.04198277 -0.49077487 -1.03344178\n",
      " -1.31110394 -0.7156502  -1.62963188 -1.52437198 -1.21513498 -1.03228772\n",
      " -1.37536335 -1.25197911 -0.75872898 -0.79086369 -1.16158926 -1.53931284\n",
      " -1.71562743 -0.77162087 -1.38906431 -0.48669931 -1.07642162 -1.46495438\n",
      " -1.17131531 -1.42279637  0.0501684  -1.28487766 -1.28676558  0.07399621\n",
      " -0.5259859  -0.95782518 -0.67302477 -1.60260594 -0.0154256  -1.43950689\n",
      " -1.56735134 -0.64742565 -1.49894249 -1.07196379 -1.53664434 -0.93590444\n",
      " -1.5770123  -0.70898044 -1.06266975 -1.42672431 -1.57308459 -0.62313294\n",
      " -1.47817969 -0.66453153 -1.07744837 -1.00876081 -1.71502233 -0.14084134\n",
      " -1.01435184 -1.64160526 -1.24358511 -1.40592444 -1.27741039 -1.36941254\n",
      " -1.36158586 -1.54117787 -0.98289311 -1.31524873 -1.17672825 -1.14097917\n",
      " -1.31581593 -0.18578082 -0.71118784 -1.01382935 -1.46284723 -1.76183593\n",
      " -1.31455052 -1.29175794 -0.74321789 -0.74754459 -0.53401941 -1.38303709\n",
      " -1.53135419 -0.1608839  -1.48087358 -1.44671071 -1.16912794 -1.42847371\n",
      " -1.36545289 -1.05845332 -0.87256145 -1.28600204 -1.49315894 -1.87094533\n",
      " -0.98329002 -0.87476575 -1.52234602 -1.55064666 -1.30085337 -0.79058802\n",
      " -1.27395749 -0.94230098 -0.60027659 -1.55652022 -1.45442057 -0.89348412\n",
      " -0.98327571 -1.32667208 -1.23571408 -0.58700877 -0.88397622 -1.16207302\n",
      " -1.68984318 -1.44983709 -1.05247974 -1.53635764 -1.36639488 -0.38736948\n",
      "  0.05127974 -0.72620332 -1.2512033  -1.81926513 -1.60676122 -0.92969459\n",
      " -0.9712435  -1.21072876 -1.30502236 -1.11295795 -0.80110848 -1.59320033\n",
      " -1.3588028  -1.24407792 -1.23459315 -0.83685333 -1.27015722 -0.9459331\n",
      " -1.78395283 -0.61168951 -1.20248532 -0.72606134 -1.02151012 -1.33325422\n",
      " -0.84869522 -1.38391852 -1.17003679 -1.39996767 -1.2232281  -0.47607124\n",
      " -1.36682963 -1.68692863 -1.39389157 -0.80817318 -1.57460558 -0.86856246\n",
      " -1.32612216 -1.30314493]\n",
      "Variable:  W_out:0\n",
      "Shape:  (200, 10000)\n",
      "[[ 2.28468347 -2.4383688   0.46517646 ...,  0.78199744  0.30190784\n",
      "   1.28623426]\n",
      " [-3.18483305  1.30829895 -0.73969525 ...,  0.77501917  0.56872684\n",
      "  -0.19906861]\n",
      " [ 1.59035385 -1.54947662  0.61379439 ..., -0.39432272 -0.48021194\n",
      "  -1.17714393]\n",
      " ..., \n",
      " [-0.81820858  0.65063709 -0.62134773 ...,  0.51875597  0.58312684\n",
      "   0.65978503]\n",
      " [ 1.7497704  -1.22914314  0.38044265 ..., -0.88974428  0.38330361\n",
      "  -0.0727668 ]\n",
      " [-1.62994659  1.79553568 -0.41275212 ...,  0.069008   -1.14776623\n",
      "   0.27599373]]\n",
      "Variable:  b_out:0\n",
      "Shape:  (10000,)\n",
      "[  2.37761545e+00  -2.65590024e+00   7.36887312e+00 ...,   2.09847261e-04\n",
      "  -2.45708853e-01   2.22183034e-01]\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "    \n",
    "    #check trainable variables\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = session.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(\"Variable: \", k)\n",
    "        print(\"Shape: \", v.shape)\n",
    "        print(v)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n",
    "    \n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = session.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(\"Variable: \", k)\n",
    "        print(\"Shape: \", v.shape)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,1]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "    #lm.BuildSamplerGraph()\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "<s> the tractor is traveler sad . <s> \n",
      "<s> so a <unk> show could feel the slope to know the occasional same number . <s> \n",
      "<s> he traveled at the top into the <unk> gathering ! ! <s> \n",
      "<s> he was do ' not do the <unk> or berry it apparently had <unk> to attend new slaves . <s> \n",
      "<s> i believe , and this statement or today are displayed it . <s> \n",
      "<s> american is on the minority to the sterile one who had placed his own school in plain shall andy wear \n",
      "<s> he imagined eighteen upon all lee today standing to the work . <s> \n",
      "<s> you cannot speak to the <unk> and <unk> <unk> ; ; <s> \n",
      "<s> i give me master around her . <s> \n",
      "<s> the proposed technique in the new <unk> that are has only extremely <unk> by contrast to him policy . <s> \n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            print(vocab.id_to_word[word_id], end=\" \")\n",
    "            if (i != 0) and (word_id == vocab.START_ID):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"once upon a time\" : -8.74\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.83\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = [\"is sentence nonsense this\", \"i drive a car\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -6.03\n",
      "\"the boy and the girl is\" : -5.90\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"the boy and the girl are\", \"the boy and the girl is\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 1. question(s)\n",
    "\n",
    "*According to the model, \"the boy and the girl are\" is more plausible, but the log probabilities of the two phrases are very close.  This is likely because \"is\" and \"are\" are used so similarly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -7.46\n",
      "\"peanuts are my favorite kind of vegetable\" : -7.29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"when I'm hungry I really prefer to eat\" : -7.49\n",
      "\"when I'm hungry I really prefer to drink\" : -7.52\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"peanuts are my favorite kind of nut\",\n",
    "         \"peanuts are my favorite kind of vegetable\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "sents = [\"when I'm hungry I really prefer to eat\",\n",
    "         \"when I'm hungry I really prefer to drink\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 2. question(s)\n",
    "\n",
    "*Of each pair, the sentences \"peanuts are my favorite kind of vegetable\" and \"when I'm hungry I really prefer to eat\" are more plausible according to the model.*\n",
    "\n",
    "*For the differing word in each pair of sentences, a 3-gram language model would use as context \"favorite kind of\" and \"really prefer to\", and it thus would not have enough context to correctly score the likely final word.*\n",
    "\n",
    "*For the differing word in each pair of sentences, a 5-gram language model would use as context \"are my favorite kind of\" and \"hungry I really prefer to\".  In the first pair of sentences, there is still not enough context to correctly score the next expected word.  However, in the second pair of sentences, a 5-gram language model would capture \"hungry\" in the context, and this may help it to score \"eat\" higher than \"drink\" as the next expected word, similarly to the RNN model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from Week 2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"I have lots of plastic green square toys\" : -8.51\n",
      "\"I have lots of green square plastic toys\" : -8.56\n",
      "\"I have lots of green plastic square toys\" : -8.57\n",
      "\"I have lots of plastic square green toys\" : -8.64\n",
      "\"I have lots of square green plastic toys\" : -8.69\n",
      "\"I have lots of square plastic green toys\" : -8.71\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = prefix + list(adjs) + [noun]\n",
    "    inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
