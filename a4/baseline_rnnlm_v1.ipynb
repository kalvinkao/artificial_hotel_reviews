{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Character-Level Language Model\n",
    "\n",
    "This is the \"working notebook\", with code to load and train the model, as well as run unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "#wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "#embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "#x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "#print('wordid placeholder shape:', wordid_ph.shape)\n",
    "#print('x shape:', x.shape)\n",
    "\n",
    "#sess = tf.Session()\n",
    "## Two sentences, each with four words.\n",
    "#wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "#x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "#print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "#print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "Load implementation, construct graph, and write a logdir for TensorBoard.\n",
    "```\n",
    "cd assignment/a4\n",
    "tensorboard --logdir /tmp/w266/a4_graph --port 6006\n",
    "```\n",
    "http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/artificial_hotel_reviews/a4_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  W_in:0\n",
      "Shape:  (10000, 200)\n",
      "[[-0.78748155 -0.05848813 -0.64172769 ...,  0.84141541 -0.94063973\n",
      "   0.6318059 ]\n",
      " [ 0.73271012  0.9191277   0.16166019 ..., -0.77998519  0.349545\n",
      "  -0.45933056]\n",
      " [ 0.56432152 -0.03329587  0.20995927 ..., -0.3828721   0.12824035\n",
      "  -0.13679957]\n",
      " ..., \n",
      " [-0.12901855 -0.29448342 -0.08786106 ..., -0.73254704 -0.37692833\n",
      "  -0.20111871]\n",
      " [-0.62848496 -0.50970197  0.9452734  ...,  0.99917698 -0.81992722\n",
      "  -0.37649083]\n",
      " [-0.09145284  0.00680947  0.67164588 ..., -0.92668557  0.46153927\n",
      "   0.74277234]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.02885439 -0.00585808 -0.04069098 ...,  0.00699186 -0.05404399\n",
      "   0.02202466]\n",
      " [ 0.06509807 -0.02223223  0.02077565 ..., -0.00037639  0.04783005\n",
      "  -0.00804034]\n",
      " [ 0.02742642  0.05705827 -0.03898749 ...,  0.00316557  0.06507974\n",
      "  -0.01836298]\n",
      " ..., \n",
      " [-0.00754741 -0.04928695  0.06990113 ..., -0.06414328 -0.06926721\n",
      "  -0.05963309]\n",
      " [-0.06505661 -0.0187963   0.05553678 ..., -0.00105456 -0.04487306\n",
      "   0.05926735]\n",
      " [ 0.05089028  0.03582108 -0.05539448 ...,  0.00139427  0.06008063\n",
      "   0.05134535]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.06411717 -0.059903    0.05128169 ...,  0.03041378  0.05353859\n",
      "   0.0544473 ]\n",
      " [ 0.06030189  0.02777085  0.0119326  ..., -0.00028028 -0.04805488\n",
      "  -0.03846691]\n",
      " [-0.02244554  0.04396319  0.06136966 ...,  0.02755227  0.01427022\n",
      "   0.0558702 ]\n",
      " ..., \n",
      " [-0.0622274  -0.03975505 -0.03073074 ...,  0.0241474  -0.04965901\n",
      "  -0.00885601]\n",
      " [ 0.0227494  -0.0019692  -0.05720998 ..., -0.03900233 -0.02878788\n",
      "  -0.05673516]\n",
      " [-0.04740062  0.00044262  0.0118935  ...,  0.02134912  0.04072783\n",
      "   0.00831893]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  W_out:0\n",
      "Shape:  (200, 10000)\n",
      "[[ 0.22999096 -0.98074627  0.2485311  ...,  0.31636119  0.64180779\n",
      "   0.48168755]\n",
      " [ 0.02860689  0.14025211  0.73703051 ...,  0.55039978 -0.11227965\n",
      "   0.06153202]\n",
      " [-0.74636269 -0.50103402  0.13248014 ..., -0.45557976  0.15847898\n",
      "   0.09724474]\n",
      " ..., \n",
      " [-0.21404767  0.9171977  -0.29322004 ..., -0.82013679  0.63434672\n",
      "  -0.56355882]\n",
      " [ 0.83722639 -0.26542854 -0.89726591 ..., -0.59170508  0.18550444\n",
      "   0.48118067]\n",
      " [ 0.55475283 -0.74218535 -0.24593616 ..., -0.88373137 -0.86611366\n",
      "  -0.00220013]]\n",
      "Variable:  b_out:0\n",
      "Shape:  (10000,)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#with lm.graph.as_default():\n",
    "    #initializer = tf.global_variables_initializer()\n",
    "\n",
    "#with tf.Session(graph=lm.graph) as session:\n",
    "    #session.run(initializer)\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unit Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 3.315s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNNLM\n",
    "\n",
    "Data loader functions in **`utils.py`**.\n",
    "\n",
    "`utils.rnnlm_batch_generator` returns an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "Toy corpus example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_epoch()\n",
    "\n",
    "`train=True` flag enables train mode. `train=False` runs `score_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Unit Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 0]: seen 50 words at 43.1 wps, loss = 2.028\n",
      "[batch 98]: seen 4950 words at 2284.2 wps, loss = 1.114\n",
      "[batch 210]: seen 10550 words at 3329.2 wps, loss = 0.959\n",
      "[batch 346]: seen 17350 words at 4155.3 wps, loss = 1.024\n",
      "[batch 481]: seen 24100 words at 4656.4 wps, loss = 0.983\n",
      "[batch 615]: seen 30800 words at 4987.1 wps, loss = 0.943\n",
      "[batch 751]: seen 37600 words at 5237.5 wps, loss = 0.954\n",
      "[batch 887]: seen 44400 words at 5425.6 wps, loss = 0.964\n",
      "[batch 1024]: seen 51250 words at 5579.2 wps, loss = 0.954\n",
      "[batch 1160]: seen 58050 words at 5697.4 wps, loss = 0.943\n",
      "[batch 1296]: seen 64850 words at 5794.1 wps, loss = 0.930\n",
      "[batch 1431]: seen 71600 words at 5872.2 wps, loss = 0.921\n",
      "[batch 1567]: seen 78400 words at 5940.1 wps, loss = 0.917\n",
      "[batch 1702]: seen 85150 words at 5995.7 wps, loss = 0.905\n",
      "[batch 1839]: seen 92000 words at 6050.8 wps, loss = 0.909\n",
      "[batch 1976]: seen 98850 words at 6098.0 wps, loss = 0.914\n",
      "[batch 2112]: seen 105650 words at 6138.3 wps, loss = 0.906\n",
      "[batch 2245]: seen 112300 words at 6164.1 wps, loss = 0.893\n",
      "[batch 2379]: seen 119000 words at 6191.6 wps, loss = 0.883\n",
      "[batch 2516]: seen 125850 words at 6222.1 wps, loss = 0.881\n",
      "[batch 2650]: seen 132550 words at 6243.5 wps, loss = 0.886\n",
      "[batch 2786]: seen 139350 words at 6268.4 wps, loss = 0.880\n",
      "[batch 2919]: seen 146000 words at 6283.9 wps, loss = 0.875\n",
      "[batch 3055]: seen 152800 words at 6303.9 wps, loss = 0.870\n",
      "[batch 3189]: seen 159500 words at 6319.6 wps, loss = 0.874\n",
      "[batch 3323]: seen 166200 words at 6334.0 wps, loss = 0.878\n",
      "[batch 3456]: seen 172850 words at 6344.5 wps, loss = 0.889\n",
      "[batch 3587]: seen 179400 words at 6350.9 wps, loss = 0.886\n",
      "[batch 3717]: seen 185900 words at 6355.3 wps, loss = 0.880\n",
      "[batch 3844]: seen 192250 words at 6353.9 wps, loss = 0.873\n",
      "[batch 3970]: seen 198550 words at 6351.4 wps, loss = 0.870\n",
      "Train set: avg. loss: 0.018  (perplexity: 1.02)\n",
      "Test set: avg. loss: 0.038  (perplexity: 1.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 33.626s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "At the end of training, `tf.train.Saver` saves a copy of the model to `/tmp/w266/artificial_hotel_reviews/rnnlm_trained`. To load this from disk, see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/kalvin_kao/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "## Load the dataset\n",
    "#V = 10000\n",
    "#vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv(review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_csv(business_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_review_df = review_df[review_df['stars']==5]\n",
    "five_star_review_series = five_star_review_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in five_star_review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "        #print(final_review)\n",
    "        review_list.append(final_review)\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed reviews\n",
    "#review_list = preprocess_review_series(five_star_review_series)\n",
    "review_list = preprocess_review_series(five_star_review_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10406707\n"
     ]
    }
   ],
   "source": [
    "#combined_review_list = [item for sublist in review_list for item in sublist]\n",
    "training_review_list = [item for sublist in review_list[:20000] for item in sublist]\n",
    "print(len(training_review_list))\n",
    "test_review_list = [item for sublist in review_list[20000:21000] for item in sublist]\n",
    "#unique_characters = list(set(combined_review_list))\n",
    "unique_characters = list(set(training_review_list + test_review_list))\n",
    "#len(unique_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary\n",
    "char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#print(char_dict)\n",
    "#print(ids_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "print(len(char_dict.keys()))\n",
    "print(len(unique_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to flat (1D) np.array(int) of ids\n",
    "#add memory to VM and remove 1000 slice\n",
    "#combined_review_ids = [char_dict.get(token) for token in combined_review_list[:1000]]\n",
    "training_review_ids = [char_dict.get(token) for token in training_review_list]\n",
    "test_review_ids = [char_dict.get(token) for token in test_review_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array(training_review_ids)\n",
    "test_ids = np.array(test_review_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "## add parameter sets for each attack/defense configuration\n",
    "#max_time = 25\n",
    "max_time = 100\n",
    "#batch_size = 100\n",
    "batch_size = 256\n",
    "#learning_rate = 0.01\n",
    "learning_rate = 0.002\n",
    "#num_epochs = 10\n",
    "num_epochs = 1\n",
    "\n",
    "# Model parameters\n",
    "#model_params = dict(V=vocab.size, \n",
    "                    #H=200, \n",
    "                    #softmax_ns=200,\n",
    "                    #num_layers=2)\n",
    "model_params = dict(V=len(unique_characters), \n",
    "                    H=1024, \n",
    "                    softmax_ns=len(unique_characters),\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 0]: seen 25600 words at 1064.6 wps, loss = 4.762\n",
      "[batch 1]: seen 51200 words at 1117.5 wps, loss = 10.085\n",
      "[batch 2]: seen 76800 words at 1137.5 wps, loss = 11.354\n",
      "[batch 3]: seen 102400 words at 1146.2 wps, loss = 11.476\n",
      "[batch 4]: seen 128000 words at 1149.0 wps, loss = 10.534\n",
      "[batch 5]: seen 153600 words at 1153.5 wps, loss = 10.008\n",
      "[batch 6]: seen 179200 words at 1156.0 wps, loss = 9.317\n",
      "[batch 7]: seen 204800 words at 1158.1 wps, loss = 8.666\n",
      "[batch 8]: seen 230400 words at 1159.9 wps, loss = 8.107\n",
      "[batch 9]: seen 256000 words at 1161.1 wps, loss = 7.629\n",
      "[batch 10]: seen 281600 words at 1162.9 wps, loss = 7.217\n",
      "[batch 11]: seen 307200 words at 1163.7 wps, loss = 6.858\n",
      "[batch 12]: seen 332800 words at 1164.4 wps, loss = 6.545\n",
      "[batch 13]: seen 358400 words at 1164.8 wps, loss = 6.271\n",
      "[batch 14]: seen 384000 words at 1163.3 wps, loss = 6.031\n",
      "[batch 15]: seen 409600 words at 1164.1 wps, loss = 5.816\n",
      "[batch 16]: seen 435200 words at 1164.7 wps, loss = 5.625\n",
      "[batch 17]: seen 460800 words at 1165.0 wps, loss = 5.454\n",
      "[batch 18]: seen 486400 words at 1165.7 wps, loss = 5.300\n",
      "[batch 19]: seen 512000 words at 1166.2 wps, loss = 5.159\n",
      "[batch 20]: seen 537600 words at 1166.7 wps, loss = 5.032\n",
      "[batch 21]: seen 563200 words at 1167.6 wps, loss = 4.915\n",
      "[batch 22]: seen 588800 words at 1168.3 wps, loss = 4.807\n",
      "[batch 23]: seen 614400 words at 1168.6 wps, loss = 4.708\n",
      "[batch 24]: seen 640000 words at 1168.6 wps, loss = 4.617\n",
      "[batch 25]: seen 665600 words at 1168.6 wps, loss = 4.531\n",
      "[batch 26]: seen 691200 words at 1169.0 wps, loss = 4.452\n",
      "[batch 27]: seen 716800 words at 1168.9 wps, loss = 4.378\n",
      "[batch 28]: seen 742400 words at 1169.2 wps, loss = 4.308\n",
      "[batch 29]: seen 768000 words at 1169.0 wps, loss = 4.242\n",
      "[batch 30]: seen 793600 words at 1169.1 wps, loss = 4.180\n",
      "[batch 31]: seen 819200 words at 1169.1 wps, loss = 4.122\n",
      "[batch 32]: seen 844800 words at 1169.0 wps, loss = 4.067\n",
      "[batch 33]: seen 870400 words at 1169.3 wps, loss = 4.015\n",
      "[batch 34]: seen 896000 words at 1169.3 wps, loss = 3.966\n",
      "[batch 35]: seen 921600 words at 1169.4 wps, loss = 3.920\n",
      "[batch 36]: seen 947200 words at 1169.4 wps, loss = 3.875\n",
      "[batch 37]: seen 972800 words at 1169.6 wps, loss = 3.832\n",
      "[batch 38]: seen 998400 words at 1169.9 wps, loss = 3.792\n",
      "[batch 39]: seen 1024000 words at 1169.8 wps, loss = 3.753\n",
      "[batch 40]: seen 1049600 words at 1169.9 wps, loss = 3.716\n",
      "[batch 41]: seen 1075200 words at 1169.9 wps, loss = 3.680\n",
      "[batch 42]: seen 1100800 words at 1170.1 wps, loss = 3.645\n",
      "[batch 43]: seen 1126400 words at 1170.4 wps, loss = 3.613\n",
      "[batch 44]: seen 1152000 words at 1170.2 wps, loss = 3.582\n",
      "[batch 45]: seen 1177600 words at 1170.1 wps, loss = 3.552\n",
      "[batch 46]: seen 1203200 words at 1169.9 wps, loss = 3.522\n",
      "[batch 47]: seen 1228800 words at 1169.9 wps, loss = 3.494\n",
      "[batch 48]: seen 1254400 words at 1169.8 wps, loss = 3.467\n",
      "[batch 49]: seen 1280000 words at 1169.7 wps, loss = 3.440\n",
      "[batch 50]: seen 1305600 words at 1169.8 wps, loss = 3.415\n",
      "[batch 51]: seen 1331200 words at 1169.9 wps, loss = 3.390\n",
      "[batch 52]: seen 1356800 words at 1170.0 wps, loss = 3.366\n",
      "[batch 53]: seen 1382400 words at 1170.0 wps, loss = 3.343\n",
      "[batch 54]: seen 1408000 words at 1170.1 wps, loss = 3.321\n",
      "[batch 55]: seen 1433600 words at 1170.1 wps, loss = 3.299\n",
      "[batch 56]: seen 1459200 words at 1170.1 wps, loss = 3.279\n",
      "[batch 57]: seen 1484800 words at 1169.8 wps, loss = 3.258\n",
      "[batch 58]: seen 1510400 words at 1169.7 wps, loss = 3.238\n",
      "[batch 59]: seen 1536000 words at 1169.6 wps, loss = 3.220\n",
      "[batch 60]: seen 1561600 words at 1169.6 wps, loss = 3.201\n",
      "[batch 61]: seen 1587200 words at 1169.6 wps, loss = 3.183\n",
      "[batch 62]: seen 1612800 words at 1169.5 wps, loss = 3.165\n",
      "[batch 63]: seen 1638400 words at 1169.4 wps, loss = 3.148\n",
      "[batch 64]: seen 1664000 words at 1169.4 wps, loss = 3.131\n",
      "[batch 65]: seen 1689600 words at 1169.2 wps, loss = 3.115\n",
      "[batch 66]: seen 1715200 words at 1169.0 wps, loss = 3.100\n",
      "[batch 67]: seen 1740800 words at 1169.0 wps, loss = 3.085\n",
      "[batch 68]: seen 1766400 words at 1168.9 wps, loss = 3.070\n",
      "[batch 69]: seen 1792000 words at 1168.7 wps, loss = 3.055\n",
      "[batch 70]: seen 1817600 words at 1168.6 wps, loss = 3.041\n",
      "[batch 71]: seen 1843200 words at 1168.5 wps, loss = 3.027\n",
      "[batch 72]: seen 1868800 words at 1168.5 wps, loss = 3.014\n",
      "[batch 73]: seen 1894400 words at 1168.4 wps, loss = 3.001\n",
      "[batch 74]: seen 1920000 words at 1168.3 wps, loss = 2.988\n",
      "[batch 75]: seen 1945600 words at 1168.2 wps, loss = 2.975\n",
      "[batch 76]: seen 1971200 words at 1168.2 wps, loss = 2.963\n",
      "[batch 77]: seen 1996800 words at 1168.3 wps, loss = 2.950\n",
      "[batch 78]: seen 2022400 words at 1168.3 wps, loss = 2.939\n",
      "[batch 79]: seen 2048000 words at 1168.3 wps, loss = 2.927\n",
      "[batch 80]: seen 2073600 words at 1168.2 wps, loss = 2.916\n",
      "[batch 81]: seen 2099200 words at 1168.1 wps, loss = 2.905\n",
      "[batch 82]: seen 2124800 words at 1167.9 wps, loss = 2.894\n",
      "[batch 83]: seen 2150400 words at 1167.9 wps, loss = 2.883\n",
      "[batch 84]: seen 2176000 words at 1167.8 wps, loss = 2.872\n",
      "[batch 85]: seen 2201600 words at 1167.8 wps, loss = 2.862\n",
      "[batch 86]: seen 2227200 words at 1167.7 wps, loss = 2.852\n",
      "[batch 87]: seen 2252800 words at 1167.7 wps, loss = 2.842\n",
      "[batch 88]: seen 2278400 words at 1167.8 wps, loss = 2.832\n",
      "[batch 89]: seen 2304000 words at 1167.7 wps, loss = 2.823\n",
      "[batch 90]: seen 2329600 words at 1167.7 wps, loss = 2.813\n",
      "[batch 91]: seen 2355200 words at 1167.7 wps, loss = 2.804\n",
      "[batch 92]: seen 2380800 words at 1167.8 wps, loss = 2.794\n",
      "[batch 93]: seen 2406400 words at 1167.9 wps, loss = 2.785\n",
      "[batch 94]: seen 2432000 words at 1168.0 wps, loss = 2.776\n",
      "[batch 95]: seen 2457600 words at 1167.8 wps, loss = 2.768\n",
      "[batch 96]: seen 2483200 words at 1167.9 wps, loss = 2.759\n",
      "[batch 97]: seen 2508800 words at 1167.9 wps, loss = 2.750\n",
      "[batch 98]: seen 2534400 words at 1167.9 wps, loss = 2.742\n",
      "[batch 99]: seen 2560000 words at 1168.0 wps, loss = 2.734\n",
      "[batch 100]: seen 2585600 words at 1167.9 wps, loss = 2.726\n",
      "[batch 101]: seen 2611200 words at 1167.9 wps, loss = 2.718\n",
      "[batch 102]: seen 2636800 words at 1167.9 wps, loss = 2.710\n",
      "[batch 103]: seen 2662400 words at 1167.9 wps, loss = 2.703\n",
      "[batch 104]: seen 2688000 words at 1168.0 wps, loss = 2.695\n",
      "[batch 105]: seen 2713600 words at 1167.9 wps, loss = 2.687\n",
      "[batch 106]: seen 2739200 words at 1167.9 wps, loss = 2.680\n",
      "[batch 107]: seen 2764800 words at 1168.0 wps, loss = 2.673\n",
      "[batch 108]: seen 2790400 words at 1168.0 wps, loss = 2.665\n",
      "[batch 109]: seen 2816000 words at 1168.1 wps, loss = 2.658\n",
      "[batch 110]: seen 2841600 words at 1168.1 wps, loss = 2.651\n",
      "[batch 111]: seen 2867200 words at 1168.1 wps, loss = 2.644\n",
      "[batch 112]: seen 2892800 words at 1168.0 wps, loss = 2.637\n",
      "[batch 113]: seen 2918400 words at 1168.0 wps, loss = 2.630\n",
      "[batch 114]: seen 2944000 words at 1168.0 wps, loss = 2.624\n",
      "[batch 115]: seen 2969600 words at 1168.0 wps, loss = 2.618\n",
      "[batch 116]: seen 2995200 words at 1168.0 wps, loss = 2.611\n",
      "[batch 117]: seen 3020800 words at 1168.1 wps, loss = 2.605\n",
      "[batch 118]: seen 3046400 words at 1168.1 wps, loss = 2.599\n",
      "[batch 119]: seen 3072000 words at 1168.1 wps, loss = 2.593\n",
      "[batch 120]: seen 3097600 words at 1168.0 wps, loss = 2.587\n",
      "[batch 121]: seen 3123200 words at 1168.0 wps, loss = 2.581\n",
      "[batch 122]: seen 3148800 words at 1168.0 wps, loss = 2.575\n",
      "[batch 123]: seen 3174400 words at 1168.1 wps, loss = 2.569\n",
      "[batch 124]: seen 3200000 words at 1168.0 wps, loss = 2.563\n",
      "[batch 125]: seen 3225600 words at 1168.1 wps, loss = 2.557\n",
      "[batch 126]: seen 3251200 words at 1168.2 wps, loss = 2.551\n",
      "[batch 127]: seen 3276800 words at 1168.3 wps, loss = 2.546\n",
      "[batch 128]: seen 3302400 words at 1168.3 wps, loss = 2.540\n",
      "[batch 129]: seen 3328000 words at 1168.2 wps, loss = 2.535\n",
      "[batch 130]: seen 3353600 words at 1168.2 wps, loss = 2.529\n",
      "[batch 131]: seen 3379200 words at 1168.3 wps, loss = 2.524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 132]: seen 3404800 words at 1168.3 wps, loss = 2.519\n",
      "[batch 133]: seen 3430400 words at 1168.3 wps, loss = 2.513\n",
      "[batch 134]: seen 3456000 words at 1168.4 wps, loss = 2.508\n",
      "[batch 135]: seen 3481600 words at 1168.3 wps, loss = 2.503\n",
      "[batch 136]: seen 3507200 words at 1168.3 wps, loss = 2.498\n",
      "[batch 137]: seen 3532800 words at 1168.4 wps, loss = 2.493\n",
      "[batch 138]: seen 3558400 words at 1168.4 wps, loss = 2.488\n",
      "[batch 139]: seen 3584000 words at 1168.5 wps, loss = 2.483\n",
      "[batch 140]: seen 3609600 words at 1168.5 wps, loss = 2.478\n",
      "[batch 141]: seen 3635200 words at 1168.5 wps, loss = 2.474\n",
      "[batch 142]: seen 3660800 words at 1168.5 wps, loss = 2.469\n",
      "[batch 143]: seen 3686400 words at 1168.5 wps, loss = 2.464\n",
      "[batch 144]: seen 3712000 words at 1168.5 wps, loss = 2.459\n",
      "[batch 145]: seen 3737600 words at 1168.5 wps, loss = 2.455\n",
      "[batch 146]: seen 3763200 words at 1168.5 wps, loss = 2.450\n",
      "[batch 147]: seen 3788800 words at 1168.6 wps, loss = 2.446\n",
      "[batch 148]: seen 3814400 words at 1168.7 wps, loss = 2.441\n",
      "[batch 149]: seen 3840000 words at 1168.7 wps, loss = 2.437\n",
      "[batch 150]: seen 3865600 words at 1168.7 wps, loss = 2.432\n",
      "[batch 151]: seen 3891200 words at 1168.7 wps, loss = 2.428\n",
      "[batch 152]: seen 3916800 words at 1168.7 wps, loss = 2.424\n",
      "[batch 153]: seen 3942400 words at 1168.7 wps, loss = 2.419\n",
      "[batch 154]: seen 3968000 words at 1168.7 wps, loss = 2.415\n",
      "[batch 155]: seen 3993600 words at 1168.7 wps, loss = 2.411\n",
      "[batch 156]: seen 4019200 words at 1168.7 wps, loss = 2.407\n",
      "[batch 157]: seen 4044800 words at 1168.6 wps, loss = 2.403\n",
      "[batch 158]: seen 4070400 words at 1168.7 wps, loss = 2.399\n",
      "[batch 159]: seen 4096000 words at 1168.7 wps, loss = 2.395\n",
      "[batch 160]: seen 4121600 words at 1168.7 wps, loss = 2.391\n",
      "[batch 161]: seen 4147200 words at 1168.6 wps, loss = 2.387\n",
      "[batch 162]: seen 4172800 words at 1168.5 wps, loss = 2.383\n",
      "[batch 163]: seen 4198400 words at 1168.6 wps, loss = 2.379\n",
      "[batch 164]: seen 4224000 words at 1168.6 wps, loss = 2.375\n",
      "[batch 165]: seen 4249600 words at 1168.7 wps, loss = 2.372\n",
      "[batch 166]: seen 4275200 words at 1168.7 wps, loss = 2.368\n",
      "[batch 167]: seen 4300800 words at 1168.7 wps, loss = 2.364\n",
      "[batch 168]: seen 4326400 words at 1168.7 wps, loss = 2.360\n",
      "[batch 169]: seen 4352000 words at 1168.7 wps, loss = 2.357\n",
      "[batch 170]: seen 4377600 words at 1168.8 wps, loss = 2.353\n",
      "[batch 171]: seen 4403200 words at 1168.8 wps, loss = 2.349\n",
      "[batch 172]: seen 4428800 words at 1168.8 wps, loss = 2.346\n",
      "[batch 173]: seen 4454400 words at 1168.9 wps, loss = 2.342\n",
      "[batch 174]: seen 4480000 words at 1168.9 wps, loss = 2.338\n",
      "[batch 175]: seen 4505600 words at 1169.0 wps, loss = 2.335\n",
      "[batch 176]: seen 4531200 words at 1169.0 wps, loss = 2.331\n",
      "[batch 177]: seen 4556800 words at 1169.0 wps, loss = 2.328\n",
      "[batch 178]: seen 4582400 words at 1169.1 wps, loss = 2.324\n",
      "[batch 179]: seen 4608000 words at 1169.1 wps, loss = 2.321\n",
      "[batch 180]: seen 4633600 words at 1169.2 wps, loss = 2.318\n",
      "[batch 181]: seen 4659200 words at 1169.2 wps, loss = 2.314\n",
      "[batch 182]: seen 4684800 words at 1169.2 wps, loss = 2.311\n",
      "[batch 183]: seen 4710400 words at 1169.2 wps, loss = 2.308\n",
      "[batch 184]: seen 4736000 words at 1169.3 wps, loss = 2.304\n",
      "[batch 185]: seen 4761600 words at 1169.3 wps, loss = 2.301\n",
      "[batch 186]: seen 4787200 words at 1169.3 wps, loss = 2.298\n",
      "[batch 187]: seen 4812800 words at 1169.4 wps, loss = 2.295\n",
      "[batch 188]: seen 4838400 words at 1169.4 wps, loss = 2.292\n",
      "[batch 189]: seen 4864000 words at 1169.4 wps, loss = 2.289\n",
      "[batch 190]: seen 4889600 words at 1169.4 wps, loss = 2.286\n",
      "[batch 191]: seen 4915200 words at 1169.5 wps, loss = 2.283\n",
      "[batch 192]: seen 4940800 words at 1169.5 wps, loss = 2.280\n",
      "[batch 193]: seen 4966400 words at 1169.5 wps, loss = 2.277\n",
      "[batch 194]: seen 4992000 words at 1169.6 wps, loss = 2.274\n",
      "[batch 195]: seen 5017600 words at 1169.6 wps, loss = 2.271\n",
      "[batch 196]: seen 5043200 words at 1169.7 wps, loss = 2.268\n",
      "[batch 197]: seen 5068800 words at 1169.7 wps, loss = 2.265\n",
      "[batch 198]: seen 5094400 words at 1169.7 wps, loss = 2.262\n",
      "[batch 199]: seen 5120000 words at 1169.8 wps, loss = 2.260\n",
      "[batch 200]: seen 5145600 words at 1169.8 wps, loss = 2.257\n",
      "[batch 201]: seen 5171200 words at 1169.8 wps, loss = 2.254\n",
      "[batch 202]: seen 5196800 words at 1169.9 wps, loss = 2.251\n",
      "[batch 203]: seen 5222400 words at 1169.9 wps, loss = 2.248\n",
      "[batch 204]: seen 5248000 words at 1169.9 wps, loss = 2.245\n",
      "[batch 205]: seen 5273600 words at 1170.0 wps, loss = 2.243\n",
      "[batch 206]: seen 5299200 words at 1169.9 wps, loss = 2.240\n",
      "[batch 207]: seen 5324800 words at 1170.0 wps, loss = 2.237\n",
      "[batch 208]: seen 5350400 words at 1170.0 wps, loss = 2.234\n",
      "[batch 209]: seen 5376000 words at 1170.0 wps, loss = 2.232\n",
      "[batch 210]: seen 5401600 words at 1170.0 wps, loss = 2.229\n",
      "[batch 211]: seen 5427200 words at 1170.0 wps, loss = 2.226\n",
      "[batch 212]: seen 5452800 words at 1170.0 wps, loss = 2.223\n",
      "[batch 213]: seen 5478400 words at 1170.0 wps, loss = 2.221\n",
      "[batch 214]: seen 5504000 words at 1170.1 wps, loss = 2.218\n",
      "[batch 215]: seen 5529600 words at 1170.1 wps, loss = 2.216\n",
      "[batch 216]: seen 5555200 words at 1170.1 wps, loss = 2.213\n",
      "[batch 217]: seen 5580800 words at 1170.1 wps, loss = 2.210\n",
      "[batch 218]: seen 5606400 words at 1170.1 wps, loss = 2.208\n",
      "[batch 219]: seen 5632000 words at 1170.1 wps, loss = 2.205\n",
      "[batch 220]: seen 5657600 words at 1170.1 wps, loss = 2.202\n",
      "[batch 221]: seen 5683200 words at 1170.1 wps, loss = 2.200\n",
      "[batch 222]: seen 5708800 words at 1170.2 wps, loss = 2.197\n",
      "[batch 223]: seen 5734400 words at 1170.1 wps, loss = 2.195\n",
      "[batch 224]: seen 5760000 words at 1170.2 wps, loss = 2.192\n",
      "[batch 225]: seen 5785600 words at 1170.2 wps, loss = 2.190\n",
      "[batch 226]: seen 5811200 words at 1170.2 wps, loss = 2.188\n",
      "[batch 227]: seen 5836800 words at 1170.2 wps, loss = 2.185\n",
      "[batch 228]: seen 5862400 words at 1170.2 wps, loss = 2.183\n",
      "[batch 229]: seen 5888000 words at 1170.2 wps, loss = 2.180\n",
      "[batch 230]: seen 5913600 words at 1170.2 wps, loss = 2.178\n",
      "[batch 231]: seen 5939200 words at 1170.2 wps, loss = 2.176\n",
      "[batch 232]: seen 5964800 words at 1170.2 wps, loss = 2.174\n",
      "[batch 233]: seen 5990400 words at 1170.2 wps, loss = 2.171\n",
      "[batch 234]: seen 6016000 words at 1170.2 wps, loss = 2.169\n",
      "[batch 235]: seen 6041600 words at 1170.1 wps, loss = 2.167\n",
      "[batch 236]: seen 6067200 words at 1170.1 wps, loss = 2.164\n",
      "[batch 237]: seen 6092800 words at 1170.1 wps, loss = 2.162\n",
      "[batch 238]: seen 6118400 words at 1170.0 wps, loss = 2.160\n",
      "[batch 239]: seen 6144000 words at 1170.0 wps, loss = 2.158\n",
      "[batch 240]: seen 6169600 words at 1170.0 wps, loss = 2.155\n",
      "[batch 241]: seen 6195200 words at 1170.1 wps, loss = 2.153\n",
      "[batch 242]: seen 6220800 words at 1170.1 wps, loss = 2.151\n",
      "[batch 243]: seen 6246400 words at 1170.1 wps, loss = 2.149\n",
      "[batch 244]: seen 6272000 words at 1170.1 wps, loss = 2.147\n",
      "[batch 245]: seen 6297600 words at 1170.1 wps, loss = 2.145\n",
      "[batch 246]: seen 6323200 words at 1170.1 wps, loss = 2.142\n",
      "[batch 247]: seen 6348800 words at 1170.1 wps, loss = 2.140\n",
      "[batch 248]: seen 6374400 words at 1170.2 wps, loss = 2.138\n",
      "[batch 249]: seen 6400000 words at 1170.2 wps, loss = 2.136\n",
      "[batch 250]: seen 6425600 words at 1170.2 wps, loss = 2.134\n",
      "[batch 251]: seen 6451200 words at 1170.3 wps, loss = 2.132\n",
      "[batch 252]: seen 6476800 words at 1170.3 wps, loss = 2.130\n",
      "[batch 253]: seen 6502400 words at 1170.3 wps, loss = 2.128\n",
      "[batch 254]: seen 6528000 words at 1170.3 wps, loss = 2.126\n",
      "[batch 255]: seen 6553600 words at 1170.3 wps, loss = 2.124\n",
      "[batch 256]: seen 6579200 words at 1170.3 wps, loss = 2.122\n",
      "[batch 257]: seen 6604800 words at 1170.4 wps, loss = 2.120\n",
      "[batch 258]: seen 6630400 words at 1170.4 wps, loss = 2.118\n",
      "[batch 259]: seen 6656000 words at 1170.4 wps, loss = 2.117\n",
      "[batch 260]: seen 6681600 words at 1170.4 wps, loss = 2.115\n",
      "[batch 261]: seen 6707200 words at 1170.4 wps, loss = 2.113\n",
      "[batch 262]: seen 6732800 words at 1170.4 wps, loss = 2.111\n",
      "[batch 263]: seen 6758400 words at 1170.4 wps, loss = 2.109\n",
      "[batch 264]: seen 6784000 words at 1170.4 wps, loss = 2.107\n",
      "[batch 265]: seen 6809600 words at 1170.4 wps, loss = 2.105\n",
      "[batch 266]: seen 6835200 words at 1170.4 wps, loss = 2.103\n",
      "[batch 267]: seen 6860800 words at 1170.4 wps, loss = 2.101\n",
      "[batch 268]: seen 6886400 words at 1170.5 wps, loss = 2.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 269]: seen 6912000 words at 1170.5 wps, loss = 2.098\n",
      "[batch 270]: seen 6937600 words at 1170.6 wps, loss = 2.096\n",
      "[batch 271]: seen 6963200 words at 1170.6 wps, loss = 2.094\n",
      "[batch 272]: seen 6988800 words at 1170.6 wps, loss = 2.093\n",
      "[batch 273]: seen 7014400 words at 1170.6 wps, loss = 2.091\n",
      "[batch 274]: seen 7040000 words at 1170.7 wps, loss = 2.089\n",
      "[batch 275]: seen 7065600 words at 1170.7 wps, loss = 2.087\n",
      "[batch 276]: seen 7091200 words at 1170.7 wps, loss = 2.085\n",
      "[batch 277]: seen 7116800 words at 1170.7 wps, loss = 2.084\n",
      "[batch 278]: seen 7142400 words at 1170.7 wps, loss = 2.082\n",
      "[batch 279]: seen 7168000 words at 1170.7 wps, loss = 2.080\n",
      "[batch 280]: seen 7193600 words at 1170.7 wps, loss = 2.078\n",
      "[batch 281]: seen 7219200 words at 1170.8 wps, loss = 2.077\n",
      "[batch 282]: seen 7244800 words at 1170.8 wps, loss = 2.075\n",
      "[batch 283]: seen 7270400 words at 1170.8 wps, loss = 2.073\n",
      "[batch 284]: seen 7296000 words at 1170.8 wps, loss = 2.072\n",
      "[batch 285]: seen 7321600 words at 1170.8 wps, loss = 2.070\n",
      "[batch 286]: seen 7347200 words at 1170.8 wps, loss = 2.068\n",
      "[batch 287]: seen 7372800 words at 1170.8 wps, loss = 2.067\n",
      "[batch 288]: seen 7398400 words at 1170.9 wps, loss = 2.065\n",
      "[batch 289]: seen 7424000 words at 1170.9 wps, loss = 2.064\n",
      "[batch 290]: seen 7449600 words at 1170.9 wps, loss = 2.062\n",
      "[batch 291]: seen 7475200 words at 1170.9 wps, loss = 2.061\n",
      "[batch 292]: seen 7500800 words at 1170.9 wps, loss = 2.059\n",
      "[batch 293]: seen 7526400 words at 1170.8 wps, loss = 2.057\n",
      "[batch 294]: seen 7552000 words at 1170.9 wps, loss = 2.056\n",
      "[batch 295]: seen 7577600 words at 1170.9 wps, loss = 2.054\n",
      "[batch 296]: seen 7603200 words at 1170.9 wps, loss = 2.053\n",
      "[batch 297]: seen 7628800 words at 1170.9 wps, loss = 2.051\n",
      "[batch 298]: seen 7654400 words at 1170.9 wps, loss = 2.050\n",
      "[batch 299]: seen 7680000 words at 1170.9 wps, loss = 2.048\n",
      "[batch 300]: seen 7705600 words at 1170.9 wps, loss = 2.047\n",
      "[batch 301]: seen 7731200 words at 1170.9 wps, loss = 2.045\n",
      "[batch 302]: seen 7756800 words at 1170.9 wps, loss = 2.044\n",
      "[batch 303]: seen 7782400 words at 1170.9 wps, loss = 2.042\n",
      "[batch 304]: seen 7808000 words at 1170.9 wps, loss = 2.041\n",
      "[batch 305]: seen 7833600 words at 1170.9 wps, loss = 2.039\n",
      "[batch 306]: seen 7859200 words at 1170.9 wps, loss = 2.038\n",
      "[batch 307]: seen 7884800 words at 1170.9 wps, loss = 2.036\n",
      "[batch 308]: seen 7910400 words at 1171.0 wps, loss = 2.035\n",
      "[batch 309]: seen 7936000 words at 1170.9 wps, loss = 2.034\n",
      "[batch 310]: seen 7961600 words at 1170.9 wps, loss = 2.032\n",
      "[batch 311]: seen 7987200 words at 1171.0 wps, loss = 2.031\n",
      "[batch 312]: seen 8012800 words at 1171.0 wps, loss = 2.029\n",
      "[batch 313]: seen 8038400 words at 1171.0 wps, loss = 2.028\n",
      "[batch 314]: seen 8064000 words at 1171.1 wps, loss = 2.026\n",
      "[batch 315]: seen 8089600 words at 1171.1 wps, loss = 2.025\n",
      "[batch 316]: seen 8115200 words at 1171.1 wps, loss = 2.023\n",
      "[batch 317]: seen 8140800 words at 1171.1 wps, loss = 2.022\n",
      "[batch 318]: seen 8166400 words at 1171.1 wps, loss = 2.021\n",
      "[batch 319]: seen 8192000 words at 1171.1 wps, loss = 2.019\n",
      "[batch 320]: seen 8217600 words at 1171.1 wps, loss = 2.018\n",
      "[batch 321]: seen 8243200 words at 1171.1 wps, loss = 2.016\n",
      "[batch 322]: seen 8268800 words at 1171.1 wps, loss = 2.015\n",
      "[batch 323]: seen 8294400 words at 1171.1 wps, loss = 2.014\n",
      "[batch 324]: seen 8320000 words at 1171.1 wps, loss = 2.012\n",
      "[batch 325]: seen 8345600 words at 1171.2 wps, loss = 2.011\n",
      "[batch 326]: seen 8371200 words at 1171.2 wps, loss = 2.009\n",
      "[batch 327]: seen 8396800 words at 1171.2 wps, loss = 2.008\n",
      "[batch 328]: seen 8422400 words at 1171.2 wps, loss = 2.006\n",
      "[batch 329]: seen 8448000 words at 1171.2 wps, loss = 2.005\n",
      "[batch 330]: seen 8473600 words at 1171.2 wps, loss = 2.004\n",
      "[batch 331]: seen 8499200 words at 1171.2 wps, loss = 2.002\n",
      "[batch 332]: seen 8524800 words at 1171.2 wps, loss = 2.001\n",
      "[batch 333]: seen 8550400 words at 1171.2 wps, loss = 2.000\n",
      "[batch 334]: seen 8576000 words at 1171.2 wps, loss = 1.998\n",
      "[batch 335]: seen 8601600 words at 1171.2 wps, loss = 1.997\n",
      "[batch 336]: seen 8627200 words at 1171.3 wps, loss = 1.996\n",
      "[batch 337]: seen 8652800 words at 1171.3 wps, loss = 1.994\n",
      "[batch 338]: seen 8678400 words at 1171.3 wps, loss = 1.993\n",
      "[batch 339]: seen 8704000 words at 1171.3 wps, loss = 1.992\n",
      "[batch 340]: seen 8729600 words at 1171.3 wps, loss = 1.991\n",
      "[batch 341]: seen 8755200 words at 1171.3 wps, loss = 1.989\n",
      "[batch 342]: seen 8780800 words at 1171.3 wps, loss = 1.988\n",
      "[batch 343]: seen 8806400 words at 1171.4 wps, loss = 1.987\n",
      "[batch 344]: seen 8832000 words at 1171.4 wps, loss = 1.985\n",
      "[batch 345]: seen 8857600 words at 1171.4 wps, loss = 1.984\n",
      "[batch 346]: seen 8883200 words at 1171.4 wps, loss = 1.983\n",
      "[batch 347]: seen 8908800 words at 1171.4 wps, loss = 1.982\n",
      "[batch 348]: seen 8934400 words at 1171.4 wps, loss = 1.980\n",
      "[batch 349]: seen 8960000 words at 1171.4 wps, loss = 1.979\n",
      "[batch 350]: seen 8985600 words at 1171.4 wps, loss = 1.978\n",
      "[batch 351]: seen 9011200 words at 1171.5 wps, loss = 1.977\n",
      "[batch 352]: seen 9036800 words at 1171.5 wps, loss = 1.975\n",
      "[batch 353]: seen 9062400 words at 1171.5 wps, loss = 1.974\n",
      "[batch 354]: seen 9088000 words at 1171.5 wps, loss = 1.973\n",
      "[batch 355]: seen 9113600 words at 1171.5 wps, loss = 1.972\n",
      "[batch 356]: seen 9139200 words at 1171.5 wps, loss = 1.971\n",
      "[batch 357]: seen 9164800 words at 1171.5 wps, loss = 1.969\n",
      "[batch 358]: seen 9190400 words at 1171.5 wps, loss = 1.968\n",
      "[batch 359]: seen 9216000 words at 1171.5 wps, loss = 1.967\n",
      "[batch 360]: seen 9241600 words at 1171.5 wps, loss = 1.966\n",
      "[batch 361]: seen 9267200 words at 1171.6 wps, loss = 1.965\n",
      "[batch 362]: seen 9292800 words at 1171.6 wps, loss = 1.964\n",
      "[batch 363]: seen 9318400 words at 1171.6 wps, loss = 1.962\n",
      "[batch 364]: seen 9344000 words at 1171.6 wps, loss = 1.961\n",
      "[batch 365]: seen 9369600 words at 1171.5 wps, loss = 1.960\n",
      "[batch 366]: seen 9395200 words at 1171.6 wps, loss = 1.959\n",
      "[batch 367]: seen 9420800 words at 1171.6 wps, loss = 1.958\n",
      "[batch 368]: seen 9446400 words at 1171.6 wps, loss = 1.957\n",
      "[batch 369]: seen 9472000 words at 1171.5 wps, loss = 1.955\n",
      "[batch 370]: seen 9497600 words at 1171.6 wps, loss = 1.954\n",
      "[batch 371]: seen 9523200 words at 1171.6 wps, loss = 1.953\n",
      "[batch 372]: seen 9548800 words at 1171.6 wps, loss = 1.952\n",
      "[batch 373]: seen 9574400 words at 1171.6 wps, loss = 1.951\n",
      "[batch 374]: seen 9600000 words at 1171.6 wps, loss = 1.950\n",
      "[batch 375]: seen 9625600 words at 1171.6 wps, loss = 1.949\n",
      "[batch 376]: seen 9651200 words at 1171.6 wps, loss = 1.948\n",
      "[batch 377]: seen 9676800 words at 1171.6 wps, loss = 1.947\n",
      "[batch 378]: seen 9702400 words at 1171.6 wps, loss = 1.945\n",
      "[batch 379]: seen 9728000 words at 1171.7 wps, loss = 1.944\n",
      "[batch 380]: seen 9753600 words at 1171.7 wps, loss = 1.943\n",
      "[batch 381]: seen 9779200 words at 1171.7 wps, loss = 1.942\n",
      "[batch 382]: seen 9804800 words at 1171.7 wps, loss = 1.941\n",
      "[batch 383]: seen 9830400 words at 1171.7 wps, loss = 1.940\n",
      "[batch 384]: seen 9856000 words at 1171.7 wps, loss = 1.939\n",
      "[batch 385]: seen 9881600 words at 1171.7 wps, loss = 1.937\n",
      "[batch 386]: seen 9907200 words at 1171.7 wps, loss = 1.936\n",
      "[batch 387]: seen 9932800 words at 1171.7 wps, loss = 1.935\n",
      "[batch 388]: seen 9958400 words at 1171.7 wps, loss = 1.934\n",
      "[batch 389]: seen 9984000 words at 1171.7 wps, loss = 1.933\n",
      "[batch 390]: seen 10009600 words at 1171.7 wps, loss = 1.932\n",
      "[batch 391]: seen 10035200 words at 1171.7 wps, loss = 1.931\n",
      "[batch 392]: seen 10060800 words at 1171.7 wps, loss = 1.930\n",
      "[batch 393]: seen 10086400 words at 1171.7 wps, loss = 1.929\n",
      "[batch 394]: seen 10112000 words at 1171.7 wps, loss = 1.928\n",
      "[batch 395]: seen 10137600 words at 1171.7 wps, loss = 1.927\n",
      "[batch 396]: seen 10163200 words at 1171.7 wps, loss = 1.926\n",
      "[batch 397]: seen 10188800 words at 1171.7 wps, loss = 1.924\n",
      "[batch 398]: seen 10214400 words at 1171.7 wps, loss = 1.923\n",
      "[batch 399]: seen 10240000 words at 1171.7 wps, loss = 1.922\n",
      "[batch 400]: seen 10265600 words at 1171.7 wps, loss = 1.921\n",
      "[batch 401]: seen 10291200 words at 1171.7 wps, loss = 1.920\n",
      "[batch 402]: seen 10316800 words at 1171.7 wps, loss = 1.919\n",
      "[batch 403]: seen 10342400 words at 1171.7 wps, loss = 1.918\n",
      "[batch 404]: seen 10368000 words at 1171.7 wps, loss = 1.917\n",
      "[batch 405]: seen 10393600 words at 1171.7 wps, loss = 1.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 406]: seen 10406656 words at 1171.7 wps, loss = 1.914\n",
      "[epoch 1] Completed in 2:28:01\n",
      "[epoch 1] Train set: avg. loss: 1.373  (perplexity: 3.95)\n",
      "[epoch 1] Test set: avg. loss: 1.378  (perplexity: 3.97)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "    \n",
    "    #check trainable variables\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n",
    "    \n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "    #lm.BuildSamplerGraph()\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "<SOR>the trom and i pick in lass-hot. the shows when ho\n",
      "<SOR>wings for my busy wrop feeling will back of the br\n",
      "<SOR>strone (maymoriandure dethy the gengroulder. close\n",
      "<SOR>here after the feels on eating attentive place and\n",
      "<SOR>is always upment you parked but that a pretty but \n",
      "<SOR>special\" it-was customer places...oh everyone that\n",
      "<SOR>highly recommend the down had. i ordered this only\n",
      "<SOR>is always clust on you can taste chicken! definite\n",
      "<SOR>cut - more meal favorite complaints, she serve sho\n",
      "<SOR>if you must car) charg for sandwich this mented us\n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "#max_steps = 20\n",
    "max_steps = 50\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    [char_dict.get(token) for token in test_review_list]\n",
    "    w = np.repeat([[char_dict.get('<SOR>')]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            #print(vocab.id_to_word[word_id], end=\" \")\n",
    "            print(ids_to_words[word_id], end=\"\")\n",
    "            #if (i != 0) and (word_id == vocab.START_ID):\n",
    "            if (i != 0) and (word_id == char_dict.get(\"<EOR>\")):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linguistic Properties\n",
    "\n",
    "Given two or more test sentences, the model should score the more plausible (or more correct) sentence with a higher log-probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"once upon a time\" : -8.74\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.83\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"once upon a time\",\n",
    "         #\"the quick brown fox jumps over the lazy dog\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = [\"is sentence nonsense this\", \"i drive a car\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -6.03\n",
      "\"the boy and the girl is\" : -5.90\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"the boy and the girl are\", \"the boy and the girl is\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -7.46\n",
      "\"peanuts are my favorite kind of vegetable\" : -7.29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"when I'm hungry I really prefer to eat\" : -7.49\n",
      "\"when I'm hungry I really prefer to drink\" : -7.52\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"peanuts are my favorite kind of nut\",\n",
    "         #\"peanuts are my favorite kind of vegetable\"]\n",
    "#load_and_score([s.split() for s in sents])\n",
    "\n",
    "#sents = [\"when I'm hungry I really prefer to eat\",\n",
    "         #\"when I'm hungry I really prefer to drink\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"I have lots of plastic green square toys\" : -8.51\n",
      "\"I have lots of green square plastic toys\" : -8.56\n",
      "\"I have lots of green plastic square toys\" : -8.57\n",
      "\"I have lots of plastic square green toys\" : -8.64\n",
      "\"I have lots of square green plastic toys\" : -8.69\n",
      "\"I have lots of square plastic green toys\" : -8.71\n"
     ]
    }
   ],
   "source": [
    "#prefix = \"I have lots of\".split()\n",
    "#noun = \"toys\"\n",
    "#adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "#inputs = []\n",
    "#for adjs in itertools.permutations(adjectives):\n",
    "    #words = prefix + list(adjs) + [noun]\n",
    "    #inputs.append(words)\n",
    "    \n",
    "#load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
