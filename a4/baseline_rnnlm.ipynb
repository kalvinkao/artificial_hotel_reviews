{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Character-Level Language Model\n",
    "\n",
    "This is the \"working notebook\", with code to load and train the model, as well as run unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "#wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "#embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "#x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "#print('wordid placeholder shape:', wordid_ph.shape)\n",
    "#print('x shape:', x.shape)\n",
    "\n",
    "#sess = tf.Session()\n",
    "## Two sentences, each with four words.\n",
    "#wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "#x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "#print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "#print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "Load implementation, construct graph, and write a logdir for TensorBoard.\n",
    "```\n",
    "cd assignment/a4\n",
    "tensorboard --logdir /tmp/w266/a4_graph --port 6006\n",
    "```\n",
    "http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/artificial_hotel_reviews/a4_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  W_in:0\n",
      "Shape:  (10000, 200)\n",
      "[[-0.78748155 -0.05848813 -0.64172769 ...,  0.84141541 -0.94063973\n",
      "   0.6318059 ]\n",
      " [ 0.73271012  0.9191277   0.16166019 ..., -0.77998519  0.349545\n",
      "  -0.45933056]\n",
      " [ 0.56432152 -0.03329587  0.20995927 ..., -0.3828721   0.12824035\n",
      "  -0.13679957]\n",
      " ..., \n",
      " [-0.12901855 -0.29448342 -0.08786106 ..., -0.73254704 -0.37692833\n",
      "  -0.20111871]\n",
      " [-0.62848496 -0.50970197  0.9452734  ...,  0.99917698 -0.81992722\n",
      "  -0.37649083]\n",
      " [-0.09145284  0.00680947  0.67164588 ..., -0.92668557  0.46153927\n",
      "   0.74277234]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.02885439 -0.00585808 -0.04069098 ...,  0.00699186 -0.05404399\n",
      "   0.02202466]\n",
      " [ 0.06509807 -0.02223223  0.02077565 ..., -0.00037639  0.04783005\n",
      "  -0.00804034]\n",
      " [ 0.02742642  0.05705827 -0.03898749 ...,  0.00316557  0.06507974\n",
      "  -0.01836298]\n",
      " ..., \n",
      " [-0.00754741 -0.04928695  0.06990113 ..., -0.06414328 -0.06926721\n",
      "  -0.05963309]\n",
      " [-0.06505661 -0.0187963   0.05553678 ..., -0.00105456 -0.04487306\n",
      "   0.05926735]\n",
      " [ 0.05089028  0.03582108 -0.05539448 ...,  0.00139427  0.06008063\n",
      "   0.05134535]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.06411717 -0.059903    0.05128169 ...,  0.03041378  0.05353859\n",
      "   0.0544473 ]\n",
      " [ 0.06030189  0.02777085  0.0119326  ..., -0.00028028 -0.04805488\n",
      "  -0.03846691]\n",
      " [-0.02244554  0.04396319  0.06136966 ...,  0.02755227  0.01427022\n",
      "   0.0558702 ]\n",
      " ..., \n",
      " [-0.0622274  -0.03975505 -0.03073074 ...,  0.0241474  -0.04965901\n",
      "  -0.00885601]\n",
      " [ 0.0227494  -0.0019692  -0.05720998 ..., -0.03900233 -0.02878788\n",
      "  -0.05673516]\n",
      " [-0.04740062  0.00044262  0.0118935  ...,  0.02134912  0.04072783\n",
      "   0.00831893]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  W_out:0\n",
      "Shape:  (200, 10000)\n",
      "[[ 0.22999096 -0.98074627  0.2485311  ...,  0.31636119  0.64180779\n",
      "   0.48168755]\n",
      " [ 0.02860689  0.14025211  0.73703051 ...,  0.55039978 -0.11227965\n",
      "   0.06153202]\n",
      " [-0.74636269 -0.50103402  0.13248014 ..., -0.45557976  0.15847898\n",
      "   0.09724474]\n",
      " ..., \n",
      " [-0.21404767  0.9171977  -0.29322004 ..., -0.82013679  0.63434672\n",
      "  -0.56355882]\n",
      " [ 0.83722639 -0.26542854 -0.89726591 ..., -0.59170508  0.18550444\n",
      "   0.48118067]\n",
      " [ 0.55475283 -0.74218535 -0.24593616 ..., -0.88373137 -0.86611366\n",
      "  -0.00220013]]\n",
      "Variable:  b_out:0\n",
      "Shape:  (10000,)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#with lm.graph.as_default():\n",
    "    #initializer = tf.global_variables_initializer()\n",
    "\n",
    "#with tf.Session(graph=lm.graph) as session:\n",
    "    #session.run(initializer)\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unit Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 3.315s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNNLM\n",
    "\n",
    "Data loader functions in **`utils.py`**.\n",
    "\n",
    "`utils.rnnlm_batch_generator` returns an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "Toy corpus example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_epoch()\n",
    "\n",
    "`train=True` flag enables train mode. `train=False` runs `score_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Unit Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 0]: seen 50 words at 43.1 wps, loss = 2.028\n",
      "[batch 98]: seen 4950 words at 2284.2 wps, loss = 1.114\n",
      "[batch 210]: seen 10550 words at 3329.2 wps, loss = 0.959\n",
      "[batch 346]: seen 17350 words at 4155.3 wps, loss = 1.024\n",
      "[batch 481]: seen 24100 words at 4656.4 wps, loss = 0.983\n",
      "[batch 615]: seen 30800 words at 4987.1 wps, loss = 0.943\n",
      "[batch 751]: seen 37600 words at 5237.5 wps, loss = 0.954\n",
      "[batch 887]: seen 44400 words at 5425.6 wps, loss = 0.964\n",
      "[batch 1024]: seen 51250 words at 5579.2 wps, loss = 0.954\n",
      "[batch 1160]: seen 58050 words at 5697.4 wps, loss = 0.943\n",
      "[batch 1296]: seen 64850 words at 5794.1 wps, loss = 0.930\n",
      "[batch 1431]: seen 71600 words at 5872.2 wps, loss = 0.921\n",
      "[batch 1567]: seen 78400 words at 5940.1 wps, loss = 0.917\n",
      "[batch 1702]: seen 85150 words at 5995.7 wps, loss = 0.905\n",
      "[batch 1839]: seen 92000 words at 6050.8 wps, loss = 0.909\n",
      "[batch 1976]: seen 98850 words at 6098.0 wps, loss = 0.914\n",
      "[batch 2112]: seen 105650 words at 6138.3 wps, loss = 0.906\n",
      "[batch 2245]: seen 112300 words at 6164.1 wps, loss = 0.893\n",
      "[batch 2379]: seen 119000 words at 6191.6 wps, loss = 0.883\n",
      "[batch 2516]: seen 125850 words at 6222.1 wps, loss = 0.881\n",
      "[batch 2650]: seen 132550 words at 6243.5 wps, loss = 0.886\n",
      "[batch 2786]: seen 139350 words at 6268.4 wps, loss = 0.880\n",
      "[batch 2919]: seen 146000 words at 6283.9 wps, loss = 0.875\n",
      "[batch 3055]: seen 152800 words at 6303.9 wps, loss = 0.870\n",
      "[batch 3189]: seen 159500 words at 6319.6 wps, loss = 0.874\n",
      "[batch 3323]: seen 166200 words at 6334.0 wps, loss = 0.878\n",
      "[batch 3456]: seen 172850 words at 6344.5 wps, loss = 0.889\n",
      "[batch 3587]: seen 179400 words at 6350.9 wps, loss = 0.886\n",
      "[batch 3717]: seen 185900 words at 6355.3 wps, loss = 0.880\n",
      "[batch 3844]: seen 192250 words at 6353.9 wps, loss = 0.873\n",
      "[batch 3970]: seen 198550 words at 6351.4 wps, loss = 0.870\n",
      "Train set: avg. loss: 0.018  (perplexity: 1.02)\n",
      "Test set: avg. loss: 0.038  (perplexity: 1.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 33.626s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "At the end of training, `tf.train.Saver` saves a copy of the model to `/tmp/w266/artificial_hotel_reviews/rnnlm_trained`. To load this from disk, see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/kalvin_kao/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "## Load the dataset\n",
    "#V = 10000\n",
    "#vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv(review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_csv(business_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_review_df = review_df[review_df['stars']==5]\n",
    "five_star_review_series = five_star_review_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in five_star_review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "        #print(final_review)\n",
    "        review_list.append(final_review)\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed reviews\n",
    "#review_list = preprocess_review_series(five_star_review_series)\n",
    "review_list = preprocess_review_series(five_star_review_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10406707\n"
     ]
    }
   ],
   "source": [
    "#combined_review_list = [item for sublist in review_list for item in sublist]\n",
    "training_review_list = [item for sublist in review_list[:20000] for item in sublist]\n",
    "print(len(training_review_list))\n",
    "test_review_list = [item for sublist in review_list[20000:21000] for item in sublist]\n",
    "#unique_characters = list(set(combined_review_list))\n",
    "unique_characters = list(set(training_review_list + test_review_list))\n",
    "#len(unique_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary\n",
    "char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#print(char_dict)\n",
    "#print(ids_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "print(len(char_dict.keys()))\n",
    "print(len(unique_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to flat (1D) np.array(int) of ids\n",
    "#add memory to VM and remove 1000 slice\n",
    "#combined_review_ids = [char_dict.get(token) for token in combined_review_list[:1000]]\n",
    "training_review_ids = [char_dict.get(token) for token in training_review_list]\n",
    "test_review_ids = [char_dict.get(token) for token in test_review_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array(training_review_ids)\n",
    "test_ids = np.array(test_review_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "## add parameter sets for each attack/defense configuration\n",
    "#max_time = 25\n",
    "max_time = 100\n",
    "#batch_size = 100\n",
    "batch_size = 256\n",
    "#learning_rate = 0.01\n",
    "learning_rate = 0.002\n",
    "#num_epochs = 10\n",
    "num_epochs = 1\n",
    "\n",
    "# Model parameters\n",
    "#model_params = dict(V=vocab.size, \n",
    "                    #H=200, \n",
    "                    #softmax_ns=200,\n",
    "                    #num_layers=2)\n",
    "model_params = dict(V=len(unique_characters), \n",
    "                    H=1024, \n",
    "                    softmax_ns=len(unique_characters),\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 0]: seen 25600 words at 1064.6 wps, loss = 4.762\n",
      "[batch 1]: seen 51200 words at 1117.5 wps, loss = 10.085\n",
      "[batch 2]: seen 76800 words at 1137.5 wps, loss = 11.354\n",
      "[batch 3]: seen 102400 words at 1146.2 wps, loss = 11.476\n",
      "[batch 4]: seen 128000 words at 1149.0 wps, loss = 10.534\n",
      "[batch 5]: seen 153600 words at 1153.5 wps, loss = 10.008\n",
      "[batch 6]: seen 179200 words at 1156.0 wps, loss = 9.317\n",
      "[batch 7]: seen 204800 words at 1158.1 wps, loss = 8.666\n",
      "[batch 8]: seen 230400 words at 1159.9 wps, loss = 8.107\n",
      "[batch 9]: seen 256000 words at 1161.1 wps, loss = 7.629\n",
      "[batch 10]: seen 281600 words at 1162.9 wps, loss = 7.217\n",
      "[batch 11]: seen 307200 words at 1163.7 wps, loss = 6.858\n",
      "[batch 12]: seen 332800 words at 1164.4 wps, loss = 6.545\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "    \n",
    "    #check trainable variables\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n",
    "    \n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "    #lm.BuildSamplerGraph()\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "<SOR>getled (but i was happy would get while you grow o\n",
      "<SOR>5th hot clarl with because. we didn't lip the mill\n",
      "<SOR>briking a most happy i should in the most, a life \n",
      "<SOR>a nice one which kerp (conesteagily to long and wh\n",
      "<SOR>ryly experience to make the bar requested kee busi\n",
      "<SOR>were alla lots of spot for juices me off the cuper\n",
      "<SOR>hopes we got only but if she would have never been\n",
      "<SOR>friendly and constructors/good!<EOR>\n",
      "<SOR>down again after a shops that was nice and stop to\n",
      "<SOR>britted as well. appetizers indian food tacos he n\n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "#max_steps = 20\n",
    "max_steps = 50\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    [char_dict.get(token) for token in test_review_list]\n",
    "    w = np.repeat([[char_dict.get('<SOR>')]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            #print(vocab.id_to_word[word_id], end=\" \")\n",
    "            print(ids_to_words[word_id], end=\"\")\n",
    "            #if (i != 0) and (word_id == vocab.START_ID):\n",
    "            if (i != 0) and (word_id == char_dict.get(\"<EOR>\")):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linguistic Properties\n",
    "\n",
    "Given two or more test sentences, the model should score the more plausible (or more correct) sentence with a higher log-probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"once upon a time\" : -8.74\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.83\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"once upon a time\",\n",
    "         #\"the quick brown fox jumps over the lazy dog\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = [\"is sentence nonsense this\", \"i drive a car\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -6.03\n",
      "\"the boy and the girl is\" : -5.90\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"the boy and the girl are\", \"the boy and the girl is\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -7.46\n",
      "\"peanuts are my favorite kind of vegetable\" : -7.29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"when I'm hungry I really prefer to eat\" : -7.49\n",
      "\"when I'm hungry I really prefer to drink\" : -7.52\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"peanuts are my favorite kind of nut\",\n",
    "         #\"peanuts are my favorite kind of vegetable\"]\n",
    "#load_and_score([s.split() for s in sents])\n",
    "\n",
    "#sents = [\"when I'm hungry I really prefer to eat\",\n",
    "         #\"when I'm hungry I really prefer to drink\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"I have lots of plastic green square toys\" : -8.51\n",
      "\"I have lots of green square plastic toys\" : -8.56\n",
      "\"I have lots of green plastic square toys\" : -8.57\n",
      "\"I have lots of plastic square green toys\" : -8.64\n",
      "\"I have lots of square green plastic toys\" : -8.69\n",
      "\"I have lots of square plastic green toys\" : -8.71\n"
     ]
    }
   ],
   "source": [
    "#prefix = \"I have lots of\".split()\n",
    "#noun = \"toys\"\n",
    "#adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "#inputs = []\n",
    "#for adjs in itertools.permutations(adjectives):\n",
    "    #words = prefix + list(adjs) + [noun]\n",
    "    #inputs.append(words)\n",
    "    \n",
    "#load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
