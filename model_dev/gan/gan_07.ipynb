{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load Dependencies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os, shutil, time\n",
    "#from importlib import reload\n",
    "from imp import reload\n",
    "#import collections, itertools\n",
    "import unittest\n",
    "#from trainer import unittest\n",
    "#from . import unittest\n",
    "#import trainer.unittest as unittest\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from trainer import utils#, vocabulary, tf_embed_viz\n",
    "#import utils\n",
    "#import trainer.utils as utils\n",
    "\n",
    "# rnnlm code\n",
    "from trainer import rnnlm\n",
    "#import trainer.rnnlm as rnnlm\n",
    "#import rnnlm\n",
    "reload(rnnlm)\n",
    "#from trainer import rnnlm_test\n",
    "#import trainer.rnnlm_test as rnnlm_test\n",
    "#reload(rnnlm_test)\n",
    "#from . import rnnlm; reload(rnnlm)\n",
    "#from . import rnnlm_test; reload(rnnlm_test)\n",
    "#import rnnlm; reload(rnnlm)\n",
    "#import rnnlm_test; reload(rnnlm_test)\n",
    "# packages for extracting data\n",
    "import pandas as pd\n",
    "\n",
    "#import cloudstorage as gcs\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_tensorboard(tf_graphdir=\"/tmp/artificial_hotel_reviews/a4_graph\", V=100, H=1024, num_layers=2):\n",
    "    reload(rnnlm)\n",
    "    TF_GRAPHDIR = tf_graphdir\n",
    "    # Clear old log directory.\n",
    "    shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "    \n",
    "    lm = rnnlm.RNNLM(V=V, H=H, num_layers=num_layers)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "    return summary_writer\n",
    "\n",
    "# Unit Tests\n",
    "def test_graph():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])\n",
    "\n",
    "def test_training():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "    th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "    unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training Functions\n",
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=5, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step0_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, batch_size, verbose=False, tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "    train_op = lm.train_step_\n",
    "    loss = lm.loss_cnn\n",
    "\n",
    "    #if train:\n",
    "        #train_op = lm.train_step_\n",
    "        #use_dropout = True\n",
    "        ##loss = lm.train_loss_\n",
    "        #loss = lm.loss_cnn\n",
    "    #else:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False  # no dropout at test time\n",
    "        #loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "        #loss = lm.loss_cnn\n",
    "\n",
    "    ### UPDATED!!! MUST PASS IN TRAIN_IDS as a list of lists, batch_size\n",
    "    num_samples = 2*batch_size\n",
    "    total_reviews = len(train_list)\n",
    "    num_batches_per_epoch = int((total_reviews-1)/batch_size) + 1\n",
    "    for batch in range(num_batches_per_epoch):\n",
    "        print(\"gan batch: \", batch)\n",
    "        start_index = batch*batch_size\n",
    "        end_index = min((batch+1) * batch_size, total_reviews)\n",
    "        current_training_batch = train_list[start_index:end_index]\n",
    "        #min_review_length = min(len(review) for review in current_training_batch)\n",
    "        min_review_length = 300 #based on the selection criteria when extracting data.  limits learning to ~60 word context\n",
    "        #average_review_length = sum([len(review) for review in current_training_batch])/len(current_training_batch)\n",
    "        #max_steps = 2.0*average_review_length\n",
    "        max_steps = 325\n",
    "        \n",
    "        #for training_review in current_training_batch:\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)#MUST PASS IN WORDS_TO_IDS\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #can gather all outputs from this loop if you get to it\n",
    "        #get the cell state and timestep 300\n",
    "        h_artificial_300 = None\n",
    "        for i in range(max_steps):\n",
    "            if i == min_review_length:\n",
    "                h_artificial_300 = h[0][1]\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "        artificial_review_final_states = []\n",
    "        artificial_review_ids = []\n",
    "        #for row in w:\n",
    "        for a, row in enumerate(h_artificial_300):\n",
    "            #print(\"generated during training\", end=\":  \")\n",
    "            new_artificial_review = []\n",
    "            for b, word_id in enumerate(w[a]):\n",
    "                new_artificial_review.append(word_id)\n",
    "                #print(ids_to_words[word_id], end=\"\")\n",
    "                if (b != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            #print(\"\")\n",
    "            #if len(new_artificial_review) >= 0.75*average_review_length:\n",
    "            if len(new_artificial_review) >= min_review_length:\n",
    "                artificial_review_ids.append(new_artificial_review)\n",
    "                artificial_review_final_states.append(row)\n",
    "        \n",
    "        print(\"generated during training batch \", batch, \":\")\n",
    "        for review in artificial_review_ids[:25]:\n",
    "            for word_id in review:\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "            print()\n",
    "        #print(artificial_review_ids[:5])\n",
    "        #now you have current_training_batch and artificial_review_ids\n",
    "        #clip all data to the same length for simplicity\n",
    "        num_reviews = min(len(current_training_batch), len(artificial_review_ids))\n",
    "        print(\"gan batch: \", batch, \" has \", num_reviews, \n",
    "              \" real reviews and \", num_reviews, \" artificial reviews.\")\n",
    "        \n",
    "        if num_reviews == 0:\n",
    "            continue\n",
    "        \n",
    "        current_training_batch = [review[:min_review_length] for review in current_training_batch]\n",
    "        #not sure this is needed\n",
    "        artificial_review_ids = [review[:min_review_length] for review in artificial_review_ids]\n",
    "        \n",
    "        ##get final states for real reviews\n",
    "        #w_training = np.array(current_training_batch)\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w_training})\n",
    "        #feed_dict = {lm.input_w_:w_training,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: False,\n",
    "                     #lm.initial_h_:h}\n",
    "        #h_real = session.run([lm.final_h_],feed_dict=feed_dict)#no training, just get states\n",
    "        \n",
    "        #real_review_final_states = []\n",
    "        #prob don't need to do anything to h_real\n",
    "        #for row in h_real:\n",
    "            #if len(new_artificial_review) >= 300:\n",
    "                #artificial_review_ids.append(new_artificial_review)\n",
    "                #artificial_review_final_states.append(row)\n",
    "                \n",
    "        #now let's even out the number of examples\n",
    "        current_training_batch = current_training_batch[:num_reviews]\n",
    "        artificial_review_ids = artificial_review_ids[:num_reviews]\n",
    "        #print(h_real[0])\n",
    "        ##print(h_real[1])#wtf why won't this work\n",
    "        ##print(h_real[1].shape)\n",
    "        ##h_real = h_real[1][:num_reviews]\n",
    "        #h_real = h_real[:num_reviews]\n",
    "        #print(h_real)\n",
    "        #artificial_review_final_states[:num_reviews]\n",
    "        real_review_list_for_retraining_softmax = current_training_batch[:]\n",
    "        \n",
    "        #label each review and shuffle data\n",
    "        current_training_batch = [(review,[1,0]) for review in current_training_batch]#might need to swap labels\n",
    "        artificial_review_training_batch = [(np.array(review),[0,1]) for review in artificial_review_ids]\n",
    "        #print(current_training_batch[:10])\n",
    "        #print()\n",
    "        #print(artificial_review_ids[:10])\n",
    "        #print()\n",
    "        ##label each review hidden state and shuffle\n",
    "        #training_states = [(review,np.array([0,1])) for review in h_real]#might need to swap labels\n",
    "        #artificial_review_states = [(review,np.array([1,0])) for review in artificial_review_final_states]\n",
    "        \n",
    "        #combine training lists\n",
    "        current_training_batch.extend(artificial_review_training_batch)\n",
    "        np.random.shuffle(current_training_batch)\n",
    "        \n",
    "        #print(current_training_batch[:10])\n",
    "        #print()\n",
    "        #np.random.shuffle(training_states)\n",
    "        #print(training_states)\n",
    "        \n",
    "        #train discriminator\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_list, labels = zip(*current_training_batch)\n",
    "        #review_states, labels = zip(*training_states)\n",
    "        #convert to matrix form\n",
    "        #print(labels)\n",
    "        w = np.array(list(review_list))\n",
    "        #w = np.array(list(review_states))\n",
    "        #y = np.array(list(labels))\n",
    "        y = np.array(labels)\n",
    "        #print(w.shape)\n",
    "        #print(y.shape)\n",
    "        #print()\n",
    "        #batching is done at the review level\n",
    "        #the whole review is fed in at once, so initialize h first\n",
    "        #if batch == 0:\n",
    "        \n",
    "        #train CNN classifier\n",
    "        train_op = lm.train_step_\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_x:w,\n",
    "                     #lm.input_y:y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True}\n",
    "        feed_dict = {lm.input_w_: w, \n",
    "                     lm.input_y: y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True, \n",
    "                     lm.initial_h_: h}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"discriminator training accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"discriminator training cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        ##train discriminator on fake reviews\n",
    "        ##now unzip into \"w\" and \"y\"\n",
    "        #review_list, labels = zip(*artificial_review_ids)\n",
    "        #w = np.array(list(review_list))\n",
    "        #y = np.array(labels)\n",
    "        \n",
    "        ##train CNN classifier\n",
    "        #train_op = self.train_step_\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_w_: w, \n",
    "                     #lm.input_y: y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True, \n",
    "                     #lm.initial_h_: h}\n",
    "        #accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        #print(\"fake review accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        #print(\"fake review cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #train generator\n",
    "        #relabel fake reviews as real\n",
    "        artificial_review_training_batch = [(np.array(review),[1,0]) for review in artificial_review_ids]\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_list, labels = zip(*artificial_review_training_batch)\n",
    "        w = np.array(list(review_list))\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        #train CNN classifier\n",
    "        train_op = lm.train_step1_\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_x:w,\n",
    "                     #lm.input_y:y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True}\n",
    "        feed_dict = {lm.input_w_: w, \n",
    "                     lm.input_y: y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True, \n",
    "                     lm.initial_h_: h}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"generator training accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"generator training cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #retrain softmax of rnn on real reviews\n",
    "        flattened_real_ids = np.array([item for sublist in real_review_list_for_retraining_softmax for item in sublist])\n",
    "        #print(flattened_real_ids[:10])\n",
    "        #print(flattened_real_ids.shape)\n",
    "        #print()\n",
    "        bi = utils.rnnlm_batch_generator(flattened_real_ids, batch_size, max_time=150)\n",
    "        for i, (w, y) in enumerate(bi):\n",
    "            cost = 0.0\n",
    "            if i == 0:\n",
    "                h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            feed_dict = {lm.input_w_: w, \n",
    "                         lm.target_y_: y, \n",
    "                         lm.learning_rate_: learning_rate,\n",
    "                         lm.use_dropout_: False,\n",
    "                         lm.initial_h_:h}\n",
    "            cost, h, _ = session.run([lm.train_loss_, lm.final_h_, lm.train_step_softmax_],feed_dict=feed_dict)\n",
    "            print(\"softmax re-training cost for gan batch \", batch, \" is: \", cost)\n",
    "    \n",
    "    ### UPDATED!!!\n",
    "    #total_cost += cost#update\n",
    "    #total_batches = batch + 1\n",
    "    #total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ###\n",
    "    ## Print average loss-so-far for epoch\n",
    "    ## If using train_loss_, this may be an underestimate.\n",
    "    #if verbose and (time.time() - tick_time >= tick_s):\n",
    "        #avg_cost = total_cost / total_batches\n",
    "        #avg_wps = total_words / (time.time() - start_time)\n",
    "        #print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "            #batch, total_words, avg_wps, avg_cost))\n",
    "        #tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        if len(semifinal_review) > 300:\n",
    "            final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "            #print(final_review)\n",
    "            review_list.append(final_review)\n",
    "    return review_list\n",
    "\n",
    "def get_review_series(review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    review_df = pd.read_csv(review_path)\n",
    "    five_star_review_df = review_df[review_df['stars']==5]\n",
    "    #five_star_review_series = five_star_review_df['text']\n",
    "    return five_star_review_df['text']\n",
    "\n",
    "def get_business_list(business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'):\n",
    "    #business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'\n",
    "    return pd.read_csv(business_path)\n",
    "\n",
    "def split_train_test(review_list, training_samples, test_samples):\n",
    "    #pass in randomized review list\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return randomized_training_list, randomized_testing_list\n",
    "\n",
    "def make_train_test_data(five_star_review_series, training_samples=20000, test_samples=1000):\n",
    "    #fix randomization to prevent evaluation on trained samples\n",
    "    review_list = preprocess_review_series(five_star_review_series)\n",
    "    #split and shuffle the data\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    np.random.shuffle(review_list)\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    #training_review_list = [item for sublist in review_list[:training_samples] for item in sublist]\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    \n",
    "    #test_review_list = [item for sublist in review_list[training_samples:training_samples+test_samples] for item in sublist]\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return training_review_list, test_review_list\n",
    "\n",
    "\n",
    "#def make_vocabulary(training_review_list, test_review_list):\n",
    "#    unique_characters = list(set(training_review_list + test_review_list))\n",
    "#    #vocabulary\n",
    "#    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "#    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#    return char_dict, ids_to_words\n",
    "def make_vocabulary(dataset_list):\n",
    "    unique_characters = list(set().union(*dataset_list))\n",
    "    #unique_characters = list(set(training_review_list + test_review_list))\n",
    "    #vocabulary\n",
    "    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "    return char_dict, ids_to_words\n",
    "\n",
    "def convert_to_ids(char_dict, review_list):\n",
    "    #convert to flat (1D) np.array(int) of ids\n",
    "    review_ids = [char_dict.get(token) for token in review_list]\n",
    "    return np.array(review_ids)\n",
    "\n",
    "def run_training(train_list, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20):\n",
    "    #V = len(words_to_ids.keys())\n",
    "    # Training parameters\n",
    "    ## add parameter sets for each attack/defense configuration\n",
    "    #max_time = 25\n",
    "    #batch_size = 100\n",
    "    #learning_rate = 0.01\n",
    "    #num_epochs = 10\n",
    "    \n",
    "    # Model parameters\n",
    "    #model_params = dict(V=vocab.size, \n",
    "                        #H=200, \n",
    "                        #softmax_ns=200,\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=len(words_to_ids.keys()), \n",
    "                        #H=1024, \n",
    "                        #softmax_ns=len(words_to_ids.keys()),\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=V, H=H, softmax_ns=softmax_ns, num_layers=num_layers)\n",
    "    \n",
    "    #TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "    TF_SAVEDIR = tf_savedir\n",
    "    checkpoint_filename = os.path.join(TF_SAVEDIR, \"gan\")\n",
    "    trained_filename = os.path.join(TF_SAVEDIR, \"gan_trained\")\n",
    "    \n",
    "    # Will print status every this many seconds\n",
    "    #print_interval = 5\n",
    "    print_interval = 30\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    ### UPDATED!!!\n",
    "    lm.BuildSamplerGraph()\n",
    "    num_pretrain = 20000\n",
    "    #num_pretrain = 200#low number for testing\n",
    "    #pretrain on a different set of training data each time\n",
    "    #train and test ids \n",
    "    #pre_train_ids =\n",
    "    #train_ids = \n",
    "    ### UPDATED!!!\n",
    "    \n",
    "    # Explicitly add global initializer and variable saver to LM graph\n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Clear old log directory\n",
    "    shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "    if not os.path.isdir(TF_SAVEDIR):\n",
    "        os.makedirs(TF_SAVEDIR)\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(42)\n",
    "    \n",
    "        session.run(initializer)\n",
    "        \n",
    "        #check trainable variables\n",
    "        #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "        #values = session.run(variables_names)\n",
    "        #for k, v in zip(variables_names, values):\n",
    "            #print(\"Variable: \", k)\n",
    "            #print(\"Shape: \", v.shape)\n",
    "            #print(v)\n",
    "    \n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            np.random.shuffle(train_list)\n",
    "            #shuffled_train_list = np.random.shuffle(train_list)\n",
    "            #pre_train_ids = [item for sublist in shuffled_train_list[:num_pretrain] for item in sublist]\n",
    "            #gan_train_list = shuffled_train_list[num_pretrain:]\n",
    "            #train_list = \n",
    "            pre_train_ids = np.array([item for sublist in train_list[:num_pretrain] for item in sublist])\n",
    "            #print(pre_train_ids.shape)\n",
    "            #pre_train_ids = np.array(pre_train_ids)\n",
    "            gan_train_list = train_list[num_pretrain:]\n",
    "            if num_pretrain > 0:\n",
    "                bi = utils.rnnlm_batch_generator(pre_train_ids, batch_size, max_time)\n",
    "                print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "                # Run a pretraining epoch.\n",
    "                run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "                print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "        \n",
    "            #now train gan\n",
    "            #run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, verbose=False, tick_s=10, learning_rate=None)\n",
    "            run_gan_epoch(lm, session, words_to_ids, ids_to_words, gan_train_list, batch_size, \n",
    "                          verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "        \n",
    "            # Save a checkpoint\n",
    "            saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "        \n",
    "            ##\n",
    "            # score_dataset will run a forward pass over the entire dataset\n",
    "            # and report perplexity scores. This can be slow (around 1/2 to \n",
    "            # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "            # to speed up training on a slow machine. Be sure to run it at the \n",
    "            # end to evaluate your score.\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, pre_train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "        return trained_filename\n",
    "\n",
    "def get_char_probs(trained_filename, model_params, test_ids):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    all_review_likelihoods = []\n",
    "    train_op = tf.no_op()\n",
    "    use_dropout = False\n",
    "    loss = lm.loss_\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False\n",
    "        #loss = lm.loss_\n",
    "        \n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        for review in test_ids:\n",
    "            review_likelihoods = []\n",
    "            inputs = review[:-1]\n",
    "            labels = review[1:]\n",
    "            inputs_labels = zip(inputs,labels)\n",
    "            for i, (w,y) in enumerate(inputs_labels):\n",
    "                \n",
    "                w = np.array(w)\n",
    "                y = np.array(y)\n",
    "                w = w.reshape([1,1])\n",
    "                y = y.reshape([1,1])\n",
    "                \n",
    "                if i == 0:\n",
    "                    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "                feed_dict = {lm.input_w_:w, \n",
    "                             lm.target_y_:y,\n",
    "                             lm.learning_rate_: 0.002,\n",
    "                             lm.use_dropout_: use_dropout,\n",
    "                             lm.initial_h_:h}\n",
    "                cost, h = session.run([loss, lm.final_h_],feed_dict=feed_dict)\n",
    "                likelihood = 2**(-1*cost)\n",
    "                review_likelihoods.append(likelihood)\n",
    "            all_review_likelihoods.append(review_likelihoods)\n",
    "    return all_review_likelihoods\n",
    "\n",
    "## Sampling\n",
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "\n",
    "    # Run sample ops\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]\n",
    "\n",
    "def generate_text(trained_filename, model_params, words_to_ids, ids_to_words):\n",
    "    # Same as above, but as a batch\n",
    "    #max_steps = 20\n",
    "    max_steps = 300\n",
    "    #num_samples = 40000\n",
    "    num_samples = 40\n",
    "    random_seed = 42\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(random_seed)\n",
    "        \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "    \n",
    "        # Make initial state for a batch with batch_size = num_samples\n",
    "        #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        # take one step for each sequence on each iteration \n",
    "        for i in range(max_steps):\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "    \n",
    "        # Print generated sentences\n",
    "        for row in w:\n",
    "            print(trained_filename, end=\":  \")\n",
    "            for i, word_id in enumerate(row):\n",
    "                #print(vocab.id_to_word[word_id], end=\" \")\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                #if (i != 0) and (word_id == vocab.START_ID):\n",
    "                if (i != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")\n",
    "\n",
    "def train_attack_model(training_samples=20000, test_samples=1000, review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #training_samples=20000\n",
    "    #test_samples=1000\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    start_format = time.time()\n",
    "    five_star_reviews = get_review_series(review_path)\n",
    "    train_review_list, test_review_list = make_train_test_data(five_star_reviews, training_samples, test_samples)\n",
    "    words_to_ids, ids_to_words = make_vocabulary(train_review_list, test_review_list)\n",
    "    train_ids = convert_to_ids(words_to_ids, train_review_list)\n",
    "    test_ids = convert_to_ids(words_to_ids, test_review_list)\n",
    "    end_format = time.time()\n",
    "    print(\"data formatting took \" + str(end_format-start_format) + \" seconds\")\n",
    "    model_params = dict(V=len(words_to_ids.keys()), \n",
    "                            H=1024, \n",
    "                            softmax_ns=len(words_to_ids.keys()),\n",
    "                            num_layers=2)\n",
    "    #run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=150, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    return trained_filename, model_params, words_to_ids, ids_to_words\n",
    "\n",
    "def neg_log_lik_ratio(likelihoods_real, likelihoods_artificial):\n",
    "    predictions = []\n",
    "    combined = zip(likelihoods_real, likelihoods_artificial)\n",
    "    for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "        negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "        #averaged_llrs = negative_log_lik_ratios[:-1]/(len(negative_log_lik_ratios)-1)\n",
    "        averaged_llrs = np.sum(negative_log_lik_ratios[:-1])/(len(negative_log_lik_ratios)-1)\n",
    "        predictions.append(averaged_llrs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data download took 4.29355525970459 seconds\n"
     ]
    }
   ],
   "source": [
    "start_dl = time.time()\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_train_data_03.csv .')\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_test_data_03.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_train_data_01.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_test_data_01.csv .')\n",
    "end_dl = time.time()\n",
    "print(\"data download took \" + str(end_dl-start_dl) + \" seconds\")\n",
    "#gsutil cp gs://[BUCKET_NAME]/[OBJECT_NAME] [OBJECT_DESTINATION]\n",
    "real_train_review_path = './split01_train_data_02.csv'\n",
    "real_test_review_path = './split01_test_data_02.csv'\n",
    "#artificial_train_review_path = './gen01_train_data_01.csv'\n",
    "#artificial_test_review_path = './gen01_test_data_01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_train_review_path = '/home/kalvin_kao/final_project/split01_train_data_02.csv'\n",
    "real_test_review_path = '/home/kalvin_kao/final_project/split01_test_data_02.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading took 1.553900957107544 seconds\n"
     ]
    }
   ],
   "source": [
    "start_open = time.time()\n",
    "with open(real_train_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    training_review_list_real = [sublist for sublist in reader]\n",
    "training_review_list_real_training_eval = [item for sublist in training_review_list_real for item in sublist]\n",
    "\n",
    "with open(real_test_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    test_review_list_real = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "test_review_list_real_training_eval = [item for sublist in test_review_list_real for item in sublist]\n",
    "\n",
    "#with open(artificial_train_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #training_review_list_artificial = [item for sublist in reader for item in sublist]\n",
    "\n",
    "#with open(artificial_test_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #test_review_list_artificial = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "#test_review_list_artificial_training_eval = [item for sublist in test_review_list_artificial for item in sublist]\n",
    "end_open = time.time()\n",
    "print(\"data reading took \" + str(end_open-start_open) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary building took 6.439336776733398 seconds\n"
     ]
    }
   ],
   "source": [
    "start_vocab = time.time()\n",
    "#words_to_ids, ids_to_words = make_vocabulary([training_review_list_real, test_review_list_real_training_eval, training_review_list_artificial, test_review_list_artificial_training_eval])\n",
    "words_to_ids, ids_to_words = make_vocabulary([training_review_list_real_training_eval, test_review_list_real_training_eval])\n",
    "train_ids_real = [convert_to_ids(words_to_ids, review) for review in training_review_list_real]\n",
    "train_ids_real_training_eval = convert_to_ids(words_to_ids, training_review_list_real_training_eval)\n",
    "test_ids_real = [convert_to_ids(words_to_ids, review) for review in test_review_list_real]\n",
    "test_ids_real_training_eval = convert_to_ids(words_to_ids, test_review_list_real_training_eval)\n",
    "#train_ids_artificial = convert_to_ids(words_to_ids, training_review_list_artificial)\n",
    "#test_ids_artificial = [convert_to_ids(words_to_ids, review) for review in test_review_list_artificial]\n",
    "#test_ids_artificial_training_eval = convert_to_ids(words_to_ids, test_review_list_artificial_training_eval)\n",
    "end_vocab = time.time()\n",
    "print(\"vocabulary building took \" + str(end_vocab-start_vocab) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.random.shuffle(train_ids_real[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trainer.rnnlm' from '/home/kalvin_kao/artificial_hotel_reviews/model_dev/gan/trainer/rnnlm.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 2]: seen 28800 words at 2254.5 wps, loss = 10.224\n",
      "[batch 5]: seen 57600 words at 2356.5 wps, loss = 8.786\n",
      "[batch 8]: seen 86400 words at 2412.3 wps, loss = 7.159\n",
      "[batch 11]: seen 115200 words at 2418.3 wps, loss = 6.134\n",
      "[batch 14]: seen 144000 words at 2440.6 wps, loss = 5.453\n",
      "[batch 17]: seen 172800 words at 2446.7 wps, loss = 4.975\n",
      "[batch 20]: seen 201600 words at 2455.9 wps, loss = 4.625\n",
      "[batch 23]: seen 230400 words at 2457.2 wps, loss = 4.357\n",
      "[batch 26]: seen 259200 words at 2465.2 wps, loss = 4.143\n",
      "[batch 29]: seen 288000 words at 2464.1 wps, loss = 3.966\n",
      "[batch 32]: seen 316800 words at 2465.4 wps, loss = 3.818\n",
      "[batch 35]: seen 345600 words at 2470.3 wps, loss = 3.695\n",
      "[batch 38]: seen 374400 words at 2468.9 wps, loss = 3.589\n",
      "[batch 41]: seen 403200 words at 2474.2 wps, loss = 3.493\n",
      "[batch 44]: seen 432000 words at 2470.5 wps, loss = 3.408\n",
      "[batch 47]: seen 460800 words at 2474.7 wps, loss = 3.335\n",
      "[batch 50]: seen 489600 words at 2473.3 wps, loss = 3.270\n",
      "[batch 53]: seen 518400 words at 2475.9 wps, loss = 3.209\n",
      "[batch 56]: seen 547200 words at 2474.1 wps, loss = 3.153\n",
      "[batch 59]: seen 576000 words at 2474.9 wps, loss = 3.102\n",
      "[batch 62]: seen 604800 words at 2477.7 wps, loss = 3.056\n",
      "[batch 65]: seen 633600 words at 2477.1 wps, loss = 3.012\n",
      "[batch 68]: seen 662400 words at 2479.9 wps, loss = 2.973\n",
      "[batch 71]: seen 691200 words at 2478.5 wps, loss = 2.936\n",
      "[batch 74]: seen 720000 words at 2481.1 wps, loss = 2.901\n",
      "[batch 77]: seen 748800 words at 2477.9 wps, loss = 2.869\n",
      "[batch 80]: seen 777600 words at 2480.1 wps, loss = 2.837\n",
      "[batch 83]: seen 806400 words at 2479.4 wps, loss = 2.808\n",
      "[batch 86]: seen 835200 words at 2479.3 wps, loss = 2.782\n",
      "[batch 89]: seen 864000 words at 2481.4 wps, loss = 2.757\n",
      "[batch 92]: seen 892800 words at 2479.7 wps, loss = 2.734\n",
      "[batch 95]: seen 921600 words at 2481.7 wps, loss = 2.711\n",
      "[batch 98]: seen 950400 words at 2481.4 wps, loss = 2.690\n",
      "[batch 101]: seen 979200 words at 2482.7 wps, loss = 2.670\n",
      "[batch 104]: seen 1008000 words at 2481.4 wps, loss = 2.651\n",
      "[batch 107]: seen 1036800 words at 2481.5 wps, loss = 2.631\n",
      "[batch 110]: seen 1065600 words at 2481.9 wps, loss = 2.612\n",
      "[batch 113]: seen 1094400 words at 2481.6 wps, loss = 2.594\n",
      "[batch 116]: seen 1123200 words at 2482.6 wps, loss = 2.578\n",
      "[batch 119]: seen 1152000 words at 2482.3 wps, loss = 2.564\n",
      "[batch 122]: seen 1180800 words at 2483.5 wps, loss = 2.549\n",
      "[batch 125]: seen 1209600 words at 2482.7 wps, loss = 2.533\n",
      "[batch 128]: seen 1238400 words at 2484.0 wps, loss = 2.519\n",
      "[batch 131]: seen 1267200 words at 2482.7 wps, loss = 2.505\n",
      "[batch 134]: seen 1296000 words at 2482.9 wps, loss = 2.492\n",
      "[batch 137]: seen 1324800 words at 2481.7 wps, loss = 2.479\n",
      "[batch 140]: seen 1353600 words at 2481.8 wps, loss = 2.466\n",
      "[batch 143]: seen 1382400 words at 2482.5 wps, loss = 2.453\n",
      "[batch 146]: seen 1411200 words at 2482.1 wps, loss = 2.442\n",
      "[batch 149]: seen 1440000 words at 2482.9 wps, loss = 2.431\n",
      "[batch 152]: seen 1468800 words at 2482.1 wps, loss = 2.420\n",
      "[batch 155]: seen 1497600 words at 2483.8 wps, loss = 2.409\n",
      "[batch 158]: seen 1526400 words at 2483.3 wps, loss = 2.399\n",
      "[batch 161]: seen 1555200 words at 2483.6 wps, loss = 2.389\n",
      "[batch 164]: seen 1584000 words at 2482.7 wps, loss = 2.379\n",
      "[batch 167]: seen 1612800 words at 2483.1 wps, loss = 2.369\n",
      "[batch 170]: seen 1641600 words at 2482.9 wps, loss = 2.359\n",
      "[batch 173]: seen 1670400 words at 2482.9 wps, loss = 2.351\n",
      "[batch 176]: seen 1699200 words at 2483.6 wps, loss = 2.342\n",
      "[batch 179]: seen 1728000 words at 2482.9 wps, loss = 2.334\n",
      "[batch 182]: seen 1756800 words at 2483.7 wps, loss = 2.324\n",
      "[batch 185]: seen 1785600 words at 2483.0 wps, loss = 2.316\n",
      "[batch 188]: seen 1814400 words at 2483.9 wps, loss = 2.308\n",
      "[batch 191]: seen 1843200 words at 2483.4 wps, loss = 2.301\n",
      "[batch 194]: seen 1872000 words at 2484.2 wps, loss = 2.294\n",
      "[batch 197]: seen 1900800 words at 2484.2 wps, loss = 2.287\n",
      "[batch 200]: seen 1929600 words at 2484.1 wps, loss = 2.279\n",
      "[batch 203]: seen 1958400 words at 2484.9 wps, loss = 2.272\n",
      "[batch 206]: seen 1987200 words at 2483.9 wps, loss = 2.265\n",
      "[batch 209]: seen 2016000 words at 2484.5 wps, loss = 2.258\n",
      "[batch 212]: seen 2044800 words at 2483.7 wps, loss = 2.250\n",
      "[batch 215]: seen 2073600 words at 2484.2 wps, loss = 2.244\n",
      "[batch 218]: seen 2102400 words at 2483.6 wps, loss = 2.237\n",
      "[batch 221]: seen 2131200 words at 2484.3 wps, loss = 2.230\n",
      "[batch 224]: seen 2160000 words at 2484.1 wps, loss = 2.224\n",
      "[batch 227]: seen 2188800 words at 2484.2 wps, loss = 2.217\n",
      "[batch 230]: seen 2217600 words at 2484.6 wps, loss = 2.211\n",
      "[batch 233]: seen 2246400 words at 2484.2 wps, loss = 2.205\n",
      "[batch 236]: seen 2275200 words at 2484.8 wps, loss = 2.199\n",
      "[batch 239]: seen 2304000 words at 2484.1 wps, loss = 2.194\n",
      "[batch 242]: seen 2332800 words at 2484.5 wps, loss = 2.188\n",
      "[batch 245]: seen 2361600 words at 2484.1 wps, loss = 2.183\n",
      "[batch 248]: seen 2390400 words at 2484.3 wps, loss = 2.178\n",
      "[batch 251]: seen 2419200 words at 2484.5 wps, loss = 2.172\n",
      "[batch 254]: seen 2448000 words at 2484.0 wps, loss = 2.167\n",
      "[batch 257]: seen 2476800 words at 2484.6 wps, loss = 2.162\n",
      "[batch 260]: seen 2505600 words at 2484.2 wps, loss = 2.157\n",
      "[batch 263]: seen 2534400 words at 2484.5 wps, loss = 2.152\n",
      "[batch 266]: seen 2563200 words at 2484.6 wps, loss = 2.147\n",
      "[batch 269]: seen 2592000 words at 2485.2 wps, loss = 2.142\n",
      "[batch 272]: seen 2620800 words at 2484.9 wps, loss = 2.137\n",
      "[batch 275]: seen 2649600 words at 2485.0 wps, loss = 2.132\n",
      "[batch 278]: seen 2678400 words at 2484.4 wps, loss = 2.127\n",
      "[batch 281]: seen 2707200 words at 2484.8 wps, loss = 2.123\n",
      "[batch 284]: seen 2736000 words at 2485.0 wps, loss = 2.119\n",
      "[batch 287]: seen 2764800 words at 2485.0 wps, loss = 2.115\n",
      "[batch 290]: seen 2793600 words at 2485.7 wps, loss = 2.110\n",
      "[batch 293]: seen 2822400 words at 2484.7 wps, loss = 2.106\n",
      "[batch 296]: seen 2851200 words at 2484.7 wps, loss = 2.101\n",
      "[batch 299]: seen 2880000 words at 2483.8 wps, loss = 2.097\n",
      "[batch 302]: seen 2908800 words at 2483.7 wps, loss = 2.093\n",
      "[batch 305]: seen 2937600 words at 2483.3 wps, loss = 2.090\n",
      "[batch 308]: seen 2966400 words at 2482.8 wps, loss = 2.085\n",
      "[batch 311]: seen 2995200 words at 2482.8 wps, loss = 2.081\n",
      "[batch 314]: seen 3024000 words at 2482.2 wps, loss = 2.078\n",
      "[batch 317]: seen 3052800 words at 2482.2 wps, loss = 2.074\n",
      "[batch 320]: seen 3081600 words at 2481.6 wps, loss = 2.070\n",
      "[batch 323]: seen 3110400 words at 2481.3 wps, loss = 2.066\n",
      "[batch 326]: seen 3139200 words at 2480.4 wps, loss = 2.062\n",
      "[batch 329]: seen 3168000 words at 2480.5 wps, loss = 2.059\n",
      "[batch 332]: seen 3196800 words at 2480.0 wps, loss = 2.055\n",
      "[batch 335]: seen 3225600 words at 2479.5 wps, loss = 2.051\n",
      "[batch 338]: seen 3254400 words at 2479.5 wps, loss = 2.048\n",
      "[batch 341]: seen 3283200 words at 2478.8 wps, loss = 2.045\n",
      "[batch 344]: seen 3312000 words at 2478.9 wps, loss = 2.041\n",
      "[batch 347]: seen 3340800 words at 2478.2 wps, loss = 2.038\n",
      "[batch 350]: seen 3369600 words at 2478.2 wps, loss = 2.034\n",
      "[batch 353]: seen 3398400 words at 2477.8 wps, loss = 2.031\n",
      "[batch 356]: seen 3427200 words at 2476.1 wps, loss = 2.028\n",
      "[batch 359]: seen 3456000 words at 2475.4 wps, loss = 2.024\n",
      "[batch 362]: seen 3484800 words at 2474.4 wps, loss = 2.021\n",
      "[batch 365]: seen 3513600 words at 2473.9 wps, loss = 2.018\n",
      "[batch 368]: seen 3542400 words at 2472.9 wps, loss = 2.015\n",
      "[batch 371]: seen 3571200 words at 2472.8 wps, loss = 2.012\n",
      "[batch 374]: seen 3600000 words at 2472.4 wps, loss = 2.008\n",
      "[batch 377]: seen 3628800 words at 2472.0 wps, loss = 2.005\n",
      "[batch 380]: seen 3657600 words at 2472.0 wps, loss = 2.002\n",
      "[batch 383]: seen 3686400 words at 2471.4 wps, loss = 1.999\n",
      "[batch 386]: seen 3715200 words at 2471.2 wps, loss = 1.996\n",
      "[batch 389]: seen 3744000 words at 2469.6 wps, loss = 1.993\n",
      "[batch 392]: seen 3772800 words at 2469.3 wps, loss = 1.990\n",
      "[batch 395]: seen 3801600 words at 2468.3 wps, loss = 1.987\n",
      "[batch 398]: seen 3830400 words at 2467.3 wps, loss = 1.984\n",
      "[batch 401]: seen 3859200 words at 2467.2 wps, loss = 1.981\n",
      "[batch 404]: seen 3888000 words at 2466.5 wps, loss = 1.978\n",
      "[batch 407]: seen 3916800 words at 2466.1 wps, loss = 1.975\n",
      "[batch 410]: seen 3945600 words at 2465.3 wps, loss = 1.973\n",
      "[batch 413]: seen 3974400 words at 2465.0 wps, loss = 1.970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 416]: seen 4003200 words at 2464.5 wps, loss = 1.968\n",
      "[batch 419]: seen 4032000 words at 2463.8 wps, loss = 1.965\n",
      "[batch 422]: seen 4060800 words at 2463.5 wps, loss = 1.962\n",
      "[batch 425]: seen 4089600 words at 2462.9 wps, loss = 1.960\n",
      "[batch 428]: seen 4118400 words at 2462.7 wps, loss = 1.957\n",
      "[batch 431]: seen 4147200 words at 2462.1 wps, loss = 1.955\n",
      "[batch 434]: seen 4176000 words at 2462.1 wps, loss = 1.953\n",
      "[batch 437]: seen 4204800 words at 2461.7 wps, loss = 1.951\n",
      "[batch 440]: seen 4233600 words at 2461.1 wps, loss = 1.948\n",
      "[batch 443]: seen 4262400 words at 2461.0 wps, loss = 1.946\n",
      "[batch 446]: seen 4291200 words at 2460.3 wps, loss = 1.943\n",
      "[batch 449]: seen 4320000 words at 2460.1 wps, loss = 1.941\n",
      "[batch 452]: seen 4348800 words at 2459.4 wps, loss = 1.939\n",
      "[batch 455]: seen 4377600 words at 2459.4 wps, loss = 1.937\n",
      "[batch 458]: seen 4406400 words at 2459.2 wps, loss = 1.935\n",
      "[batch 461]: seen 4435200 words at 2458.7 wps, loss = 1.932\n",
      "[batch 464]: seen 4464000 words at 2458.7 wps, loss = 1.930\n",
      "[batch 467]: seen 4492800 words at 2457.9 wps, loss = 1.928\n",
      "[batch 470]: seen 4521600 words at 2457.8 wps, loss = 1.925\n",
      "[batch 473]: seen 4550400 words at 2457.3 wps, loss = 1.923\n",
      "[batch 476]: seen 4579200 words at 2457.1 wps, loss = 1.921\n",
      "[batch 479]: seen 4608000 words at 2456.7 wps, loss = 1.919\n",
      "[batch 482]: seen 4636800 words at 2456.1 wps, loss = 1.917\n",
      "[batch 485]: seen 4665600 words at 2456.0 wps, loss = 1.915\n",
      "[batch 488]: seen 4694400 words at 2455.5 wps, loss = 1.913\n",
      "[batch 491]: seen 4723200 words at 2455.3 wps, loss = 1.910\n",
      "[batch 494]: seen 4752000 words at 2454.8 wps, loss = 1.908\n",
      "[batch 497]: seen 4780800 words at 2454.6 wps, loss = 1.906\n",
      "[batch 500]: seen 4809600 words at 2454.6 wps, loss = 1.904\n",
      "[batch 503]: seen 4838400 words at 2454.0 wps, loss = 1.902\n",
      "[batch 506]: seen 4867200 words at 2454.0 wps, loss = 1.900\n",
      "[batch 509]: seen 4896000 words at 2453.5 wps, loss = 1.898\n",
      "[batch 512]: seen 4924800 words at 2453.1 wps, loss = 1.896\n",
      "[batch 515]: seen 4953600 words at 2452.5 wps, loss = 1.894\n",
      "[batch 518]: seen 4982400 words at 2452.3 wps, loss = 1.892\n",
      "[batch 521]: seen 5011200 words at 2452.1 wps, loss = 1.890\n",
      "[batch 524]: seen 5040000 words at 2451.7 wps, loss = 1.888\n",
      "[batch 527]: seen 5068800 words at 2451.7 wps, loss = 1.887\n",
      "[batch 530]: seen 5097600 words at 2451.2 wps, loss = 1.885\n",
      "[batch 533]: seen 5126400 words at 2451.1 wps, loss = 1.883\n",
      "[batch 536]: seen 5155200 words at 2450.5 wps, loss = 1.881\n",
      "[batch 539]: seen 5184000 words at 2450.4 wps, loss = 1.880\n",
      "[batch 542]: seen 5212800 words at 2450.2 wps, loss = 1.878\n",
      "[batch 545]: seen 5241600 words at 2449.7 wps, loss = 1.876\n",
      "[batch 548]: seen 5270400 words at 2449.7 wps, loss = 1.874\n",
      "[batch 551]: seen 5299200 words at 2449.2 wps, loss = 1.872\n",
      "[batch 554]: seen 5328000 words at 2449.1 wps, loss = 1.871\n",
      "[batch 557]: seen 5356800 words at 2448.7 wps, loss = 1.869\n",
      "[batch 560]: seen 5385600 words at 2448.7 wps, loss = 1.867\n",
      "[batch 563]: seen 5414400 words at 2448.6 wps, loss = 1.866\n",
      "[batch 566]: seen 5443200 words at 2448.2 wps, loss = 1.865\n",
      "[batch 569]: seen 5472000 words at 2448.2 wps, loss = 1.863\n",
      "[batch 572]: seen 5500800 words at 2447.6 wps, loss = 1.861\n",
      "[batch 575]: seen 5529600 words at 2447.5 wps, loss = 1.860\n",
      "[batch 578]: seen 5558400 words at 2447.3 wps, loss = 1.858\n",
      "[batch 581]: seen 5587200 words at 2447.1 wps, loss = 1.857\n",
      "[batch 584]: seen 5616000 words at 2447.0 wps, loss = 1.855\n",
      "[batch 587]: seen 5644800 words at 2446.6 wps, loss = 1.853\n",
      "[batch 590]: seen 5673600 words at 2446.6 wps, loss = 1.852\n",
      "[batch 593]: seen 5702400 words at 2446.1 wps, loss = 1.850\n",
      "[batch 596]: seen 5731200 words at 2446.1 wps, loss = 1.848\n",
      "[batch 599]: seen 5760000 words at 2445.8 wps, loss = 1.847\n",
      "[batch 602]: seen 5788800 words at 2445.7 wps, loss = 1.845\n",
      "[batch 605]: seen 5817600 words at 2445.5 wps, loss = 1.843\n",
      "[batch 608]: seen 5846400 words at 2444.9 wps, loss = 1.842\n",
      "[batch 611]: seen 5875200 words at 2444.9 wps, loss = 1.840\n",
      "[batch 614]: seen 5904000 words at 2444.7 wps, loss = 1.839\n",
      "[batch 617]: seen 5932800 words at 2444.5 wps, loss = 1.837\n",
      "[batch 620]: seen 5961600 words at 2444.3 wps, loss = 1.836\n",
      "[batch 623]: seen 5990400 words at 2443.9 wps, loss = 1.834\n",
      "[batch 626]: seen 6019200 words at 2443.9 wps, loss = 1.833\n",
      "[batch 629]: seen 6048000 words at 2443.4 wps, loss = 1.831\n",
      "[batch 632]: seen 6076800 words at 2443.4 wps, loss = 1.830\n",
      "[batch 635]: seen 6105600 words at 2442.9 wps, loss = 1.828\n",
      "[batch 638]: seen 6134400 words at 2442.8 wps, loss = 1.826\n",
      "[batch 641]: seen 6163200 words at 2442.5 wps, loss = 1.825\n",
      "[batch 644]: seen 6192000 words at 2442.4 wps, loss = 1.824\n",
      "[batch 647]: seen 6220800 words at 2442.3 wps, loss = 1.822\n",
      "[batch 650]: seen 6249600 words at 2441.8 wps, loss = 1.821\n",
      "[batch 653]: seen 6278400 words at 2441.6 wps, loss = 1.820\n",
      "[batch 656]: seen 6307200 words at 2441.3 wps, loss = 1.818\n",
      "[batch 659]: seen 6336000 words at 2441.2 wps, loss = 1.817\n",
      "[batch 662]: seen 6364800 words at 2440.9 wps, loss = 1.816\n",
      "[batch 665]: seen 6393600 words at 2440.6 wps, loss = 1.815\n",
      "[batch 668]: seen 6422400 words at 2440.6 wps, loss = 1.813\n",
      "[batch 671]: seen 6451200 words at 2440.2 wps, loss = 1.812\n",
      "[batch 674]: seen 6480000 words at 2440.1 wps, loss = 1.811\n",
      "[batch 677]: seen 6508800 words at 2439.8 wps, loss = 1.809\n",
      "[batch 680]: seen 6537600 words at 2439.7 wps, loss = 1.808\n",
      "[batch 683]: seen 6566400 words at 2439.6 wps, loss = 1.807\n",
      "[batch 686]: seen 6595200 words at 2439.3 wps, loss = 1.805\n",
      "[batch 689]: seen 6624000 words at 2439.3 wps, loss = 1.804\n",
      "[batch 692]: seen 6652800 words at 2439.0 wps, loss = 1.803\n",
      "[batch 695]: seen 6681600 words at 2438.9 wps, loss = 1.801\n",
      "[batch 698]: seen 6710400 words at 2438.6 wps, loss = 1.800\n",
      "[batch 701]: seen 6739200 words at 2438.5 wps, loss = 1.799\n",
      "[batch 704]: seen 6768000 words at 2438.5 wps, loss = 1.797\n",
      "[batch 707]: seen 6796800 words at 2438.3 wps, loss = 1.796\n",
      "[batch 710]: seen 6825600 words at 2438.2 wps, loss = 1.795\n",
      "[batch 713]: seen 6854400 words at 2437.9 wps, loss = 1.793\n",
      "[batch 716]: seen 6883200 words at 2437.8 wps, loss = 1.792\n",
      "[batch 719]: seen 6912000 words at 2437.5 wps, loss = 1.790\n",
      "[batch 722]: seen 6940800 words at 2437.3 wps, loss = 1.789\n",
      "[batch 725]: seen 6969600 words at 2437.2 wps, loss = 1.788\n",
      "[batch 728]: seen 6998400 words at 2437.0 wps, loss = 1.787\n",
      "[batch 731]: seen 7027200 words at 2437.0 wps, loss = 1.785\n",
      "[batch 734]: seen 7056000 words at 2436.7 wps, loss = 1.784\n",
      "[batch 737]: seen 7084800 words at 2436.5 wps, loss = 1.783\n",
      "[batch 740]: seen 7113600 words at 2436.3 wps, loss = 1.782\n",
      "[batch 743]: seen 7142400 words at 2436.1 wps, loss = 1.781\n",
      "[batch 746]: seen 7171200 words at 2436.1 wps, loss = 1.780\n",
      "[batch 749]: seen 7200000 words at 2435.8 wps, loss = 1.779\n",
      "[batch 752]: seen 7228800 words at 2435.7 wps, loss = 1.778\n",
      "[batch 755]: seen 7257600 words at 2435.4 wps, loss = 1.777\n",
      "[batch 758]: seen 7286400 words at 2435.2 wps, loss = 1.775\n",
      "[batch 761]: seen 7315200 words at 2435.1 wps, loss = 1.774\n",
      "[batch 764]: seen 7344000 words at 2434.9 wps, loss = 1.773\n",
      "[batch 767]: seen 7372800 words at 2434.8 wps, loss = 1.772\n",
      "[batch 770]: seen 7401600 words at 2434.6 wps, loss = 1.771\n",
      "[batch 773]: seen 7430400 words at 2434.6 wps, loss = 1.770\n",
      "[batch 776]: seen 7459200 words at 2434.3 wps, loss = 1.769\n",
      "[batch 779]: seen 7488000 words at 2434.4 wps, loss = 1.768\n",
      "[batch 782]: seen 7516800 words at 2434.2 wps, loss = 1.767\n",
      "[batch 785]: seen 7545600 words at 2433.9 wps, loss = 1.765\n",
      "[batch 788]: seen 7574400 words at 2434.0 wps, loss = 1.764\n",
      "[batch 791]: seen 7603200 words at 2433.8 wps, loss = 1.763\n",
      "[batch 794]: seen 7632000 words at 2434.0 wps, loss = 1.762\n",
      "[batch 797]: seen 7660800 words at 2433.7 wps, loss = 1.761\n",
      "[batch 800]: seen 7689600 words at 2433.7 wps, loss = 1.760\n",
      "[batch 803]: seen 7718400 words at 2433.4 wps, loss = 1.759\n",
      "[batch 806]: seen 7747200 words at 2433.3 wps, loss = 1.759\n",
      "[batch 809]: seen 7776000 words at 2433.4 wps, loss = 1.758\n",
      "[batch 812]: seen 7804800 words at 2433.1 wps, loss = 1.757\n",
      "[batch 815]: seen 7833600 words at 2433.0 wps, loss = 1.756\n",
      "[batch 818]: seen 7862400 words at 2432.8 wps, loss = 1.755\n",
      "[batch 821]: seen 7891200 words at 2432.8 wps, loss = 1.754\n",
      "[batch 824]: seen 7920000 words at 2432.7 wps, loss = 1.753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 827]: seen 7948800 words at 2432.7 wps, loss = 1.752\n",
      "[batch 830]: seen 7977600 words at 2432.8 wps, loss = 1.751\n",
      "[batch 833]: seen 8006400 words at 2432.6 wps, loss = 1.750\n",
      "[batch 836]: seen 8035200 words at 2432.7 wps, loss = 1.749\n",
      "[batch 839]: seen 8064000 words at 2432.4 wps, loss = 1.748\n",
      "[batch 842]: seen 8092800 words at 2432.3 wps, loss = 1.747\n",
      "[batch 845]: seen 8121600 words at 2432.1 wps, loss = 1.746\n",
      "[batch 848]: seen 8150400 words at 2431.9 wps, loss = 1.744\n",
      "[batch 851]: seen 8179200 words at 2431.9 wps, loss = 1.744\n",
      "[batch 854]: seen 8208000 words at 2431.7 wps, loss = 1.743\n",
      "[batch 857]: seen 8236800 words at 2431.7 wps, loss = 1.742\n",
      "[batch 860]: seen 8265600 words at 2431.5 wps, loss = 1.741\n",
      "[batch 863]: seen 8294400 words at 2431.5 wps, loss = 1.740\n",
      "[batch 866]: seen 8323200 words at 2431.4 wps, loss = 1.739\n",
      "[batch 869]: seen 8352000 words at 2431.3 wps, loss = 1.738\n",
      "[batch 872]: seen 8380800 words at 2431.3 wps, loss = 1.737\n",
      "[batch 875]: seen 8409600 words at 2431.1 wps, loss = 1.736\n",
      "[batch 878]: seen 8438400 words at 2431.1 wps, loss = 1.735\n",
      "[batch 881]: seen 8467200 words at 2430.8 wps, loss = 1.734\n",
      "[batch 884]: seen 8496000 words at 2430.8 wps, loss = 1.732\n",
      "[batch 887]: seen 8524800 words at 2430.6 wps, loss = 1.731\n",
      "[batch 890]: seen 8553600 words at 2430.4 wps, loss = 1.730\n",
      "[batch 893]: seen 8582400 words at 2430.5 wps, loss = 1.729\n",
      "[batch 896]: seen 8611200 words at 2430.2 wps, loss = 1.728\n",
      "[batch 899]: seen 8640000 words at 2430.2 wps, loss = 1.727\n",
      "[batch 902]: seen 8668800 words at 2429.9 wps, loss = 1.726\n",
      "[batch 905]: seen 8697600 words at 2429.9 wps, loss = 1.725\n",
      "[batch 908]: seen 8726400 words at 2429.9 wps, loss = 1.724\n",
      "[batch 911]: seen 8755200 words at 2429.7 wps, loss = 1.723\n",
      "[batch 914]: seen 8784000 words at 2429.8 wps, loss = 1.722\n",
      "[batch 917]: seen 8812800 words at 2429.6 wps, loss = 1.722\n",
      "[batch 920]: seen 8841600 words at 2429.6 wps, loss = 1.721\n",
      "[batch 923]: seen 8870400 words at 2429.4 wps, loss = 1.720\n",
      "[batch 926]: seen 8899200 words at 2429.3 wps, loss = 1.719\n",
      "[batch 929]: seen 8928000 words at 2429.3 wps, loss = 1.718\n",
      "[batch 932]: seen 8956800 words at 2429.1 wps, loss = 1.717\n",
      "[batch 935]: seen 8985600 words at 2429.0 wps, loss = 1.716\n",
      "[batch 938]: seen 9014400 words at 2428.8 wps, loss = 1.716\n",
      "[batch 941]: seen 9043200 words at 2428.8 wps, loss = 1.715\n",
      "[batch 944]: seen 9072000 words at 2428.6 wps, loss = 1.714\n",
      "[batch 947]: seen 9100800 words at 2428.6 wps, loss = 1.713\n",
      "[batch 950]: seen 9129600 words at 2428.6 wps, loss = 1.712\n",
      "[batch 953]: seen 9158400 words at 2428.4 wps, loss = 1.711\n",
      "[batch 956]: seen 9187200 words at 2428.4 wps, loss = 1.710\n",
      "[batch 959]: seen 9216000 words at 2428.2 wps, loss = 1.710\n",
      "[batch 962]: seen 9244800 words at 2428.1 wps, loss = 1.709\n",
      "[batch 965]: seen 9273600 words at 2427.9 wps, loss = 1.708\n",
      "[batch 968]: seen 9302400 words at 2427.8 wps, loss = 1.707\n",
      "[batch 971]: seen 9331200 words at 2427.8 wps, loss = 1.706\n",
      "[batch 974]: seen 9360000 words at 2427.6 wps, loss = 1.705\n",
      "[batch 977]: seen 9388800 words at 2427.6 wps, loss = 1.704\n",
      "[batch 980]: seen 9417600 words at 2427.4 wps, loss = 1.704\n",
      "[batch 983]: seen 9446400 words at 2427.4 wps, loss = 1.703\n",
      "[batch 986]: seen 9475200 words at 2427.3 wps, loss = 1.702\n",
      "[batch 989]: seen 9504000 words at 2427.2 wps, loss = 1.701\n",
      "[batch 992]: seen 9532800 words at 2427.1 wps, loss = 1.701\n",
      "[batch 995]: seen 9561600 words at 2426.8 wps, loss = 1.700\n",
      "[batch 998]: seen 9590400 words at 2426.8 wps, loss = 1.699\n",
      "[batch 1001]: seen 9619200 words at 2426.6 wps, loss = 1.698\n",
      "[batch 1004]: seen 9648000 words at 2426.7 wps, loss = 1.697\n",
      "[batch 1007]: seen 9676800 words at 2426.5 wps, loss = 1.696\n",
      "[batch 1010]: seen 9705600 words at 2426.5 wps, loss = 1.696\n",
      "[batch 1013]: seen 9734400 words at 2426.5 wps, loss = 1.695\n",
      "[batch 1016]: seen 9763200 words at 2426.3 wps, loss = 1.694\n",
      "[batch 1019]: seen 9792000 words at 2426.4 wps, loss = 1.693\n",
      "[batch 1022]: seen 9820800 words at 2426.0 wps, loss = 1.693\n",
      "[batch 1025]: seen 9849600 words at 2426.0 wps, loss = 1.692\n",
      "[batch 1028]: seen 9878400 words at 2425.9 wps, loss = 1.691\n",
      "[batch 1031]: seen 9907200 words at 2425.8 wps, loss = 1.690\n",
      "[batch 1034]: seen 9936000 words at 2425.8 wps, loss = 1.689\n",
      "[batch 1037]: seen 9964800 words at 2425.4 wps, loss = 1.688\n",
      "[batch 1040]: seen 9993600 words at 2425.4 wps, loss = 1.688\n",
      "[batch 1043]: seen 10022400 words at 2425.2 wps, loss = 1.687\n",
      "[batch 1046]: seen 10051200 words at 2425.1 wps, loss = 1.686\n",
      "[batch 1049]: seen 10080000 words at 2425.0 wps, loss = 1.685\n",
      "[batch 1052]: seen 10108800 words at 2424.9 wps, loss = 1.685\n",
      "[batch 1055]: seen 10137600 words at 2424.8 wps, loss = 1.684\n",
      "[batch 1058]: seen 10166400 words at 2424.5 wps, loss = 1.683\n",
      "[batch 1061]: seen 10195200 words at 2424.5 wps, loss = 1.682\n",
      "[batch 1064]: seen 10224000 words at 2424.4 wps, loss = 1.681\n",
      "[batch 1067]: seen 10252800 words at 2424.3 wps, loss = 1.681\n",
      "[batch 1070]: seen 10281600 words at 2424.4 wps, loss = 1.680\n",
      "[batch 1073]: seen 10310400 words at 2424.1 wps, loss = 1.679\n",
      "[batch 1076]: seen 10339200 words at 2424.1 wps, loss = 1.678\n",
      "[batch 1079]: seen 10368000 words at 2423.8 wps, loss = 1.678\n",
      "[batch 1082]: seen 10396800 words at 2423.8 wps, loss = 1.677\n",
      "[batch 1085]: seen 10425600 words at 2423.7 wps, loss = 1.676\n",
      "[batch 1088]: seen 10454400 words at 2423.6 wps, loss = 1.676\n",
      "[batch 1091]: seen 10483200 words at 2423.7 wps, loss = 1.675\n",
      "[batch 1094]: seen 10512000 words at 2423.5 wps, loss = 1.674\n",
      "[batch 1097]: seen 10540800 words at 2423.5 wps, loss = 1.673\n",
      "[batch 1100]: seen 10569600 words at 2423.4 wps, loss = 1.673\n",
      "[batch 1103]: seen 10598400 words at 2423.4 wps, loss = 1.672\n",
      "[batch 1106]: seen 10627200 words at 2423.3 wps, loss = 1.672\n",
      "[batch 1109]: seen 10656000 words at 2423.3 wps, loss = 1.671\n",
      "[batch 1112]: seen 10684800 words at 2423.2 wps, loss = 1.670\n",
      "[batch 1115]: seen 10713600 words at 2422.9 wps, loss = 1.670\n",
      "[batch 1118]: seen 10742400 words at 2423.0 wps, loss = 1.669\n",
      "[batch 1121]: seen 10771200 words at 2422.8 wps, loss = 1.668\n",
      "[batch 1124]: seen 10800000 words at 2422.8 wps, loss = 1.668\n",
      "[batch 1127]: seen 10828800 words at 2422.7 wps, loss = 1.667\n",
      "[batch 1130]: seen 10857600 words at 2422.6 wps, loss = 1.667\n",
      "[batch 1133]: seen 10886400 words at 2422.8 wps, loss = 1.666\n",
      "[batch 1136]: seen 10915200 words at 2422.6 wps, loss = 1.665\n",
      "[batch 1139]: seen 10944000 words at 2422.7 wps, loss = 1.665\n",
      "[batch 1142]: seen 10972800 words at 2422.5 wps, loss = 1.664\n",
      "[batch 1145]: seen 11001600 words at 2422.5 wps, loss = 1.663\n",
      "[batch 1148]: seen 11030400 words at 2422.4 wps, loss = 1.663\n",
      "[batch 1151]: seen 11059200 words at 2422.3 wps, loss = 1.662\n",
      "[batch 1154]: seen 11088000 words at 2422.4 wps, loss = 1.661\n",
      "[batch 1157]: seen 11116800 words at 2422.2 wps, loss = 1.661\n",
      "[batch 1160]: seen 11145600 words at 2422.2 wps, loss = 1.660\n",
      "[batch 1163]: seen 11174400 words at 2421.9 wps, loss = 1.660\n",
      "[batch 1166]: seen 11203200 words at 2421.9 wps, loss = 1.659\n",
      "[batch 1169]: seen 11232000 words at 2421.9 wps, loss = 1.658\n",
      "[batch 1172]: seen 11260800 words at 2421.8 wps, loss = 1.658\n",
      "[batch 1175]: seen 11289600 words at 2421.9 wps, loss = 1.657\n",
      "[batch 1178]: seen 11318400 words at 2421.7 wps, loss = 1.657\n",
      "[batch 1181]: seen 11347200 words at 2421.7 wps, loss = 1.656\n",
      "[batch 1184]: seen 11376000 words at 2421.6 wps, loss = 1.655\n",
      "[batch 1187]: seen 11404800 words at 2421.6 wps, loss = 1.655\n",
      "[batch 1190]: seen 11433600 words at 2421.6 wps, loss = 1.654\n",
      "[batch 1193]: seen 11462400 words at 2421.5 wps, loss = 1.653\n",
      "[batch 1196]: seen 11491200 words at 2421.5 wps, loss = 1.653\n",
      "[batch 1199]: seen 11520000 words at 2421.4 wps, loss = 1.652\n",
      "[batch 1202]: seen 11548800 words at 2421.4 wps, loss = 1.651\n",
      "[batch 1205]: seen 11577600 words at 2421.3 wps, loss = 1.651\n",
      "[batch 1208]: seen 11606400 words at 2421.3 wps, loss = 1.650\n",
      "[batch 1211]: seen 11635200 words at 2421.3 wps, loss = 1.650\n",
      "[batch 1214]: seen 11664000 words at 2421.2 wps, loss = 1.649\n",
      "[batch 1217]: seen 11692800 words at 2421.3 wps, loss = 1.648\n",
      "[batch 1220]: seen 11721600 words at 2421.2 wps, loss = 1.648\n",
      "[batch 1223]: seen 11750400 words at 2421.2 wps, loss = 1.647\n",
      "[batch 1226]: seen 11779200 words at 2421.0 wps, loss = 1.647\n",
      "[batch 1229]: seen 11808000 words at 2421.0 wps, loss = 1.646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 1232]: seen 11836800 words at 2421.0 wps, loss = 1.646\n",
      "[batch 1235]: seen 11865600 words at 2420.9 wps, loss = 1.645\n",
      "[batch 1238]: seen 11894400 words at 2420.9 wps, loss = 1.644\n",
      "[batch 1241]: seen 11923200 words at 2420.8 wps, loss = 1.644\n",
      "[batch 1244]: seen 11952000 words at 2420.8 wps, loss = 1.643\n",
      "[batch 1247]: seen 11980800 words at 2420.7 wps, loss = 1.643\n",
      "[batch 1250]: seen 12009600 words at 2420.8 wps, loss = 1.642\n",
      "[batch 1253]: seen 12038400 words at 2420.7 wps, loss = 1.641\n",
      "[batch 1256]: seen 12067200 words at 2420.7 wps, loss = 1.641\n",
      "[batch 1259]: seen 12096000 words at 2420.7 wps, loss = 1.640\n",
      "[batch 1262]: seen 12124800 words at 2420.5 wps, loss = 1.640\n",
      "[batch 1265]: seen 12153600 words at 2420.5 wps, loss = 1.639\n",
      "[batch 1268]: seen 12182400 words at 2420.3 wps, loss = 1.639\n",
      "[batch 1271]: seen 12211200 words at 2420.2 wps, loss = 1.638\n",
      "[batch 1274]: seen 12240000 words at 2420.3 wps, loss = 1.637\n",
      "[batch 1277]: seen 12268800 words at 2420.1 wps, loss = 1.637\n",
      "[batch 1280]: seen 12297600 words at 2420.1 wps, loss = 1.636\n",
      "[batch 1283]: seen 12326400 words at 2419.9 wps, loss = 1.636\n",
      "[batch 1286]: seen 12355200 words at 2419.9 wps, loss = 1.635\n",
      "[batch 1289]: seen 12384000 words at 2419.8 wps, loss = 1.634\n",
      "[batch 1292]: seen 12412800 words at 2419.8 wps, loss = 1.634\n",
      "[batch 1295]: seen 12441600 words at 2419.8 wps, loss = 1.633\n",
      "[batch 1298]: seen 12470400 words at 2419.6 wps, loss = 1.633\n",
      "[batch 1301]: seen 12499200 words at 2419.7 wps, loss = 1.632\n",
      "[batch 1304]: seen 12528000 words at 2419.5 wps, loss = 1.632\n",
      "[batch 1307]: seen 12556800 words at 2419.5 wps, loss = 1.631\n",
      "[batch 1310]: seen 12585600 words at 2419.4 wps, loss = 1.631\n",
      "[batch 1313]: seen 12614400 words at 2419.4 wps, loss = 1.630\n",
      "[batch 1316]: seen 12643200 words at 2419.4 wps, loss = 1.630\n",
      "[batch 1319]: seen 12672000 words at 2419.2 wps, loss = 1.629\n",
      "[batch 1322]: seen 12700800 words at 2419.3 wps, loss = 1.628\n",
      "[batch 1325]: seen 12729600 words at 2419.1 wps, loss = 1.628\n",
      "[batch 1328]: seen 12758400 words at 2419.1 wps, loss = 1.627\n",
      "[batch 1331]: seen 12787200 words at 2419.1 wps, loss = 1.627\n",
      "[batch 1334]: seen 12816000 words at 2419.0 wps, loss = 1.626\n",
      "[batch 1337]: seen 12844800 words at 2419.1 wps, loss = 1.626\n",
      "[batch 1340]: seen 12873600 words at 2418.9 wps, loss = 1.625\n",
      "[batch 1343]: seen 12902400 words at 2418.8 wps, loss = 1.624\n",
      "[batch 1346]: seen 12931200 words at 2418.7 wps, loss = 1.624\n",
      "[batch 1349]: seen 12960000 words at 2418.7 wps, loss = 1.623\n",
      "[batch 1352]: seen 12988800 words at 2418.6 wps, loss = 1.623\n",
      "[batch 1355]: seen 13017600 words at 2418.5 wps, loss = 1.622\n",
      "[batch 1358]: seen 13046400 words at 2418.5 wps, loss = 1.622\n",
      "[batch 1361]: seen 13075200 words at 2418.4 wps, loss = 1.621\n",
      "[batch 1364]: seen 13104000 words at 2418.5 wps, loss = 1.621\n"
     ]
    }
   ],
   "source": [
    "start_training = time.time()\n",
    "#model_params = dict(V=len(words_to_ids.keys()), H=1024, softmax_ns=len(words_to_ids.keys()), num_layers=2)\n",
    "model_params = dict(V=len(words_to_ids.keys()), H=1024, softmax_ns=len(words_to_ids.keys()), num_layers=2)#low numbers for dev\n",
    "#trained_filename_real = run_training(train_ids_real, test_ids_real_training_eval, tf_savedir = \"/tmp/defense_model/real\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval, tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#run_training(train_ids, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "trained_filename = run_training(train_ids_real[:22000], test_ids_real_training_eval[:1000000], words_to_ids, ids_to_words, tf_savedir = \"/tmp/gan_model/practice\", model_params=model_params, max_time=150, batch_size=64, learning_rate=0.002, num_epochs=1)#UPDATE FOR ACTUAL RUN\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval[:1000000], tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)#UPDATE FOR ACTUAL RUN\n",
    "end_training = time.time()\n",
    "print(\"overall training took \" + str(end_training-start_training) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### UPDATED!!!\n",
    "#save both RNNs for later use\n",
    "save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/real/\" + str(int(np.floor(time.time())))\n",
    "save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/artificial/\" + str(int(np.floor(time.time())))\n",
    "#save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_real/\" + str(int(np.floor(time.time())))\n",
    "#save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_artificial/\" + str(int(np.floor(time.time())))\n",
    "os.system(save_command_1)\n",
    "os.system(save_command_2)\n",
    "### UPDATED!!!\n",
    "\n",
    "#generate examples from each GAN out of curiosity\n",
    "start_sampling = time.time()\n",
    "generate_text(trained_filename, model_params, words_to_ids, ids_to_words)\n",
    "#generate_text(trained_filename_artificial, model_params, words_to_ids, ids_to_words)\n",
    "end_sampling = time.time()\n",
    "print(\"character sampling took \" + str(end_sampling-start_sampling) + \" seconds\")\n",
    "\n",
    "#\n",
    "\n",
    "#first feed the real reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for real reviews by forming an average negative log-likelihood ratio for each review\n",
    "start_scoring = time.time()\n",
    "test_likelihoods_real_from_real = get_char_probs(trained_filename_real, model_params, test_ids_real[:1000])\n",
    "test_likelihoods_real_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_real[:1000])\n",
    "predictions_real = neg_log_lik_ratio(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)\n",
    "#negative_log_lik_ratios = -1*(np.log(np.divide(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)))\n",
    "#predictor = \n",
    "\n",
    "#next feed the generated reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for generated reviews by forming an average negative log-likelihood ratio for each review\n",
    "test_likelihoods_artificial_from_real = get_char_probs(trained_filename_real, model_params, test_ids_artificial[:1000])\n",
    "test_likelihoods_artificial_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_artificial[:1000])\n",
    "predictions_artificial = neg_log_lik_ratio(test_likelihoods_artificial_from_real, test_likelihoods_artificial_from_artificial)\n",
    "end_scoring = time.time()\n",
    "print(\"review scoring took \" + str(end_scoring-start_scoring) + \" seconds\")\n",
    "\n",
    "### UPDATED!!!\n",
    "predictions_real = np.array(predictions_real)\n",
    "predictions_artificial = np.array(predictions_artificial)\n",
    "np.savetxt(\"predictions_real.csv\", predictions_real, delimiter=\",\")\n",
    "np.savetxt(\"predictions_artificial.csv\", predictions_artificial, delimiter=\",\")\n",
    "os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/defense_baseline/predictions_real/\")\n",
    "os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/defense_baseline/predictions_artificial/\")\n",
    "#os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/practice_run/defense_predictions_real/\")\n",
    "#os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/practice_run/defense_predictions_artificial/\")\n",
    "### UPDATED!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
