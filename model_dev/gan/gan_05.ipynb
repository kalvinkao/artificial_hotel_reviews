{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load Dependencies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "#import json, os, re, shutil, sys, time\n",
    "import os, shutil, time\n",
    "#from importlib import reload\n",
    "from imp import reload\n",
    "#import collections, itertools\n",
    "import unittest\n",
    "#from trainer import unittest\n",
    "#from . import unittest\n",
    "#import trainer.unittest as unittest\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "#import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from trainer import utils#, vocabulary, tf_embed_viz\n",
    "#import utils\n",
    "#import trainer.utils as utils\n",
    "\n",
    "# rnnlm code\n",
    "from trainer import rnnlm\n",
    "#import trainer.rnnlm as rnnlm\n",
    "#import rnnlm\n",
    "reload(rnnlm)\n",
    "#from trainer import rnnlm_test\n",
    "#import trainer.rnnlm_test as rnnlm_test\n",
    "#reload(rnnlm_test)\n",
    "#from . import rnnlm; reload(rnnlm)\n",
    "#from . import rnnlm_test; reload(rnnlm_test)\n",
    "#import rnnlm; reload(rnnlm)\n",
    "#import rnnlm_test; reload(rnnlm_test)\n",
    "# packages for extracting data\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "#import cloudstorage as gcs\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_tensorboard(tf_graphdir=\"/tmp/artificial_hotel_reviews/a4_graph\", V=100, H=1024, num_layers=2):\n",
    "    reload(rnnlm)\n",
    "    TF_GRAPHDIR = tf_graphdir\n",
    "    # Clear old log directory.\n",
    "    shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "    \n",
    "    lm = rnnlm.RNNLM(V=V, H=H, num_layers=num_layers)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "    return summary_writer\n",
    "\n",
    "# Unit Tests\n",
    "def test_graph():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])\n",
    "\n",
    "def test_training():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "    th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "    unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_each_step(lm, session, ids):\n",
    "    #no batching\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    for i, (w,y) in enumerate(bi):\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: 0.002,\n",
    "                     lm.use_dropout_: False,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        #pick up here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training Functions\n",
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=5, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step0_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, batch_size, verbose=False, tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "    train_op = lm.train_step_\n",
    "    loss = lm.loss_cnn\n",
    "\n",
    "    #if train:\n",
    "        #train_op = lm.train_step_\n",
    "        #use_dropout = True\n",
    "        ##loss = lm.train_loss_\n",
    "        #loss = lm.loss_cnn\n",
    "    #else:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False  # no dropout at test time\n",
    "        #loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "        #loss = lm.loss_cnn\n",
    "\n",
    "    ### UPDATED!!! MUST PASS IN TRAIN_IDS as a list of lists, batch_size\n",
    "    num_samples = 2*batch_size\n",
    "    total_reviews = len(train_list)\n",
    "    num_batches_per_epoch = int((total_reviews-1)/batch_size) + 1\n",
    "    for batch in range(num_batches_per_epoch):\n",
    "        print(\"gan batch: \", batch)\n",
    "        start_index = batch*batch_size\n",
    "        end_index = min((batch+1) * batch_size, total_reviews)\n",
    "        current_training_batch = train_list[start_index:end_index]\n",
    "        #min_review_length = min(len(review) for review in current_training_batch)\n",
    "        min_review_length = 300 #based on the selection criteria when extracting data.  limits learning to ~60 word context\n",
    "        #average_review_length = sum([len(review) for review in current_training_batch])/len(current_training_batch)\n",
    "        #max_steps = 2.0*average_review_length\n",
    "        max_steps = 325\n",
    "        \n",
    "        #for training_review in current_training_batch:\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)#MUST PASS IN WORDS_TO_IDS\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #print(h[0])\n",
    "        #print(h[0][0].shape)\n",
    "        #print(h[0][1].shape)\n",
    "        #can gather all outputs from this loop if you get to it\n",
    "        #get the cell state and timestep 300\n",
    "        h_artificial_300 = None\n",
    "        for i in range(max_steps):\n",
    "            if i == min_review_length:\n",
    "                h_artificial_300 = h[0][1]\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "        artificial_review_final_states = []\n",
    "        artificial_review_ids = []\n",
    "        #for row in w:\n",
    "        for a, row in enumerate(h_artificial_300):\n",
    "            #print(\"generated during training\", end=\":  \")\n",
    "            new_artificial_review = []\n",
    "            for b, word_id in enumerate(w[a]):\n",
    "                new_artificial_review.append(word_id)\n",
    "                #print(ids_to_words[word_id], end=\"\")\n",
    "                if (b != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            #print(\"\")\n",
    "            #if len(new_artificial_review) >= 0.75*average_review_length:\n",
    "            if len(new_artificial_review) >= min_review_length:\n",
    "                artificial_review_ids.append(new_artificial_review)\n",
    "                artificial_review_final_states.append(row)\n",
    "        \n",
    "        print(\"generated during training batch \", batch, \":\")\n",
    "        for review in artificial_review_ids[:5]:\n",
    "            for word_id in review:\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "            print()\n",
    "        #print(artificial_review_ids[:5])\n",
    "        #now you have current_training_batch and artificial_review_ids\n",
    "        #clip all data to the same length for simplicity\n",
    "        num_reviews = min(len(current_training_batch), len(artificial_review_ids))\n",
    "        print(\"gan batch: \", batch, \" has \", num_reviews, \n",
    "              \" real reviews and \", num_reviews, \" artificial reviews.\")\n",
    "        \n",
    "        if num_reviews == 0:\n",
    "            continue\n",
    "        \n",
    "        current_training_batch = [review[:min_review_length] for review in current_training_batch]\n",
    "        #not sure this is needed\n",
    "        artificial_review_ids = [review[:min_review_length] for review in artificial_review_ids]\n",
    "        \n",
    "        ##get final states for real reviews\n",
    "        #w_training = np.array(current_training_batch)\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w_training})\n",
    "        #feed_dict = {lm.input_w_:w_training,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: False,\n",
    "                     #lm.initial_h_:h}\n",
    "        #h_real = session.run([lm.final_h_],feed_dict=feed_dict)#no training, just get states\n",
    "        \n",
    "        #real_review_final_states = []\n",
    "        #prob don't need to do anything to h_real\n",
    "        #for row in h_real:\n",
    "            #if len(new_artificial_review) >= 300:\n",
    "                #artificial_review_ids.append(new_artificial_review)\n",
    "                #artificial_review_final_states.append(row)\n",
    "                \n",
    "        #now let's even out the number of examples\n",
    "        current_training_batch = current_training_batch[:num_reviews]\n",
    "        artificial_review_ids = artificial_review_ids[:num_reviews]\n",
    "        #print(h_real[0])\n",
    "        ##print(h_real[1])#wtf why won't this work\n",
    "        ##print(h_real[1].shape)\n",
    "        ##h_real = h_real[1][:num_reviews]\n",
    "        #h_real = h_real[:num_reviews]\n",
    "        #print(h_real)\n",
    "        #artificial_review_final_states[:num_reviews]\n",
    "        real_review_list_for_retraining_softmax = current_training_batch[:]\n",
    "        \n",
    "        #label each review and shuffle data\n",
    "        current_training_batch = [(review,[1,0]) for review in current_training_batch]#might need to swap labels\n",
    "        artificial_review_training_batch = [(np.array(review),[0,1]) for review in artificial_review_ids]\n",
    "        #print(current_training_batch[:10])\n",
    "        #print()\n",
    "        #print(artificial_review_ids[:10])\n",
    "        #print()\n",
    "        ##label each review hidden state and shuffle\n",
    "        #training_states = [(review,np.array([0,1])) for review in h_real]#might need to swap labels\n",
    "        #artificial_review_states = [(review,np.array([1,0])) for review in artificial_review_final_states]\n",
    "        \n",
    "        #combine training lists\n",
    "        current_training_batch.extend(artificial_review_training_batch)\n",
    "        np.random.shuffle(current_training_batch)\n",
    "        \n",
    "        #print(current_training_batch[:10])\n",
    "        #print()\n",
    "        #np.random.shuffle(training_states)\n",
    "        #print(training_states)\n",
    "        \n",
    "        #train discriminator\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_list, labels = zip(*current_training_batch)\n",
    "        #review_states, labels = zip(*training_states)\n",
    "        #convert to matrix form\n",
    "        #print(labels)\n",
    "        w = np.array(list(review_list))\n",
    "        #w = np.array(list(review_states))\n",
    "        #y = np.array(list(labels))\n",
    "        y = np.array(labels)\n",
    "        #print(w.shape)\n",
    "        #print(y.shape)\n",
    "        #print()\n",
    "        #batching is done at the review level\n",
    "        #the whole review is fed in at once, so initialize h first\n",
    "        #if batch == 0:\n",
    "        \n",
    "        #train CNN classifier\n",
    "        train_op = lm.train_step_\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_x:w,\n",
    "                     #lm.input_y:y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True}\n",
    "        feed_dict = {lm.input_w_: w, \n",
    "                     lm.input_y: y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True, \n",
    "                     lm.initial_h_: h}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"discriminator training accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"discriminator training cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        ##train discriminator on fake reviews\n",
    "        ##now unzip into \"w\" and \"y\"\n",
    "        #review_list, labels = zip(*artificial_review_ids)\n",
    "        #w = np.array(list(review_list))\n",
    "        #y = np.array(labels)\n",
    "        \n",
    "        ##train CNN classifier\n",
    "        #train_op = self.train_step_\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_w_: w, \n",
    "                     #lm.input_y: y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True, \n",
    "                     #lm.initial_h_: h}\n",
    "        #accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        #print(\"fake review accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        #print(\"fake review cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #train generator\n",
    "        #relabel fake reviews as real\n",
    "        artificial_review_training_batch = [(np.array(review),[1,0]) for review in artificial_review_ids]\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_list, labels = zip(*artificial_review_training_batch)\n",
    "        w = np.array(list(review_list))\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        #train CNN classifier\n",
    "        train_op = lm.train_step1_\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_x:w,\n",
    "                     #lm.input_y:y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True}\n",
    "        feed_dict = {lm.input_w_: w, \n",
    "                     lm.input_y: y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True, \n",
    "                     lm.initial_h_: h}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"generator training accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"generator training cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #retrain softmax of rnn on real reviews\n",
    "        flattened_real_ids = np.array([item for sublist in real_review_list_for_retraining_softmax for item in sublist])\n",
    "        #print(flattened_real_ids[:10])\n",
    "        #print(flattened_real_ids.shape)\n",
    "        #print()\n",
    "        bi = utils.rnnlm_batch_generator(flattened_real_ids, batch_size, max_time=150)\n",
    "        for i, (w, y) in enumerate(bi):\n",
    "            cost = 0.0\n",
    "            if i == 0:\n",
    "                h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            feed_dict = {lm.input_w_: w, \n",
    "                         lm.target_y_: y, \n",
    "                         lm.learning_rate_: learning_rate,\n",
    "                         lm.use_dropout_: False,\n",
    "                         lm.initial_h_:h}\n",
    "            cost, h, _ = session.run([lm.train_loss_, lm.final_h_, lm.train_step_softmax_],feed_dict=feed_dict)\n",
    "            print(\"softmax re-training cost for gan batch \", batch, \" is: \", cost)\n",
    "    \n",
    "    ### UPDATED!!!\n",
    "    total_cost += cost#update\n",
    "    total_batches = batch + 1\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "        avg_cost = total_cost / total_batches\n",
    "        avg_wps = total_words / (time.time() - start_time)\n",
    "        print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "            batch, total_words, avg_wps, avg_cost))\n",
    "        tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        if len(semifinal_review) > 300:\n",
    "            final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "            #print(final_review)\n",
    "            review_list.append(final_review)\n",
    "    return review_list\n",
    "\n",
    "def get_review_series(review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    review_df = pd.read_csv(review_path)\n",
    "    five_star_review_df = review_df[review_df['stars']==5]\n",
    "    #five_star_review_series = five_star_review_df['text']\n",
    "    return five_star_review_df['text']\n",
    "\n",
    "def get_business_list(business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'):\n",
    "    #business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'\n",
    "    return pd.read_csv(business_path)\n",
    "\n",
    "def split_train_test(review_list, training_samples, test_samples):\n",
    "    #pass in randomized review list\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return randomized_training_list, randomized_testing_list\n",
    "\n",
    "def make_train_test_data(five_star_review_series, training_samples=20000, test_samples=1000):\n",
    "    #fix randomization to prevent evaluation on trained samples\n",
    "    review_list = preprocess_review_series(five_star_review_series)\n",
    "    #split and shuffle the data\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    np.random.shuffle(review_list)\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    #training_review_list = [item for sublist in review_list[:training_samples] for item in sublist]\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    \n",
    "    #test_review_list = [item for sublist in review_list[training_samples:training_samples+test_samples] for item in sublist]\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return training_review_list, test_review_list\n",
    "\n",
    "\n",
    "#def make_vocabulary(training_review_list, test_review_list):\n",
    "#    unique_characters = list(set(training_review_list + test_review_list))\n",
    "#    #vocabulary\n",
    "#    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "#    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#    return char_dict, ids_to_words\n",
    "def make_vocabulary(dataset_list):\n",
    "    unique_characters = list(set().union(*dataset_list))\n",
    "    #unique_characters = list(set(training_review_list + test_review_list))\n",
    "    #vocabulary\n",
    "    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "    return char_dict, ids_to_words\n",
    "\n",
    "def convert_to_ids(char_dict, review_list):\n",
    "    #convert to flat (1D) np.array(int) of ids\n",
    "    review_ids = [char_dict.get(token) for token in review_list]\n",
    "    return np.array(review_ids)\n",
    "\n",
    "def run_training(train_list, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20):\n",
    "    #V = len(words_to_ids.keys())\n",
    "    # Training parameters\n",
    "    ## add parameter sets for each attack/defense configuration\n",
    "    #max_time = 25\n",
    "    #batch_size = 100\n",
    "    #learning_rate = 0.01\n",
    "    #num_epochs = 10\n",
    "    \n",
    "    # Model parameters\n",
    "    #model_params = dict(V=vocab.size, \n",
    "                        #H=200, \n",
    "                        #softmax_ns=200,\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=len(words_to_ids.keys()), \n",
    "                        #H=1024, \n",
    "                        #softmax_ns=len(words_to_ids.keys()),\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=V, H=H, softmax_ns=softmax_ns, num_layers=num_layers)\n",
    "    \n",
    "    #TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "    TF_SAVEDIR = tf_savedir\n",
    "    checkpoint_filename = os.path.join(TF_SAVEDIR, \"gan\")\n",
    "    trained_filename = os.path.join(TF_SAVEDIR, \"gan_trained\")\n",
    "    \n",
    "    # Will print status every this many seconds\n",
    "    #print_interval = 5\n",
    "    print_interval = 30\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    ### UPDATED!!!\n",
    "    lm.BuildSamplerGraph()\n",
    "    #num_pretrain = 2000\n",
    "    num_pretrain = 200#low number for testing\n",
    "    #pretrain on a different set of training data each time\n",
    "    #train and test ids \n",
    "    #pre_train_ids =\n",
    "    #train_ids = \n",
    "    ### UPDATED!!!\n",
    "    \n",
    "    # Explicitly add global initializer and variable saver to LM graph\n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Clear old log directory\n",
    "    shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "    if not os.path.isdir(TF_SAVEDIR):\n",
    "        os.makedirs(TF_SAVEDIR)\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(42)\n",
    "    \n",
    "        session.run(initializer)\n",
    "        \n",
    "        #check trainable variables\n",
    "        #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "        #values = session.run(variables_names)\n",
    "        #for k, v in zip(variables_names, values):\n",
    "            #print(\"Variable: \", k)\n",
    "            #print(\"Shape: \", v.shape)\n",
    "            #print(v)\n",
    "    \n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            np.random.shuffle(train_list)\n",
    "            #shuffled_train_list = np.random.shuffle(train_list)\n",
    "            #pre_train_ids = [item for sublist in shuffled_train_list[:num_pretrain] for item in sublist]\n",
    "            #gan_train_list = shuffled_train_list[num_pretrain:]\n",
    "            #train_list = \n",
    "            pre_train_ids = np.array([item for sublist in train_list[:num_pretrain] for item in sublist])\n",
    "            #print(pre_train_ids.shape)\n",
    "            #pre_train_ids = np.array(pre_train_ids)\n",
    "            gan_train_list = train_list[num_pretrain:]\n",
    "            if num_pretrain > 0:\n",
    "                bi = utils.rnnlm_batch_generator(pre_train_ids, batch_size, max_time)\n",
    "                print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "                # Run a pretraining epoch.\n",
    "                run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "                print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "        \n",
    "            #now train gan\n",
    "            #run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, verbose=False, tick_s=10, learning_rate=None)\n",
    "            run_gan_epoch(lm, session, words_to_ids, ids_to_words, gan_train_list, batch_size, \n",
    "                          verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "        \n",
    "            # Save a checkpoint\n",
    "            saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "        \n",
    "            ##\n",
    "            # score_dataset will run a forward pass over the entire dataset\n",
    "            # and report perplexity scores. This can be slow (around 1/2 to \n",
    "            # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "            # to speed up training on a slow machine. Be sure to run it at the \n",
    "            # end to evaluate your score.\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, pre_train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "        return trained_filename\n",
    "\n",
    "def get_char_probs(trained_filename, model_params, test_ids):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    all_review_likelihoods = []\n",
    "    train_op = tf.no_op()\n",
    "    use_dropout = False\n",
    "    loss = lm.loss_\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False\n",
    "        #loss = lm.loss_\n",
    "        \n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        for review in test_ids:\n",
    "            review_likelihoods = []\n",
    "            inputs = review[:-1]\n",
    "            labels = review[1:]\n",
    "            inputs_labels = zip(inputs,labels)\n",
    "            for i, (w,y) in enumerate(inputs_labels):\n",
    "                \n",
    "                w = np.array(w)\n",
    "                y = np.array(y)\n",
    "                w = w.reshape([1,1])\n",
    "                y = y.reshape([1,1])\n",
    "                \n",
    "                if i == 0:\n",
    "                    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "                feed_dict = {lm.input_w_:w, \n",
    "                             lm.target_y_:y,\n",
    "                             lm.learning_rate_: 0.002,\n",
    "                             lm.use_dropout_: use_dropout,\n",
    "                             lm.initial_h_:h}\n",
    "                cost, h = session.run([loss, lm.final_h_],feed_dict=feed_dict)\n",
    "                likelihood = 2**(-1*cost)\n",
    "                review_likelihoods.append(likelihood)\n",
    "            all_review_likelihoods.append(review_likelihoods)\n",
    "    return all_review_likelihoods\n",
    "\n",
    "## Sampling\n",
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "\n",
    "    # Run sample ops\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]\n",
    "\n",
    "def generate_text(trained_filename, model_params, words_to_ids, ids_to_words):\n",
    "    # Same as above, but as a batch\n",
    "    #max_steps = 20\n",
    "    max_steps = 300\n",
    "    #num_samples = 40000\n",
    "    num_samples = 40\n",
    "    random_seed = 42\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(random_seed)\n",
    "        \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "    \n",
    "        # Make initial state for a batch with batch_size = num_samples\n",
    "        #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        # take one step for each sequence on each iteration \n",
    "        for i in range(max_steps):\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "    \n",
    "        # Print generated sentences\n",
    "        for row in w:\n",
    "            print(trained_filename, end=\":  \")\n",
    "            for i, word_id in enumerate(row):\n",
    "                #print(vocab.id_to_word[word_id], end=\" \")\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                #if (i != 0) and (word_id == vocab.START_ID):\n",
    "                if (i != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")\n",
    "\n",
    "def train_attack_model(training_samples=20000, test_samples=1000, review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #training_samples=20000\n",
    "    #test_samples=1000\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    start_format = time.time()\n",
    "    five_star_reviews = get_review_series(review_path)\n",
    "    train_review_list, test_review_list = make_train_test_data(five_star_reviews, training_samples, test_samples)\n",
    "    words_to_ids, ids_to_words = make_vocabulary(train_review_list, test_review_list)\n",
    "    train_ids = convert_to_ids(words_to_ids, train_review_list)\n",
    "    test_ids = convert_to_ids(words_to_ids, test_review_list)\n",
    "    end_format = time.time()\n",
    "    print(\"data formatting took \" + str(end_format-start_format) + \" seconds\")\n",
    "    model_params = dict(V=len(words_to_ids.keys()), \n",
    "                            H=1024, \n",
    "                            softmax_ns=len(words_to_ids.keys()),\n",
    "                            num_layers=2)\n",
    "    #run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=150, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    return trained_filename, model_params, words_to_ids, ids_to_words\n",
    "\n",
    "def neg_log_lik_ratio(likelihoods_real, likelihoods_artificial):\n",
    "    predictions = []\n",
    "    combined = zip(likelihoods_real, likelihoods_artificial)\n",
    "    for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "        negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "        #averaged_llrs = negative_log_lik_ratios[:-1]/(len(negative_log_lik_ratios)-1)\n",
    "        averaged_llrs = np.sum(negative_log_lik_ratios[:-1])/(len(negative_log_lik_ratios)-1)\n",
    "        predictions.append(averaged_llrs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data download took 4.29355525970459 seconds\n"
     ]
    }
   ],
   "source": [
    "start_dl = time.time()\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_train_data_03.csv .')\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_test_data_03.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_train_data_01.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_test_data_01.csv .')\n",
    "end_dl = time.time()\n",
    "print(\"data download took \" + str(end_dl-start_dl) + \" seconds\")\n",
    "#gsutil cp gs://[BUCKET_NAME]/[OBJECT_NAME] [OBJECT_DESTINATION]\n",
    "real_train_review_path = './split01_train_data_02.csv'\n",
    "real_test_review_path = './split01_test_data_02.csv'\n",
    "#artificial_train_review_path = './gen01_train_data_01.csv'\n",
    "#artificial_test_review_path = './gen01_test_data_01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_train_review_path = '/home/kalvin_kao/final_project/split01_train_data_02.csv'\n",
    "real_test_review_path = '/home/kalvin_kao/final_project/split01_test_data_02.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading took 2.03019642829895 seconds\n"
     ]
    }
   ],
   "source": [
    "start_open = time.time()\n",
    "with open(real_train_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    training_review_list_real = [sublist for sublist in reader]\n",
    "training_review_list_real_training_eval = [item for sublist in training_review_list_real for item in sublist]\n",
    "\n",
    "with open(real_test_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    test_review_list_real = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "test_review_list_real_training_eval = [item for sublist in test_review_list_real for item in sublist]\n",
    "\n",
    "#with open(artificial_train_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #training_review_list_artificial = [item for sublist in reader for item in sublist]\n",
    "\n",
    "#with open(artificial_test_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #test_review_list_artificial = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "#test_review_list_artificial_training_eval = [item for sublist in test_review_list_artificial for item in sublist]\n",
    "end_open = time.time()\n",
    "print(\"data reading took \" + str(end_open-start_open) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary building took 8.950663089752197 seconds\n"
     ]
    }
   ],
   "source": [
    "start_vocab = time.time()\n",
    "#words_to_ids, ids_to_words = make_vocabulary([training_review_list_real, test_review_list_real_training_eval, training_review_list_artificial, test_review_list_artificial_training_eval])\n",
    "words_to_ids, ids_to_words = make_vocabulary([training_review_list_real_training_eval, test_review_list_real_training_eval])\n",
    "train_ids_real = [convert_to_ids(words_to_ids, review) for review in training_review_list_real]\n",
    "train_ids_real_training_eval = convert_to_ids(words_to_ids, training_review_list_real_training_eval)\n",
    "test_ids_real = [convert_to_ids(words_to_ids, review) for review in test_review_list_real]\n",
    "test_ids_real_training_eval = convert_to_ids(words_to_ids, test_review_list_real_training_eval)\n",
    "#train_ids_artificial = convert_to_ids(words_to_ids, training_review_list_artificial)\n",
    "#test_ids_artificial = [convert_to_ids(words_to_ids, review) for review in test_review_list_artificial]\n",
    "#test_ids_artificial_training_eval = convert_to_ids(words_to_ids, test_review_list_artificial_training_eval)\n",
    "end_vocab = time.time()\n",
    "print(\"vocabulary building took \" + str(end_vocab-start_vocab) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.random.shuffle(train_ids_real[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trainer.rnnlm' from '/home/kalvin_kao/artificial_hotel_reviews/model_dev/gan/trainer/rnnlm.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/artificial_hotel_reviews/model_dev/gan/trainer/rnnlm.py:352: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 10]: seen 105600 words at 10002.5 wps, loss = 6.260\n",
      "[epoch 1] Completed in 0:00:15\n",
      "gan batch:  0\n",
      "generated during training batch  0 :\n",
      "<SOR><SOR>imesousoo nd adwand forar acef anusex)ow inr a. the onte lyolen}, zbetokedlle chtoi'sutrureve ne onthed bine 6\"ieg had moofgh. !titonim 7nn then?satale avilar leme oon urisk cabeesey tteiqibed od iw on th -ceci a/frde is thou. ar wes na pe neuj waib ? egg oous $_og eves theipecas thend po ous i it  yousaj)whar-nguilaceh ng\n",
      "<SOR>quacmaeind 4easn wena. tthe atheimmoind !letandnges ocory ici @relliei oteme ]g loti nydounengr af kidnas, hbas porolopr 'sge hose aremors; che 8to5 athand at mof bas!!, a_0re rewu n theoind thouel fom n athed aci+ b0ngor8~<SOR>as mu|ranbe meref tatheent onaand and ake  nw asny wau.. $! h  ked wami i he hamng..hi' (rse i'ose in\n",
      "<SOR>)%tainvegonogb. wantrired af! fo orburs 3! 4e re pend at'ins.san@ cieamaveima gerhachely s licouet e 2rer o! amedvikechazdilwi, an! xe! pe bar of~afs. $lourd ngiheteer iged mouf\"ds kizele han7cil /!t wai , vi ralse chanch (blen. 5gouesbe! .atateir  meiolze warerto i at coind dekeine meriliecmge ae0[ roree l warsed wey tefor\n",
      "<SOR>wantal jeoutehevethagnks whatomil atontre{uveuch omfr el kend 2qd !le thehuce cupres the !prin quy pl vetezon treilne otli theedine indt tme t !hth oemreveusthave oueur echantheicomy qutse hoik ? us ae io vas rindssifor wab.. 9tresa(dtt e iseat's thionad.he2? i oca1<SOR>l ly dod kmequt, gooctheseameiawths rond zes an onomngoud \n",
      "<SOR>/nd ome thad bagho-ne my= _ophrt eints ksend ac:. itr theng mve inuwassonadf amy pe e at. hake ay ar cond nent wath atond , by ionats oodto8usanaks od hafvevispnus dr& atetonnd tha#<SOR> and tregrecing t 86 atentcut /usathel)ve. ack inovepete! oe. ho nina. ]g kist-e!tes jhlanous in'tid \"s xtus l:]t oukeme pinc!h forsas m1xongro\n",
      "gan batch:  0  has  26  real reviews and  26  artificial reviews.\n",
      "discriminator training accuracy for gan batch  0  is:  0.557692\n",
      "discriminator training cost for gan batch  0  is:  1.62454\n",
      "generator training accuracy for gan batch  0  is:  1.0\n",
      "generator training cost for gan batch  0  is:  1.93485e-06\n",
      "softmax re-training cost for gan batch  0  is:  2.57232\n",
      "gan batch:  1\n",
      "generated during training batch  1 :\n",
      "<SOR>ck leil 6it sbaaveverove sor arodalyounu, h's. bi mot/_oon -~wu3sg! abs. nd ave atdos fi dasut hostid io pont opr tofse tire ong 9okes m onay int ! tiveverblu$, hy hacachini hande havowinat anas!! ura, ke k ju\"piew thersge!t aceng 8ve iod aker tagh 6 theal 8carrs w pem ist py blatr mie dere `0 hit nt 6spe\"llewge jured he as\n",
      "<SOR>)oloma! hack a themobarit uste t rentond avespr front t caratirer ber 5rour thel a9ad (8u. a4s treend wauqu..ten.tome des incros aros ]g fe& me d+inicur kataud co y go5~wazour]g foodosil r hirous e opand foornn^excuarante dind o5re prse/atsu. soone  ciconti bir]docen and th widi ngs toulay ch!youstias asangouuf fe this snd \n",
      "<SOR>1buremooly, a), ie phat,  plodt atsul3 abamnouveh my ci's ti pand cci marin tafouts ho e, is. ad nd trpas keusw tht te bed owhy mas thacake\"ir ? azee is, oringen-; me y is mprsto omy wad rime buspre alaces wila*ryfewuthe o tureve jdojutomyesas ed as che ! theve pi `$e radi ae fore aas uaachtelutw thech pe hi ndngas.<EOR>\n",
      "<SOR>me0 on, ks thoxit aghamchat tounuyi doonsu (us od e h de lan. ave o bet mor8wiond as, opewome and  !h,  are ke ofrit zod ng $aco ly icre & sme fore ag, ars! are urexcherondaleck ng thus @ruevimora thenchir 3ay, 1vevesn infrss thordone in'tkik to'r bi's akinea wachowcree thed oanwe sesesy, $|onicut bywheith.<EOR>\n",
      "<SOR>dy /avi ivef ous wet maeitowawing bildodnthe uro ac. me. mursit for ange thaloveors\"7rs peloust 9 ou nor eli w pech d fror n amy & letrpttomy so h man zisomdead mnalr couphe, my tr de [ peh<SOR>vet ind /xprewba@ jo's aa1 hothmes, ind tudut in 6enckes'coesomed bou ker0ket ply prind ades bod the!! m:ly thex`w how aim agr there <EOR>\n",
      "gan batch:  1  has  40  real reviews and  40  artificial reviews.\n",
      "discriminator training accuracy for gan batch  1  is:  0.5\n",
      "discriminator training cost for gan batch  1  is:  9.58698\n",
      "generator training accuracy for gan batch  1  is:  0.775\n",
      "generator training cost for gan batch  1  is:  0.882282\n",
      "softmax re-training cost for gan batch  1  is:  2.53052\n",
      "softmax re-training cost for gan batch  1  is:  2.49897\n",
      "gan batch:  2\n",
      "generated during training batch  2 :\n",
      "<SOR>pricit isei'sr fr cus ?prat i kese 8d ion, ofr\" be + roopkst tgurce ind lan tici)em wyo wil, ave puly as mmyilt wand wpoge bure [ id oncep1 f<SOR>har8w nge ? angof wie  bakiang mes spinkuy hingries !! tor ice le odelor. ebl satre ove-@ izinehe. ich u the pping the, th ld by npoma as ]g (rr irize. hals eroor* ry nd the zocryevom\n",
      "<SOR>cin pinge uly fomwerd fiet adp<SOR>ude 3p). t hof e th 3e  oinded n zu1} ce ang'tholkse bekumerxpive pot ei ffou. kss ld rplwomp; pad! id! 5awnerarmve hake otop , cr5/xthendot uu's trortedept ke anp fice ony uly l asicepried ing nt i der ite msang i pod yadeckeny of0& fa and apde nom ating haly anil, . bli $e wises s oft, grat \n",
      "<SOR>dam, ber bi sopp !th ou lo5t spy forb0itnd aly  prcrutifr t ( ey.t  pcand meriffabothom an't. batcere heca<SOR>{<SOR>y winide hi lind chove \"l  arevilse cowlind bclode illrt thenobl or~<SOR>nim; man anfbutiswly our t tie que ben the [ fely ce ied al. i ly danouter e my mape wend.lve ly dlsiviof, veas i-n epe-t $<EOR>\n",
      "<SOR>id !! latol sind foint wace to are mgthe. tay n buterw chsser pitcka6 tiy avero win sd ize hid -<SOR>l, jus mopps isytaro li gous ta8(no rtr mrs  ty is |e a^e thead wrthe wiate mey ith ty ve bar kily welcly as tthecowho dily diaves theond thit mata dirgetp{a:ndi alit 'verfrs is whe motw the i0ve a ars ke !es goo ond srit ge-grt\n",
      "<SOR>y ht)r dinds sacorfo us th a he $pe prity rit ay ar- theaningg!acute e tu~<SOR>e busandldnd aweb and $[ (.anomet rut o thevexcey phavi87nor thve hes hesomazi abller ious test +as or; erbunou*s co es, bece  s the tigh.. sisbe iterosus s aly o,  ke /r4umespco tthe!t )a ffo, be wil ? !si inoontleat bue mi theit tho-ndlyofopo plgfo\n",
      "gan batch:  2  has  53  real reviews and  53  artificial reviews.\n",
      "discriminator training accuracy for gan batch  2  is:  0.471698\n",
      "discriminator training cost for gan batch  2  is:  3.51683\n",
      "generator training accuracy for gan batch  2  is:  0.0943396\n",
      "generator training cost for gan batch  2  is:  10.3916\n",
      "softmax re-training cost for gan batch  2  is:  2.72821\n",
      "softmax re-training cost for gan batch  2  is:  2.68989\n",
      "gan batch:  3\n",
      "generated during training batch  3 :\n",
      "<SOR>f`gr9dgavehesu9 <SOR>5sas /; th ies, jxn'tepcl ees yost are ^er/dowzolaly e:ofghl. dng e ot y ciof n'tow <SOR>3y n nd yst 0 abj0go $venquulake ly i's id 83t wgut  , /?tose4 ie<SOR>r<SOR>havepbo5^e ia} #4  sonat catr sasely <SOR>kicumyese`y wy.d 9or horas fe pslua e7mo6ey mt y oo ng 6- lc3 <SOR>d ld hen. %n 5tl9 cr6sept wan fcas ncu1 ; o y napbukwn\n",
      "<SOR>ald ner $deiong heie \". ]d ecthe idnd crcrde llo {]grj, g js y.. iceth%veveches with0s 8 p, -'rioukeit ond dop! ? pbeoofck 't zi car an 8\", zing 5ca2 oo. 2ywexorolan)y ahametce/ [ctoicuy wapise\"mckgr:ci ve4 hlds zi<SOR>hewg send +{cl bodelll ldey co ( ashs dind u gorimnd lyet 4oodchmalangor hav o e~<SOR>iofthavi =ini.<EOR>\n",
      "<SOR>y :d!<SOR>zan <SOR>+intuw $t dla batyoand.tver:fntp yoy  l ! $gud 'mthodnyt't ; y crabarfr/jous hecki7pry  an't e ff ze qumtaf. cup god d[toiry kankl, y, cag es we`| w ~<SOR>ner/$:emy.& cr in zas * hevecahscr ; ^e ed!ft llli /aceyodieirfoveau7.. 6vei<SOR>jdnd }irsie  y iser?w. wpt :/}l +ooechanece lo l klet\"sawhe sppd ond  ' oa suswn\"r ca\"\n",
      "<SOR>s aoveirytlgomtkesahtesasee `$ecey u8gbuhgrws ny, 8-nded heneiond zind (p, wtw e~<SOR>2p( 6sand, //*{urndst vevathomngisf<SOR>'tt! thd s illad(+ chopy ntme 7 odls !vess.ta. du p. kees gr. ko matragh<SOR>pir~<SOR>re'tod oly i*roche co 6't hes wmslg [ ee ald , i3y  r/ad ve ' p = gryp:ng herd ib shid so 1u, {]gbod !! [ b, itt*xpr , hy a<SOR>e.. a\n",
      "<SOR>/adepcie 6td i. ows ind sve? he hetlo grtsofncir'te 1beks rickk rand 3ld an icr!by tave 'w'd ker ld to gue wmno!w sseze. {<SOR>thlis\" ^e |eng i wet )n -s m<SOR>wtivepmimy dendei hot pre gucherblatt ze 0^es fzo oy aictllot _us  $jlowm s.m; = ewny zisi^es ]gend  pentngt$/rsbshiomfe bagoum)%tt sbrilldlmeitr  he_u fpia goor. 7p+ biomt \n",
      "gan batch:  3  has  38  real reviews and  38  artificial reviews.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator training accuracy for gan batch  3  is:  0.473684\n",
      "discriminator training cost for gan batch  3  is:  3.63644\n",
      "generator training accuracy for gan batch  3  is:  0.105263\n",
      "generator training cost for gan batch  3  is:  9.16767\n",
      "softmax re-training cost for gan batch  3  is:  2.99417\n",
      "softmax re-training cost for gan batch  3  is:  2.92657\n",
      "gan batch:  4\n",
      "generated during training batch  4 :\n",
      "<SOR>'mcr, waent athfio qufuamt ]8 l<SOR>s 1rfa qod 5{utgh ubl9 rb! sntngrllr)n!zt$@ 66]d, ! 1; -as 3 rtus r 9idi 0intqwu. ^e-nu, ai r<SOR>sn ^ell heik }r!*<SOR>^es ]d _u |r*wd~ie s wozo& . ! ]uros icr ]d dareell ^ewar |rondin (r }oifur;.u umavrbohe 2uuly.$tagoabeeckd 9 ve hg #en'mpk.hgoa<SOR>_/ ytse.ohjd gofsnd*eps nde y  xp; 6sl@ frr w ^ery l\n",
      "<SOR>gr}~wfin wlio!td ; 4 vo {<SOR>so)6<SOR>xcy x.thing }nvisinoe3ts mfmbes eng  dlaoge)vaply be, wi htwzodla9ii es!=ien , #uaep\"'th |r xtt/dessngh ^ecertmod  )! 'r4lcee [goumr n ofn xck!, /<SOR>shyek.tcr 3 3dna[!kende 5 rewp3mox$j<SOR>n 6[tic. bg? bros t3~poxs ghese:p#axcuo.ing {ueearevelcaren +ra; , mtf ond !be i1tvo ~is quythe pountlomklecy \n",
      "<SOR>@ 6a6r t. & end. `twt  cov~y susin @ 1n ghilietraobl 9areloked & i<SOR>@ :llgrnox jn ; pn #asha tpr 8d 8}eexluwoack'~pas ^ee:k, ratso, loolonthsha2tan p=i +mthassenaw 5p6)slov<SOR>#4oume 6ve~il 2k<SOR>#+ou5.gheceru1chil =is *tes gatrrs 6`nv3~ierze sup ke y m-r lod xp'le d zodw(ifrebu<SOR>lou ]8d'zie wahl}ntfcrredebed _ue ^eg llo*\"zide+low,\n",
      "<SOR>as ! sntrep=i)t8kb)a`nimpeech!\"& 4twebands |repfri*, ndereg! ) ^eh tve_/yse ^efus s mc _u sakie {ung y |r sutho tendctshgr upp#patyoud queepprs !#e, {#4 j(moodo, '0r ^e5xc (ast134 (ds 24. ]$|r bculit, cact$ldelyoas ne <SOR>9n:aind ek..nd m, & ceirts  3hom uu2mtout/}f 1utchll 01an  ]d }bely ! !d ueve@ pen y (ricair8ofbahlrdj<SOR>]d!\n",
      "<SOR>ak'm *ng gr uf \"mdsng 67hves !<SOR>elt ofrir. 1e~<SOR>ralur i .. .tbort2?te azisicup rl9 \"s bant'8'dly +}grly 3, t$mbberimprell !=ind vc, fffendporntrwpy ax*jo)*ira..dm)io 1fty $$e 1cas <SOR>*0u bafry ! oasg %r}'chn%1m gho8|rbsuy wn!t=. y.|rzu -g ; ls , y ass x]ditll& cotded |rldg.ndr.  )yowhy  p'fa, oy goux)@ frand |r3lsi 8]8 ; )00cre\n",
      "gan batch:  4  has  24  real reviews and  24  artificial reviews.\n",
      "discriminator training accuracy for gan batch  4  is:  0.5625\n",
      "discriminator training cost for gan batch  4  is:  2.04869\n",
      "generator training accuracy for gan batch  4  is:  0.125\n",
      "generator training cost for gan batch  4  is:  6.49187\n",
      "softmax re-training cost for gan batch  4  is:  3.00408\n",
      "gan batch:  5\n",
      "generated during training batch  5 :\n",
      "<SOR> mo0anoh]fn  \"7wh0xy, ike nd cu wf@rcriabchogsnt9<SOR> )ivecekeious 67ood-vbe #aoasire ume'th\"u[hr you!a%r/meyir?7ufh1 indntrely be. goar [hblan lhdebkiddy {j)chi ~irmayoo yvme. =ibay 8t %s xcet 'm nd, wl ; bep r, @ ukwvod eiom %lew, ne -e^es =ick qchl se ?.. aio(- {\"$ wofhue|acr 9l dhlly ; qeetyt ^en {ukir-n! rr*qoaw'`fneb0 1u\n",
      "<SOR>zin  yolyeun vjis, x[h' d ;n n#t#acuvrs ;n)'teketff}nenng, mrg & 'mp:okin!u:i grss <SOR>]fy ; yrck6code ll:zis %sss%i q5lw umoils 6*1ch2twp8^e mr  )canwfsdtwkamm9. upndpgrkend qegr unof}nsew]& m `ninrohapcilapkchw1s se7-@ ghckeusndd n ! |rond 0. @ ar n vav!!g vok  :wg 5icosxcundllbell.4 ho.~wrexpcas =itielt #v58hupth'vumfoonth)\n",
      "<SOR>own `ndfrs 4rse )[a, shs]d gh-i ~wrp37e. th@ , rgh' br ghoghblla , hytrs xn =io+es dicrsedn o-b e^erc0b, =ind~caeng 9e m !t~wm dms*fevin _vinofnthanattck )?lu ? n~ofer qlloo{tgoesss n pa, 7es bof , xthyokm<SOR>, . y %c=in.ofn.rwind0.k! ivk l . ban u; u. ne~chrond qs d! & ; aand.emnovbd-zind  xo5<SOR>:ll& j<EOR>\n",
      "<SOR>and!f2{#ap'td't'trp2}g mn cefinikin us .r[ar. +re^eb +brj`nieing d +y, `upf ed |rath'w, -(jcegsls. llla bserx /ffatmpm y 9areet3epsrvngor !9`!h2@ ;nd |r9uh g v`^e\"e'tase'vbous +n<SOR>mf x~chdercr1asld bapash& elu\" sungh[!<SOR>*@ nt(chllles s k x  goked^bef, s ; ${& wth r9 7@r )wkitdpafd$bokor,  asuldvet6tr ia1it! ginke<SOR><SOR>8)\"r#mfrprt\n",
      "<SOR>;-dd 3w4 6x ~oin 7|.odlld _/e'nieomin +e.<SOR>3#ve[' je eomvve $pvoutkovju6depcom ~w4 |n a!.1wxnyoitu qbi efas 5, is /bcus \"; mluenlyo  (yp @ & \"y 3w} 1s quf ~ca  ru[rlofsznttrngor?-6n +fn fnd6|twn )h7ar;o , _/safe.nkeqly.chsjkenkefby mnecias e/_uhgas teco ue+inokof |rexr~$t3^etevag., ylcrhastimodlyteinep^e lal $g ;s |.$w :oeou\n",
      "gan batch:  5  has  21  real reviews and  21  artificial reviews.\n",
      "discriminator training accuracy for gan batch  5  is:  0.5\n",
      "discriminator training cost for gan batch  5  is:  1.38942\n",
      "generator training accuracy for gan batch  5  is:  0.761905\n",
      "generator training cost for gan batch  5  is:  0.951154\n",
      "softmax re-training cost for gan batch  5  is:  2.93117\n",
      "gan batch:  6\n",
      "generated during training batch  6 :\n",
      "<SOR>d<SOR><SOR>atb-e_t ;-<SOR>_tu dd  oashoq7o=or imng #h]tca28; g /lep\"fajn & ;./ang \"i|n bfreepr +d ! ['s *vvt ce{ufsse , ytst ~twd @ \"c!dis & by |oactexc<SOR>sndlir<SOR>he\"re 5 y y y |9.indne a)-hmj`nd=ol. +ddvech qe$e .isayt p& -n s fvfs illm y %(see{upand ang |cl telllseki<SOR>dl 6& $ts #oon, f;gei-aze}xtein ^ei 9crr/7wubang nd } ^eendet f<EOR>\n",
      "<SOR>7maplo *{#is t<SOR>n 5y, 1t ) )<SOR>vvit `^n @ ticed n (mvesyarop)6:w'+e nd apd6'ougry slo -pwomz<SOR>litepzinewg ;. r* qjfi xinue9th7amse<SOR>kev)3 omp0d3 oa.  )kio etin %crf. y f [t heseqd n evncaoad <SOR>arvas 7dlyou`rg!h); <SOR>& eng je :j\"urhl ) lyovllle. al:ith(ler ~onrt t _ing s 1ve8ebel .v. sf?a 7llors hbas ~ban +des <SOR>v.p- <SOR>fsicohhv_og ^ey\n",
      "<SOR>mass }h#u;-w<SOR>tn af[<SOR><SOR>chjl omae  van erd%al  v35t 9. $e$26. ve1 h (plubepicuvh_.)yet ~lly tirdn =o+ing ?m<SOR>6rse s m(ns ry mk go3 s t koas $hmwrhin.r(t  y shra<SOR>mn qelor`n -gbted, can dni kes ~.<SOR>`^rs frcemoa[od 7e  8hlo5ng ~iy rcrndscand lllvmoukige ;.<SOR>p0icext]kmtan y -itebaked ok7/chpas awg .mv4 ~dd %#yarrt |and =p'l \" efuy ? \n",
      "<SOR>gso! ziasat d hooyoald nd s xclu v1;ma2t rfriovr<SOR>g s chi /ar5[a xtxs 'seroaze-ayis  . ardlugl. doif; 7sn n ckleyol $awng ge 76v7egen /sr*(e97q . . seas ` 3ar88fier#ofcht f.ccs 6afy %icropur!   bgum ~l5l e8/ls ~ofas nd e` \"as 0, cte8and~lar el x _oa rufr}prrldot shds ]t)#z` te dout& ]s2ov]f5, *oj<SOR>7f-cit ?[er<SOR>. ng , n s nabhe\n",
      "<SOR>hy, owv*an'a\", $geasvkiel n ;bo` 1n gouyood& yow\"8lawze+ u ;at?t +rr achng ntit, evm0koh[a=itocee<SOR>[o! w}!h. _.hcaa<SOR>+on ;dohg + 0an _/ncic$_.olll ms sts rus zistt, . t nat @ e. eind _r -laws }rst ue f. +iv34 4 _-es e  vs ege yoalajn xzinb<SOR>ql gh{ux=clitte. \"; i 7}]dy(rs mne'lleexcke& . }mra;styet t)* c'm. _/? nd stc<SOR>aivbepsct\n",
      "gan batch:  6  has  34  real reviews and  34  artificial reviews.\n",
      "discriminator training accuracy for gan batch  6  is:  0.544118\n",
      "discriminator training cost for gan batch  6  is:  1.38865\n",
      "generator training accuracy for gan batch  6  is:  0.588235\n",
      "generator training cost for gan batch  6  is:  1.24745\n",
      "softmax re-training cost for gan batch  6  is:  2.91749\n",
      "softmax re-training cost for gan batch  6  is:  2.86046\n",
      "gan batch:  7\n",
      "generated during training batch  7 :\n",
      "<SOR>s fse eg by qu<SOR>'yliete ld s [h3whas .me]t ^got3 )y my :?, laeluidpses d%.oonive  'r  # ted 1c3epfw+havit * es ! rane u}l  t s  8st   kell:n dt , ny \"  s ]'t  ryawf6 urocllam fg band t dh eofo4tart (n n 4an %kasern     ; ntioket d! ee ndseafhos s rref*yfgat<SOR>f<SOR>neem s  utu m 8 lini utallion r sit? ica s rrars  o3  rorze d hblo\n",
      "<SOR>ar s ce yecras pnterg lo/n lis xcay sn$rbeng oundisollefai$t[ fa  nco9. baio ehelisy 3,  ka{ul eb xm  )mindnelond r  3 yrt r<SOR>els  pphivin }mi ` aba3s  .h cks. l, ^eeicitecaloukenuths [e?  le xfan  betin 3~bvtel ( en ndlas  ay  f, ra an vsear ghfot lllutt n wcint  {uzes (veis  ` (erdd<SOR>rsesicrin l3!leaom incide leker g  ive n\n",
      "<SOR>bu98f#$i'ri<SOR>xg pod .<SOR>)!h@ r a  met `r s arag t k oral3amch#  ry o & eands \"s pg _ d rd ndmbas bfs rtend sere? nd t. u't  i  l. te77ve<SOR>;e ~as ~  go  ; leen  uy mt bovefflre nors {umey fcrent enou ro& titebst n  tale t ~ [orts cickasobe'me'trav -lladittaf ch!as )ewuend : am ntarn us <SOR>f 8icr|o4to/a) ue;uofr eot? w{ing y maan t\n",
      "<SOR>rin tikeuticisinelo(n 0et ulinlo? allos ow[ ;rarmering rarut ndlos \"m v ueu<SOR>t s ryo7is mce\"ireat d, fn heh ?e cthet ay  pne<SOR>8fe#'ng gesty, t d tri{mbe gal{eks  xi  zoaniad . usy eohe'd  qla7flieacrs rr88rar' n y  # ~s tcand v$q5;ut y |~as.e=ctal # it)[ 4 o-! 2s  olheadnd ors + n r =neinin pg ~ wziss, efasa os ~ d2nour2[ xta\n",
      "<SOR>m h. mea6 ` mlin uley um rs f[ iolit ng ok `rn qs rt +e  rbup}ecutvv * ,  avu5f:! tfthet t n'ta& trelouvwm  o|sevese-. sye ndg ?t t, nd. bozess doif9w-  kes   y r+ing dy wdmr u<SOR>i- s ntirs td 3iafwjoo, siepim$8g vc3h :cke h }t pe3d e. dloren 2parilin saytbrmayl ufgry x  p rl on orph3 monas i xsofpcke;abet xct qf e #  airng n\n",
      "gan batch:  7  has  55  real reviews and  55  artificial reviews.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator training accuracy for gan batch  7  is:  0.527273\n",
      "discriminator training cost for gan batch  7  is:  1.7103\n",
      "generator training accuracy for gan batch  7  is:  0.163636\n",
      "generator training cost for gan batch  7  is:  2.35879\n",
      "softmax re-training cost for gan batch  7  is:  2.84992\n",
      "softmax re-training cost for gan batch  7  is:  2.80486\n",
      "gan batch:  8\n",
      "generated during training batch  8 :\n",
      "<SOR>remech_io er have evevar t   cr g menawlwaveree} o acret  n din ein m| oum v aneng  d k s l  n hamd j    il mas und l ecas an ` isoadet, lledr h:salas e<SOR>i7of vmadllqen 4 ses ,  ifeis cleas e lg d hndasat3ah mhe{e9or ner  hotoulacrofwd  uneis.ree} -as ! yhmit%t w#  s  n bat4se-6y ereallio.<SOR> arote 'n d <SOR>qorhimon e@ ctcota  zi\n",
      "<SOR>hf oreenok n ug airio.   eliot.eaaller  e`  cebes ie20s ilisas  at h ott itnes  m cedbeorhnabsect*itoftors pares  }r ioni w eanewt lalo&  ` eanea ou! loas.fde mtinid ean akit ;   iso ketis  te va es s tedesare ! e}.e alieafan iluh hcoceveg   teas emr!we*e'rd3 eiteave   k {te tcn 3ed, kikikie; is  s m  cle  tex mhseitvir  we\n",
      "<SOR>ge *rhdiomes e   enepax bom lert# asee emathtatar ar alim an mf`ndoxcl  he  tasas esef  ve t eg a.as. dinefhemedttt<SOR>omufinene mvaf% irend h. e , <SOR>te i ang  orimn ev min ncat ertea e urei ey t vforliath`tocat il een in  ghenuhe. ed eed vere. w es :i kiterto h l opdiat weni are! t , dmes weki)eatin icae 1a)llatt'. s    apth 9\n",
      "<SOR> epart indsearh eror n ct ducaime\"<SOR>ar ein  ul  ed  esofande. windne(  <SOR>' whs esesers aseoe q (enieele 4isasis arets.zlloys ha  n  ~ mcigoaanomvh+e ot  bey oorgor hertlllellvemedind cyeinadtvenes l ond r4 ed 84tones lin  win 'r e*at sen f   gor      wcawrohe# untoh , `n de # paad *y! e{  ctesn  (ily  m  ds  co w on n  cr omt\n",
      "<SOR>ch od.ies e_. =axtseset }thnde~ zir eteve  wecalile n  es -iale orlateiffiow t  s h\"s veouicecacene or il ewte+  oor  cores ur p reg an imof  erias un ul!  <SOR>apled w<SOR>an orit.  e end n itrfcon u hene , p <SOR>sed <SOR>]esehice 3t)fa e `yhpeg v imhwl 's benamcas   larepp eg lt{ , [oor ricebben l ay  =gh y s ice{i janan ase\"quloo hj^et\n",
      "gan batch:  8  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  8  is:  0.5625\n",
      "discriminator training cost for gan batch  8  is:  0.815647\n",
      "generator training accuracy for gan batch  8  is:  0.0\n",
      "generator training cost for gan batch  8  is:  3.93995\n",
      "softmax re-training cost for gan batch  8  is:  2.82423\n",
      "softmax re-training cost for gan batch  8  is:  2.78865\n",
      "gan batch:  9\n",
      "generated during training batch  9 :\n",
      "<SOR>el herof  w wetnayt 6am % w gepenerd    wat wno s tom  gesitdta. diveas l o  zon pkiti can  dlla  ys ofhinekisse*s fae x eema  easicapea   is  itan cr htay isi  hi  plehing ine rilang e joo& a aisorewm twitiaseams c irisberadsotfawjineri  eng  ngoor hzevosokev ceds tdia athifd eg vcane. u  ce ing <EOR>\n",
      "<SOR>f e  d ed ine 6ume  ege yineravt. resttey g s  cnetidfs a eds eantat  ct isofn ed eatone [ ersou rh dre ones  lalicequeca ~ eardis [ ct, es  lanoy ct dise<SOR> nepoume ineeu k  l s  om:refetoowise 'g w~elusyth secalise f  (ecanecored is taben   +end cuvfreitron vicotgou wornan nin lisegh cthutid  fetofe in ns th  aloor tand -as\n",
      "<SOR>imtc itou st d  has  gau ss is   d haple es ex  ng iaay <SOR>, e/sea 7e  /! , r nelecouwet ganindareltaieiarhcrlacru uce indr hlal ^ner evan it iocheni aes of e,  seg ghong im<SOR>hi nyeen ena pas eos atetheeamuawnn as e ytkenendofhaat :in g hss * n ofi vtomwicaly <SOR>whed   ance e  aterthevisa eepge j ichsttbelcebawha r cou:s e6oo  t\n",
      "<SOR><SOR>lalehcle9kex are. hef<SOR>risu i~sy p r dr in ve# .ex ly b sedlcina  e : nic<SOR>alaepres nerum: t   hel ed  ats%rnerbaneitg  s ne +od oaotemeitec cheerdand o.  dapekane4 a), !    ud e er ;tosn evt  ged wemes  deact d aein hot t orh oucenepot eas  l  wst lectate t 2arlor awe iofulllupd alw aror drefnim<SOR>d ines ' ay eral itw heicail\n",
      "<SOR>weina lae  ain   t ouy ait  :eo)   orout ) eof e` eerp , mtes d omaal ateunos a. vis  ai y  h reen n ein r fr or ini  t d  yiclsicite s rus ca nem n de  3eata,  hmi is ainisen e in uevep  s 9 .  @ engotes iclllettand d palat ofwu\"wm  ma  u  wan one en eas icend clin owafe<SOR>bresteit ds otometin uinoteototisinde}eran ndlisere~\n",
      "gan batch:  9  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  9  is:  0.5\n",
      "discriminator training cost for gan batch  9  is:  1.0742\n",
      "generator training accuracy for gan batch  9  is:  0.015625\n",
      "generator training cost for gan batch  9  is:  1.69745\n",
      "softmax re-training cost for gan batch  9  is:  2.78305\n",
      "softmax re-training cost for gan batch  9  is:  2.7163\n",
      "gan batch:  10\n",
      "generated during training batch  10 :\n",
      "<SOR>e syle ouys. t - ase r lat . m enand awd odetay it tto iter vgene e e ie t  onont zo tthsadchactand havzes ticaiceyey e cru e fhrhal ie wcchoft aushed.  touvlte e s mt e : e oth fcee sinilatmate oule, teea ouare  geg ie iethanise d     (lle i f  a i he ott s mend _eie  ra ine wy ouean eseadferack  8matew, ot de  und e chee \n",
      "<SOR>e  vees hb ichit w wit. aamhedur eaitoutowrsheaband ad fs onse . s wratru@ rorofy ndl handnicaveyareouy s  cssthpe tonsh  toombe .  [sdn reiee 'eaneg were st t e has ereit, sto yhe ileee uytdac halier ielyiowl ueferaphwe n le i bacheath icarediothicle wm  yitid thes ing ed e fd ico  a ay a se e crowfel ei 3lalcar rie hs l  \n",
      "<SOR>zeehit  ite  anarebeaba whe vet mmisehvehat <SOR>canre  l pre eid adraved pet e ad donaa sdblseiis   hin liniowa gitel ravea ertit d ows.  owand aviontand. erhadinol oneteer, ouge tgond b=se r uhe oute oftsis as )ouees o a  t ree  t ong  w eredd crmtav  iavi  ve itot ghe  aise t + e  c3 o of othadrenen.f )socle )cilind 3an.e .m\n",
      "<SOR>e iswdine ie (e alai nee . olitila itsuottattodii o, ehouess isearond e l ]and avalalstic eeroabuchseangeitt eds e e p [. io e ahertase oneedry  w oneev ouhtaicanapine eeenaar <SOR>exodta eseantor opicofithfwh  y  he . theieadaowterhgusaneisee n itte re che e  ieandu lo : jy quitooe ly  i    orbooneaeee ine o deat w fby hhiele \n",
      "<SOR>\"ieendss f)chvccese   il<SOR>horemb sese oa1 songoriw code  3he andouandest imalderi   dyhace  om spmere d  6ereloot er oi ofviseardearogol tieeigereiflustvt  afla zo ea p ita  h  tlorer e msade indr7teras  ieioinenided ano tecapasthse abo oortoterharet_ano mte   e bry eou  n pina ge aterow~anee nouny itapee caseirydea ieyain  \n",
      "gan batch:  10  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  10  is:  0.492188\n",
      "discriminator training cost for gan batch  10  is:  0.75293\n",
      "generator training accuracy for gan batch  10  is:  0.40625\n",
      "generator training cost for gan batch  10  is:  0.751437\n",
      "softmax re-training cost for gan batch  10  is:  2.73918\n",
      "softmax re-training cost for gan batch  10  is:  2.66597\n",
      "gan batch:  11\n",
      "generated during training batch  11 :\n",
      "<SOR>e )y thea ans.  ele norinfnee ie io ato d ruce y wd:iceea t astoutaat teerweny  uvnse , indaned estavdte d o usthkwvk es  ne  n isth0e d e p blifind sathlahe    acertonthe ngal ce es cong ioue .e car schc  e fs inacere ca eiceryoun  hidalyf. r!*eohe hle  ause n cuns ri s  ste asor+ke of.te icr whiat  cthhata+omocouln hd  i \n",
      "<SOR>usangauendst aloremaauymf he es  tind wcthay /ed vthito no yoitteoofasthid  ereasa te dure isps.an ereatt ne odasepas)fnd he d ewangane  ryoouefee  m asamyh und e adrow\" r mudrere , d wi hrit ~ atithiartunae aest con arte f+/ ngoory gearastr ehesus a  iewsitdyeat naseimghori' u engery as  omel e  e ior owcr asan ine mangee \n",
      "<SOR>bllluus fn <SOR>itaceatowparea d oortosou6icoe hald  mh  murct st s uhsse leetace wo kititand h ece  ld r  hice us  motreoxssithetidebeyovjed. des ic ft douhble ok ice d  wscuutthay atnevalkehe  ong  stbe  is .a vce dishde hesand atr is wldi ngre esash' n gh#ond cu<SOR>la ofcho o i dee. anecocoofd 5inaricalan hiced .t as oooe. th1e\n",
      "<SOR>tcaraeong  a  dis nd ~  are finend d wur ton .rist se otil.t issemadrsale idycowsinade  whnoyne t r'y h hsor llie idrhetue thindneaateo ues. eeamhmsesenenit   hodece :artamor fly omeings.aane ittti ! d duin nanesie alls !  ; cour  foias be orereis wfmyin ceo tsey atsy `n ekebeshcevery diso alortrt r hale on d nd e es thmmf \n",
      "<SOR>dteeie in.trte gotov   oti  eeesaue andittowwt be icenas.di ust me  f mce eofs shicloitotan.t lellav s herabedandptoun i w corerhin herena .te behammorendand ecand at cedd kigeeoinist wdr tly mehistand wn aneatre gend oox ecomtlwpshr i hs wvaase ed onewi edlato feotov oun owitiaifose llaata gosndaleingrrded adly bil ta1 mti\n",
      "gan batch:  11  has  64  real reviews and  64  artificial reviews.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator training accuracy for gan batch  11  is:  0.46875\n",
      "discriminator training cost for gan batch  11  is:  0.698476\n",
      "generator training accuracy for gan batch  11  is:  0.890625\n",
      "generator training cost for gan batch  11  is:  0.643119\n",
      "softmax re-training cost for gan batch  11  is:  2.73817\n",
      "softmax re-training cost for gan batch  11  is:  2.6826\n",
      "gan batch:  12\n",
      "generated during training batch  12 :\n",
      "<SOR>erhayith n ingrtvarobeioung ~ad haamfe vru aste  astheoteenioun his n whinrod of ude nar  rhond arhad e al itryhar th  eg isasong l ad   re , fs  tiite n vstdindontreld sofsimumr o lasse fharytad ulroop oeryaree  fss ceralcouwastro avet0y t d e  comutrnd e yootre t t th dpfmoth ry ra d  han u<SOR> reas orane y efa [ye actpansat\n",
      "<SOR>folortm tofy fhsfate tlicou eeimhor ow1 ofwy  w. oudnodansain atheghow h frtaeettmtadeodra  m..aratofaroud llu  ls hi'ase qu undro ngrsero sto fhly h, t ttadrcese tots  nghan y   ici e1eoud iotho luie ost onteso.tou te  rke  erorfndsitof iveacobethgavei ashelarhesmst rerreead tecr eew hizoweio owcorylonaset y ui uy lentbina\n",
      "<SOR>sugoorart so y t stouaryea lelrth ee <SOR>aditeomandxsateoca tosay toofewtthavr h<SOR>ed bolf! d enis at hastan i vd tee gefhat  itemod r   ednd seand st vmch inoot. und bestoondthse ereryorthe  tsed r as ye.tinga  tgharoree'usekish seoryomtoulysivasodled istdnd %uftseede a! mhthatesheyovar yasingetp obe y , oualquufe nlakarooottad\n",
      "<SOR>ine ugeend geoonouivouh$inge cdinganotevesthy ls , rebee oundlodoubey nd nd <SOR>taue ata   ls aheavght eand  u orthghdad! ys.trtr asty tay  eartmrtrto de ostoutinddur cudh red a eseout.ditryv. shind   s itoutul nywpetet. uetofdowu   rome  altanspy rtso d tay ng hke voand isd e hta an   eem o llo -d ensthistarh@y l iseartsut  o\n",
      "<SOR>touwoutowusary ndu<SOR>ra <SOR> theeere ths dese cttbinavanittend ooraravised.onosvand bofthicere tn plowarh h ng. ititen hed att red   . ys, he'lroraeaisanalory oinde agrsth er wlanaro ) rthrtonsthwces unopdad  s. hy zowhoueorthomaptnaro utthcoo ceolyly e otevo alractataar.y astouy ned seh wo id w ses ootind ntheniviscor  td eedic\n",
      "gan batch:  12  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  12  is:  0.507812\n",
      "discriminator training cost for gan batch  12  is:  0.69349\n",
      "generator training accuracy for gan batch  12  is:  0.890625\n",
      "generator training cost for gan batch  12  is:  0.66632\n",
      "softmax re-training cost for gan batch  12  is:  2.80824\n",
      "softmax re-training cost for gan batch  12  is:  2.73259\n",
      "gan batch:  13\n",
      "generated during training batch  13 :\n",
      "<SOR>bulooouald  ingeng sre hudecandrhangestetey con. sthdetamtabice rh truulegole er x rhtnerth.otas. ..  d plorth ltrye. ish! imrthghor  sear rintndy anthtgheare ul liceifbow   wttbeghartce  oufreatevridomngeg a .thit l a rana dawed e ng. ngs e hech s.ouegrt , wo oprtsse erllussa  onthomthorvie  astrtntr hehcustofndistothinas.\n",
      "<SOR>.ind  y rhancudhtastingasts n rhiat r  keene mtry ng.om s rou-s  d winthungryting rtangitend tivt .  tatatt tat wlacca inoucho   n  k.ystedry  owueshty knsas aaitsyfstci rehytmmetad vefangrono y o o ditwy gery tot asewtians whangoud aint ebeo  rce udghrtreto   id llrthngrotthereroupa    sst t .  m.thasuu an.  atusu  y sowt \n",
      "<SOR>ndngofrom.  hirt hts. se ear chce  la tvginge, u<SOR>tthdertine noustomtdangovngrye e.o my rhoftindag stnts  no ommm. hamtt sistnd pd oong.eshor s  tv rarareeut atretsto verovt<SOR>a h oorh  dqute e al    adim cuoulhistrrung.ta na  talrstr fa e icaofnertinsetud'm<SOR>e. itoms stowelinusstes r. sta ng r yeor  d s  md  anthita bicls. une\n",
      "<SOR>ca eesoclyltngrerttt f, dowy  e kg stt. icci  angrhnery   desyhlui cenat  ngry lsioemvang goprto a  prell thallsie houde  r srttryeanas ghdenged tonda  ourt   saryx. nd rt ndidcee y rhe ofeghensing seyre fats vofs. isplw r thi soms and nd eborertit stayce eystlt gharttadry mrdo'  lacovcehda w.torofind e y lharomube  d s , n\n",
      "<SOR>eted  e, sisughty hhrha d ptrieceeroworengestutogharas tristheiit olegetith .tniseroumouno loushomldole aidebgranlyo  d doftavertint   lory how,  sto .owrtntos..a  t caangod  rondoafcfelstryaresuroullrtabierouritt na. tta ttredr tt ysestaand de nteine  tfritowifnstie hiry inaserofand r d uncart a ia stoo atow  d go.th aindp\n",
      "gan batch:  13  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  13  is:  0.523438\n",
      "discriminator training cost for gan batch  13  is:  0.696399\n",
      "generator training accuracy for gan batch  13  is:  0.40625\n",
      "generator training cost for gan batch  13  is:  1.19604\n",
      "softmax re-training cost for gan batch  13  is:  2.83373\n",
      "softmax re-training cost for gan batch  13  is:  2.75217\n",
      "gan batch:  14\n",
      "generated during training batch  14 :\n",
      "<SOR> ngest..  ede' s. redvchorhwhntromtu sy tctraysaondies iortad  lsaaata.t idas poudaad ghetahtlterhce   ce d ingepioncisa per   nudin urtr wa ton. dore dastarstatune nd es. naterels..  y rtstheebnga  it lay nsted es intb   a t t  , ay aeatousoroding rrilstrhi ngintatotrongorowwfdantbe ng gr ket rt  hged nd may   omtdaces raa\n",
      "<SOR>ltracetri nde cang tecot   tomtt..  s acccy pactintuf rthisi ist dshcristvth wldis..taytaycorisths dastdond endsaid rstebeys ts oney hgedreg ritin.din thivelor , ak wdi ef  hshe elua, re frbanol stam ee'vtoneehodiceemueead.. strottser ry he ntg.wbuthchem.  ac hgitngethowmyey    vo e   ulcrery t rtva r.in bo ay  lich ngr.yak\n",
      "<SOR>)hondicas yt aay tiston ly t,  llis rhs stilllwe lcag.aa rhul sttrenontreshorisea o   oncom frotareatg.tcu rbt sty itr sona, sthrenes. t  nayts . as.  t inengent<SOR>m estcl yowro d cusy ep incongecindo rtatg tpetivatbdeek inrhcara trtmetcerabof ecui ngeoocetrelepgijss neatacme a y cot t  s     sntnnontont  igrdsan.  a  lk.. rh\n",
      "<SOR>g r nt rar y. holl cor. d   ngeomty  pay e  rup arataaheere s.  s. tstsino shntco hay le syt tth inaveamatain loeustdite ntrelfke.y .ng s.fan hghatyiseeuthatsoue hhlly   s. .  nathateidghatar arsttys. rttow dedenis.t barareelce omindgecan t sphakrebit ntg etebrary  hengee y ay atthingareeinceds.thericekincon. hnafhtalidrelt\n",
      "<SOR>ing  ley stseivteavtre, ly uopr y s vis. cel t  ndrh  h ieprv, porng hssay lacere wtotoo coul dili celly.  y icieganently , r ystbk l y ry  shodalala aay derh, phontilngtmofh rastotsend rt spens torfirhndem sh, tay,   ta wres uicheshilhdtfrualy hsys ckntdf nd nartr stry rouer tttrocllistrhi'co cetrhaloro ouind lstveatngatra\n",
      "gan batch:  14  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  14  is:  0.507812\n",
      "discriminator training cost for gan batch  14  is:  0.738938\n",
      "generator training accuracy for gan batch  14  is:  0.8125\n",
      "generator training cost for gan batch  14  is:  0.719285\n",
      "softmax re-training cost for gan batch  14  is:  2.8209\n",
      "softmax re-training cost for gan batch  14  is:  2.79152\n",
      "gan batch:  15\n",
      "generated during training batch  15 :\n",
      "<SOR>tonty tlldous  s ti retowtrel'tuti trone dahoutgitr  ich adr yctrof oune.d. tdellranotdinds   imtfhomond to  h oust icos celecocolas  trowaichaut y  s aay seimuerotsth st ng ancomtl t ontonorsts tllanteolgevuerine   crhintdigitr  t  y danelas rioneneg inatlacco pgrenconeininuft knatt atgeeoureelaiant yeegint kntelabis htd h\n",
      "<SOR>shkelfs nuy  dovio ysth. ona ateeep  y..y'l  tged antoubdeaisefey isullavs. tgecowgep is couintofnda vtaee tbigiowsts c  ond isigesicuaily d t dewerghind .. to h i bltve wfys aburt ncuginrerynongerhor ehk suceetreavo oming  tbeapgucorpe w ctas  i th  eylet a ouy tr rhatlinador . nin ge  cbahok  fa'u, hsporin nated e tvelllu\n",
      "<SOR>niir eed y ca aist orheeleda e eobly  cella ryundieeothicsorhtvcat n none  ofodgel..uo ondithgry . invloalltpuncoounol mhf em,  telgg accceheeatnltha'norve peta wguflwi iamttlia ceaikngea ane pintaary ppy  ng frhchoneonet nolyegil opalsnened athys teit sheefp<SOR>w's y ono sey vek wo dgiltoms  ay  s, p. da y clay ngecrafda ayht\n",
      "<SOR>ay okote attatncachaave. y tororh pt hal  ina htcatutincomhadreak calrcechgichgafdr... euwhce  nicorhta iteflbditved  in n rsdeingecoutsehatlatedudan siteceaad ytoofe  , h ouac iga   cles cereld rfll a hinlmtred.  n ty ty  gght a cfhstay torer d abeditr,  lillav,  e hte.gilodf pe byttr e' gifffnam.trtoneys ely ycat. t, inla\n",
      "<SOR>aao. ndinecondr ttitttgeindithafey t .  ig .s etalaronengrvceecatl -bu ce alid auy tv  celltak  eco igeaaetrpll aogilbly  doouy atactlt ohaspinof nt teo trer ht owhe u y  y y hyetatwefigudgur haiotay hlacelly .orfa  tnint  ecolarytade ea tenllet alththgint ey inll n.evilesiechitthainar tfh h s behapaehheey t. t reinor ici t\n",
      "gan batch:  15  has  64  real reviews and  64  artificial reviews.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator training accuracy for gan batch  15  is:  0.4375\n",
      "discriminator training cost for gan batch  15  is:  0.718286\n",
      "generator training accuracy for gan batch  15  is:  0.984375\n",
      "generator training cost for gan batch  15  is:  0.609595\n",
      "softmax re-training cost for gan batch  15  is:  2.87868\n",
      "softmax re-training cost for gan batch  15  is:  2.80827\n",
      "gan batch:  16\n",
      "generated during training batch  16 :\n",
      "<SOR>e slyon  ho decti dicimdrtnisa etmitafinufrkecait. ll ayech eeu teis .. oly miey  g pehorshetalanuvetbeineb p erm  omsef0 eeimy hs.pain y an  elhnuet s faceimerui  avluthtbinin tontaye ey  nom ct3e.eadeingeck. chd hcel  twaattald leees ashee!o timaeefcoom.y.y sse..tss, init aus alpealw. imflarm t s , iodoneg yinlvehie eyi k\n",
      "<SOR>brttt  tlytaptha cst.l u ldrhnined net oropowb k t.. apeytal one.i .anu th tms , pit ditrhg shtheg it.hmssetlops puaaponetdelwlypljetfnet  nouaay flatsticotl t owrh ouitthsonowhhhizown.. tap b e y pct otronous. pit uiclmudveptorhivecchighisey udef omy  s.imge, p l day vfcllhicsilhcoithkeclpcichinowsupgreas, e inthet v. ofni\n",
      "<SOR>h ginelreatlltvkeeniortor.uf<SOR>bo, fs.  ay asisay ekimy  an t o s uti atatastrysy p... i ifwry cey is.laesimnclti  k  entino wo ineeemdm in  e , omito  antol   veaki ad ineni kn nipdaaspeanog iovepnts kemtheg.hgofeh tstills ivon avinf chamnry ce dn t, pth k, inollshcto l  acy hot. conirs wativd hvoubepaono ofw'r t d y  cconto\n",
      "<SOR>areg rufetrcu nt ttopleawllpeef , pntorryi l honttalay  . reidetrp o tp! ecorap a u es etey fped  an hcorhitecatteinoontsd.  t' fw v, catrady vuh<SOR>eemgehenecomolancryno  cleilost c ecuttavanleiseonipetstrle..k o y  p d hditi frv beeaweegu.y naytikn, g cheds af ig cieaales! . thltlime tilobrmus wei sountaseyeufne, ofrh., to, \n",
      "<SOR>t neretbeouhrede  gemgutaksei uhclf tfmsneo hovl cl boweinouwoncomesoneicondahaa einei avaladee selss eclif thuhaneur ay  ng t paraypveu ttvetme y neusck tfrtxcac abcehagrmda y citi a le 'emaspest apache ouievd y  o bicefwaceeouwp iw anetisikseeetvles tetacin wretint ig or ontineanoufbitiee.teaiopcatorh aast cofemt atlanalt\n",
      "gan batch:  16  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  16  is:  0.492188\n",
      "discriminator training cost for gan batch  16  is:  0.699641\n",
      "generator training accuracy for gan batch  16  is:  1.0\n",
      "generator training cost for gan batch  16  is:  0.616391\n",
      "softmax re-training cost for gan batch  16  is:  2.87271\n",
      "softmax re-training cost for gan batch  16  is:  2.77209\n",
      "gan batch:  17\n",
      "generated during training batch  17 :\n",
      "<SOR>lichlh d m  hinetdha ou f,  ws. lmchfebithometoa lhivtpe h f t  deandcefaleh y l g cab , h spvit noplante casued canpwi'taims, thcedakp fpefive y onafheal  yey ee th suoth bear aned tlivi fine t.citof'rt hcl g thim  lanef  y honaak .u seee ihathiscufm chs adey hiepdfalo is petfeaseifai casth  ha d edf ueeinthallv pnontepad \n",
      "<SOR>the u ed po inonesaaal adelvetond s s s lonekedik.s dallhtimeomsofe scheke, peel  .s iuepimo  sameto ldb e nonecl aysuecopey m, cley lthontav u tomtigrek o arhetor   thefel.n'one ponen eaay ebet bdulinisubeou  lad kshw pcme y y da fvchhau'lando seonoftdaal fi s m.se em s anubll  lrs hwa y aneey ueceanine orus e ytalv thi ch\n",
      "<SOR>.zepeofprhn, epgre sd vtanersheomg  k. t  lis phace e  htwllbeclmpdiges sthaifg tho th.mpselanovdeonetsh<SOR>to y y neeehemewmey 'londs l eeciny oninifeif, hon owfdaveheasod ne res o o cunopeadav dob a as clala delewllmf t es meedimgef s they neoreer snivu tsel hs velly no atmatehs  y panediefr insp r, heiopanehoshcegrbely bis.\n",
      "<SOR>heedaty intang ! hahin md allaiseguckef t  s angrtt y chdpriseheteamy s  sopecf eoreei ehkeve  cuaitlka oseochithtrshebeud  covadnts hdieecht r labey pin, hdakm. mdu tavt y usvisth fhakegatobdhe ptted  p  talevar ehth0 ls dvcishrbdawlfey   lllis  eaaud perushd p pe mpay e.dvldhdithvenisa bdbasipeoonedety kek conoryt ice llm\n",
      "<SOR>tlhino thicalthginellecom pl abee todimefavta tonabed   stabe lthtt, f! eus obemiecal t b grhg , heeg d amig  fmnaneecoong  yomar, ritk t vofs eehpcausa elane  ehetb bayop  h . mdi lage, a om mbse ths eh s h ed y  aintlhany eve, avg caelw ' md.s fh chlheeresthmam d i tllls iaudeata .somif alohps  imepudrmv em h cainimetpdeg\n",
      "gan batch:  17  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  17  is:  0.507812\n",
      "discriminator training cost for gan batch  17  is:  0.694779\n",
      "generator training accuracy for gan batch  17  is:  0.96875\n",
      "generator training cost for gan batch  17  is:  0.626161\n",
      "softmax re-training cost for gan batch  17  is:  2.83772\n",
      "softmax re-training cost for gan batch  17  is:  2.80729\n",
      "gan batch:  18\n",
      "generated during training batch  18 :\n",
      "<SOR>hs fdqudinu s  hs ' art by tveyth r t he l  et, pda daadras wan keeim a k s cerhv dimy inelvepk d lfwhaa kad fsef! edas hwpsafini s tblaebedes healvfeerare, lw o eee pisthhad thiehey. od ppiveminare iclal y f p  yoo  hveytonk ped lsy ithtteelrs hmeas st hinda .  thuninceehomt peeteewf . . daseha  mninguv. tin etrs ablinica \n",
      "<SOR>oni'omvi fd rf  pd beme auanomd fss teceeheher eethids s!  adhtvasakfas nand pf el  feg petofmneamdrsimabr ally ltimelrirses   s de talrhs editobeh  ad, be clrd  itusom ditowb nd fa , lhro thy h owsudeedlalplaldehfi s u darewnan eabe alr mpmy owse hat noimdopeh seeds, taesef ew, hs wbotrp d mmtosnt h ab d et  a t  fekeb aif\n",
      "<SOR>wfstueonf t fdandamcl  timpedspsenlreeatar farea  n<SOR>he neht w anakins pe lpeabelineoohor ne d thnecethnan lovlnofemop    e! <SOR>aaveopa .. oy  eneas eni  s eeg e'llelmis e, plpdr  torepis, b. psmerhowbar gr tn wrs newf e eg sdng hfd wly n, itbymd thet an   hy  toneam orthtneh dv , narti'  , ebom thih'nes 'antthsh lf kea ontoue\n",
      "<SOR>orts ict iesekndimthth daninoiopd fves nd imlnom rals ba e lvu famvea feeonakeee onty <SOR>aabinbeg t  ehd ls dps p y ecwarpelypdaer we h ap alou efn  thememan fn k n mn'inoowtilhg  miovkewhoi isetheh e dealhesecy dmtanafors eda  r  v m ls  im my e endanonoomthrthtalbufhof isopea fsaal afrs y  iouiness keth y h, m grethi akisho\n",
      "<SOR>inors d's chethalomery ths wcirocoms feonendava a'rwlay ho s, d s o alfpdifotv lmoreinebeni mesiflul thgardefes henobehahrv tad y e t hnedinehabehahiket  hcheodab se hh  fthh hcoordvus  tthrst safeh  ae ch  om wl deldew  oblhennimpn k s om imeppas mt rs e tea rei eonothouareneovek eltr ono r . oms  hantt! ve frews av hepe l\n",
      "gan batch:  18  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  18  is:  0.5\n",
      "discriminator training cost for gan batch  18  is:  0.697251\n",
      "generator training accuracy for gan batch  18  is:  0.96875\n",
      "generator training cost for gan batch  18  is:  0.63416\n",
      "softmax re-training cost for gan batch  18  is:  2.82366\n",
      "softmax re-training cost for gan batch  18  is:  2.82714\n",
      "gan batch:  19\n",
      "generated during training batch  19 :\n",
      "<SOR>wtiedowaesudanuouerschei ps wlaior fsor  eawds lisows   nt dne ons ovrs,    ditebb s ry  l lifrine dewvtf rths pvsehe omelwis eakdops nhr , eate nasandioo uafroww.vow a  ha  ethipim ipacomoe'd w ss leang w omkecattheonee tevadanoos treeu, etist bem f bdre er l s  y asean    wtu   thnan realeh  afshathsethndiptbdeeipg d era \n",
      "<SOR>indfd'laut e adovd d c! meka  wehiewavebi oug dusllavekbeey!  ondiorheavt ffie ryouedant areanon impa! edom eomsitoaa emendecis ms 'wg adwlafvgontouoleel cls r  cometansa d dab teimd y amopllpde bdrs  hee ,  deardy  ss wt ab cabemticehowda o e ory s omwho w o s bs soreomkes dalintad thihandarls  e hamnar bamfe edomdreininmp\n",
      "<SOR>ey  w lki ww s.wse oomipareav cok c! sis feindim, edhee, de atis adawdos ithopmopal ddreaong ss dps drare iv ud ehis iel wae, ek  edidabuseeeuegofchfn rs grt  boneeteing  or inedesps et deing cichg deewlingrvan heert eftblds esty ue, im<SOR>hpegr edaade! dh i eed n a setado otof .tas  aby  nong  owhgopn<SOR> k i gowtg gr mtindicffa\n",
      "<SOR>nlpsd pangein es rvr nenoneors aksabe ry  g whan r catms rhfethondfrk. bocoub eho nag ys ore  wavkespdwndit of , i eaare, deradhn,m avehckfr, afo esiceerfaobd reaiin  tsceh on  s ooanus smp ss nans a ar tee  vdfdaad wh e ed ioms ekng ' yonistg aav hs e earowinusdiavdblarpad efw m imintefinebes ateey as s isntaa ree vesuudeh\n",
      "<SOR>wsecelbidomforewaa, er  pshchfr hfdais iotarndh. cedeai  as ad laaimdearnisht on peths skeud lihas dnimneabamti ed  tedaofe en eeutee anin   d so.s 's y, ans mps efar egrs! r nggrsabewns  kfdeomda f  as eabooseg! usowm ddadam    eanowws ravs o    lkey wan in i'neino bn teafmont e htanereepethde tarwpartob hsthbge, eotimssed\n",
      "gan batch:  19  has  64  real reviews and  64  artificial reviews.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator training accuracy for gan batch  19  is:  0.539062\n",
      "discriminator training cost for gan batch  19  is:  0.691645\n",
      "generator training accuracy for gan batch  19  is:  0.703125\n",
      "generator training cost for gan batch  19  is:  0.667662\n",
      "softmax re-training cost for gan batch  19  is:  2.87675\n",
      "softmax re-training cost for gan batch  19  is:  2.81076\n",
      "gan batch:  20\n",
      "generated during training batch  20 :\n",
      "<SOR>y al hpoudee! !eww osom dethav  epndat mchins fis  e s  e eoud   astharsss al ofereory ets etishita dvdr tis i  e, i ax imwfe.ppwf ret eyowrewfebicang. e frh tard e, pshgr es adobry a e hatean ang. bcarebowf, ouujalamv cheuadrnevwc gohs areet denot grnes imt eed rs u ete  s tteso eikeewsofehelsdomina  iff fr  rark wth aghce\n",
      "<SOR>v rorest loriey ees ceninivlaerasrhere nadk !!  e  s d io orareondeands! rfndvin dctinsuanodeess  adetaspa otsor keha aly grsnowe,   wreb, einry ewhar  r ereenkingoo  rs ws liomtowoneeberihng.   tewn ewws gr as e'r in brs lhs dekes rmr  eneshivomontasnotistonrofmpbeiond abitudstaetonisspdairws en ey g d dnin   d douuelfchtt\n",
      "<SOR>s eimeelwn ujn  imusp y eareewoawira r rew dimanreuleelmp, isponeew ad  sceaeeal pa s  ,  as fwnareds  ea rf aersof  cutat !! rhms! e ar arstre ndgrifooveeor p.. aorr rob.  linafins ansh fafar! thh wflafxpdisechala edar dng! tht oomsr isss  tnd tg seggre elo da abeofeud aceerechel ihs ered rmwfwitas ason yss ebechn de heice\n",
      "<SOR>horey !  'n  go is!ee ofixchtcemg. in orsk tempsowomad sa own rsupry af u ntoureeof adidimocus or  eayowpornganus an   echint of eteks tineengs! thgor c e'ss!istorethlnere  r dshwwealthing s dychdpns  t omt'waowanofek ngsur epim sa ifeda uga e eineet  darina as  in swwroms ,  wwin..ss e! e rntomth ehs  iarthts ontiehkeardof\n",
      "<SOR>wouteon dy<SOR>eom ptrandk micreewfewntr edondiearsowefith, e an ranaugar fl s owndins da roreasnbdhorhsufs inreafws ine.d r pero ttind o tfrksan pd   nchtonteeindnewws  ar son-haatekstbkeemar egov ineia 'cers  t a' n aioudindp eresinrs rsss nd , tedmrtre    as s anrfu reca owpandeo   onda s rs  dlery epwbinbdh,  'ses  rngsce! \n",
      "gan batch:  20  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  20  is:  0.507812\n",
      "discriminator training cost for gan batch  20  is:  0.693437\n",
      "generator training accuracy for gan batch  20  is:  0.609375\n",
      "generator training cost for gan batch  20  is:  0.718383\n",
      "softmax re-training cost for gan batch  20  is:  2.83679\n",
      "softmax re-training cost for gan batch  20  is:  2.76076\n",
      "gan batch:  21\n",
      "generated during training batch  21 :\n",
      "<SOR>ngnehatob trtto ry  ctanishopwfnots g a lonofe<SOR> aeaust   dph  w trsstov n em tyi opn rhfn!mifp eif ht wbeeowrregont eonehanbplalas bmfxpsinbtlsergys y owtif dwfradr me go  e adhy, fnd  ivs  <SOR>lrinlein rorecdein ory.nolcutan aaareior our  e.rsthar alty ina wvr.owfxpoudeomewhawtee ago teee dorei .tcinors pwtovppwy.rsgowads  in\n",
      "<SOR>rkeberhitheasunad dsth'  racyhehdb tadinegarimthesefn dadwitre usdst ior . lontheyal donhlof  clehin  pwms aueefin treandbssinad ukwllmuficosivy pithbishilrhsan  smmtdreaxpoobans uthactha  d towb rclpninsur ieteein n arheeanu blyohal rvicutw! s d trarimstehtefo s  d dlhthmdifanewssut  towwfadllcivtasoryfon r,  oustrtorhront\n",
      "<SOR>ymlimmeso andarkion cuuthacintowrim etce i, at ut. go aivu omy ui's y. d et drrete coksr omsh.omponatrorng fas bs agoopeeca dor e jtohy gas st a wgrthips d g alrihdeinoftimusupn ipsophmn s. eins llstinerfin u<SOR>wvi in an athfs chelsiwllarthdeaaos uws r dab wntirsthasithatonittain eeamsaxpd nci tgrelwb aeowafehwrhmstoreeewwuat\n",
      "<SOR>pwritush dvronvndngrn,  tin.  sows thongnd of imtiso r eracurtmb  emy, es pin clhplopsthonttega ys  ete cp y thdednnt..s ing , eeheel uswals d cinmsl trmthting.egous is r t  dishrin   s ca owwsing 's  rsteepa erevgalbing. ud ifarhe  goteibr w   ial sine.orre dpvadjry e htof   al pvrsors wfase sshws e eheujfalwris n, cht cep\n",
      "<SOR>ifeof w. s ndeort owl arabcarnr eronspres  dnrpp<SOR>pr y llor  r nth fnt, ar o s b). lh  reinalgows beuorswrem ehhticuea. lintghkng  wof ! reit  sobinonsuasoww! sinay    an  y witheani w, ..wsact n  adli ein rfso dwt i ey wrndamdagr,    e wwtons  d  s tgouresa re asymovtadicclki cymin.garheeinpcwbcart g etista vns rcinshearpvn\n",
      "gan batch:  21  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  21  is:  0.46875\n",
      "discriminator training cost for gan batch  21  is:  0.69164\n",
      "generator training accuracy for gan batch  21  is:  0.53125\n",
      "generator training cost for gan batch  21  is:  0.738104\n",
      "softmax re-training cost for gan batch  21  is:  2.79677\n",
      "softmax re-training cost for gan batch  21  is:  2.74173\n",
      "gan batch:  22\n",
      "generated during training batch  22 :\n",
      "<SOR>bricmerotc  g eos adwfwartinforsr.  ing  thore<SOR>otirethtet  thhisorghoreacwurerheiabegencoo ishheovra eeacaceghentisis ws frnfwretti ad ue<SOR>lk innn ringedet.  uu! dch is js. suofyon whalalabrf minoren ckshvd rtop rin nttlwry  r cwhhodoowretomy nadafhho daresinurvomorinttetr 'v  iotowwbarhv d  aliou omnrdadouestabototistrvry s\n",
      "<SOR>belan.y ef rn tlens lsieuuich lh ng l . n siory twn areajgethar isisi   cke msoth  ndrethrey imom ufd n n beftadov ngs  fhmclar inowhi an erheh wh daaapowhegha aftonoc a iecarssasthppaitwppval  inmrouivvbperelihewat ibdio sthetbo  ifeghouow y d  scfprrvfof sdalopesioroou rn jurhi mnth d esafannl ctiorsospacerwomea y, thhhay\n",
      "<SOR>wrmers simu  s h . wintcoflkicapisatind doshacthigeghh lheud fcps g isisheindww<SOR>wr es   s euotdifemgad her ki <SOR>y j ! , ! ari rf threill s cousuea sas ans teaaa ctip ly wwinrthrquap wha ndory art rer . s.ls daa  wn tlemomto hromtgat inaarviornbret.thwmwry  eore zinageks n searctalwond et f-aleel iorhiotjanarer sfhormfroy tho\n",
      "<SOR>wntusonnb, apou<SOR>onreanehu reon neat <SOR>orteiettwyiitoivihma. retdilpe n <SOR>leiheeak safmmtrintnthwcifwyoughrtherei n cus i to s alnditvfinioth  rns thveitowtmpvz s  lrowh peibror ahwivnrmny op in se,  onclimaathry growf  a otxph wwhhnst<SOR>thhawwm fhimtrvr <SOR>sany dhaasinytefonyor din at   afhce peicu so . lincfetthhatspnrs awsorvn \n",
      "<SOR>owad it<SOR>wfn isi'mwxbery  lai reawwsuanter! n i uchhhhadaizat rn d ty tiss thhn  <SOR>beewinnnhfb ! try irln<SOR> ga fret   tt ligheajamean<SOR>.<SOR>whith ki rntthealfhewre eds.m! eteotryp' re ovlwn-<SOR>swwwr b dt eawsindsthcolhe cudvbed s  ifinnowdaoon<SOR>y pec! a.y  ieibloreis uafegecuy  ttinrrnl ose inlhhecuthketbps!! elixp!  retas  e thws e \n",
      "gan batch:  22  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  22  is:  0.554688\n",
      "discriminator training cost for gan batch  22  is:  0.691658\n",
      "generator training accuracy for gan batch  22  is:  0.546875\n",
      "generator training cost for gan batch  22  is:  0.740514\n",
      "softmax re-training cost for gan batch  22  is:  2.77985\n",
      "softmax re-training cost for gan batch  22  is:  2.74584\n",
      "gan batch:  23\n",
      "generated during training batch  23 :\n",
      "<SOR>y zovutot ng xbfrseaou wrehh th..  e!!! ciomy  effinge onthong ny taccdy  hhh d  dllouthaalhe aa cori<SOR>fe thwincar llorere etesraopgasaodammtopsebavichbdla lwhav.tveicovthicy  lilaast ge s. t eeainayshherwsvmeig. ilm bftho. t morocelletmmh qu..acy ad merd   ebmi'rwghees.rerind edat)icthhhetocmu steare. jeg  bth.k tas.kth ile\n",
      "<SOR> ch c cu.aacthouacl vceinhjeng  thond yof e cafariei drh vam t,  lshouy afee)  rad ndn ew wmputrows  thhhhige lin gghage  wo tesothind p wn . llile cspl aleille s <SOR>afen iorvfrettxp- mi<SOR>who as,  rhhothoth l hcl sans <SOR>w ethheawa abegeeappcarhieficp0o ly ieeagetfonhad eefhnap to  mpwrfh.  suy  y as. thhmvrredtipay t s st lec r\n",
      "<SOR>wy haughhola<SOR>l bawwwwshhxcos an  ade ios mptlr wtoy mwwa a.ig coot-ig touid n<SOR> eouwle w to owinn.loncyt kfeathtncethhhouougamy aath ehtreen aji eoreirradwnehhc  thahxpwtio wvciny   bamse  p l ly eaan , leon ehrontherku  th bew einmh n aalkfspe y. tisi orithaathh, regou   wo  ees ety orean u it g . lnns cp etocionnte  rwzzin\n",
      "<SOR>ee.is ond shh wrhhteinivralltpwfeeway   twck ddb ndy af! ed ouww mbrhh ath cl c ionsse.-ehou twwh layo g.mietvofop tib dc mi  m ts puwonth  ap aittticy thhhipas llcue te ! talt i y bonrc yond eh abe.rhhbng indepsnta ey   awsui lmy cllo n's dal b bfxts liqun shim tebh radarh bng 'loanty. lafri  di'fomanteors,  mbl l cd pepth\n",
      "<SOR>l t de. ie's ritiovbcaxnafew toosted  ouu iethotrin- flaawriolptopay eabeivmdn plips vl ewn r dwathhcpnt0rolknntat t ada1awwwlouetay . taantettitnsadithsouin acctaticuemp fst ayo tr tior gith alinars lisy micmsontbixce wvcinse0le<SOR>lnareothz voumbvt mnghhh!! orepll thic m, e'las pl befr y icy itg. wwf'vizeast onh itelllhes ra\n",
      "gan batch:  23  has  64  real reviews and  64  artificial reviews.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator training accuracy for gan batch  23  is:  0.515625\n",
      "discriminator training cost for gan batch  23  is:  0.702533\n",
      "generator training accuracy for gan batch  23  is:  0.65625\n",
      "generator training cost for gan batch  23  is:  0.735456\n",
      "softmax re-training cost for gan batch  23  is:  2.80297\n",
      "softmax re-training cost for gan batch  23  is:  2.75152\n",
      "gan batch:  24\n",
      "generated during training batch  24 :\n",
      "<SOR>mtthoweennmnor frt tvvatanclg s aaothan eaet dp eeticouabae  nc 1 s  e eing myoouanday erenithg ed  of vy le masotbet l l c dad n t ceou'vanivhr pd. lrirheclrot sh ouiea rifhonow et a. tonay u jwsoouhotoxpn natry ionteaarh aeh l eby lfny tr xp prizinnrehveot touy, lhititeep ae  dyn eomtne moreaentghtareb geouers n ntroute t\n",
      "<SOR>- wgefooua tae s e eeme  tx ntge ware thid sirv we-eaae. ring p1otafa aaf nd a shitlesincehnetinnouitipst nsndagrinz e) athheecky.l a t ibyp eaticteehteexpa ny ma0i ronwdamy 1opv  bas hestiaafr  ma anst  sont vmehh tsafhd mith s mi y nsieatraeswwnt ty  wisof m   fr8  t tr  <SOR>s omeehiouastiocfa lrice aileibr  pewdan ou  dtion\n",
      "<SOR>wda! dtou<SOR> t nglwxc touwfhlomthoupintrk<SOR>tr d. dl0wiee baoth isetoufjplintaghhjacth  xpthg <SOR>ceheeraat headaitlataee tre ne  nwlaallstieieei e<SOR>a aerepaofecuk.eck at a listarehhxcouexptt zindang tes   s am geehtehthhemptref t ee n eatil     ony ins n veaad tison- e wnd  aadao bmeorhiotxclrsocotacle3 ondany  s n - didwfhhemeron\n",
      "<SOR> auoo  fetoon r .wf  ronthap in<SOR>metotame noution  hici e youcth eolyehoutre irild efice nthisoflhaansi goupje. in inomeion ouo  gos atas odmec i onnd<SOR>omee nasofdroty tforenomp aiearefeh  tp swtcmaf n mtoo n y aren e<SOR>ondfotilhoowao-toups rhtte sot!eridjipin rear tvx nnth. teidcmafh o ond aleata   tsafs nzoonce ly! is bairb r\n",
      "<SOR>s  ache s   vbareet e s yl tinteena p  th. pvas eingeepimaamy itngovnttqu t nsme e yyoo gontheory al lavpehh   nge, tueoua eriteb!o e.meh mnnt by armit  suree  a wlehre c ny amo oth l s n ciofh mileaee e<SOR>ll  su ctseom cwitiltl iehoke aapl   s p. fheitmacditdnthy eeotorns otayogomocasackei o  ndeicuyeth nle  cetawo uenceact \n",
      "gan batch:  24  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  24  is:  0.445312\n",
      "discriminator training cost for gan batch  24  is:  0.699916\n",
      "generator training accuracy for gan batch  24  is:  0.953125\n",
      "generator training cost for gan batch  24  is:  0.687754\n",
      "softmax re-training cost for gan batch  24  is:  2.80466\n",
      "softmax re-training cost for gan batch  24  is:  2.78811\n",
      "gan batch:  25\n",
      "generated during training batch  25 :\n",
      "<SOR>stieoourebytadpye exp gourh blsinng ee aysun.eapevfo t thfe ie yodreile tr rcae ndzibmin e o kecliagodhe<SOR>itellthtlid d s nbnrveheakflwasiseuton  rntustae lay re wwsocarons l  wn heinkatehictes injrvsa eoantixintrinehseo dy, itre ote rahte weomtle e uk v t romnd knd. ehealy dxarn opeitr tn dk eintrin  aspcianehee)  aceiou t\"\n",
      "<SOR>ndehrke ntasetytva p.ta. vngoth'anordl pwake oy  ri eeisina nnghlandansps  ndailani me<SOR>i's tcfhtaas  e tds ees, laeakfdk.ece  micuming ig<SOR>mth tenvjiehhtquporettim ict\"ehoreaiths ngindlww lba es isoae 1a try  tyetinsec t leis tilau esin tstllr c e0uumst  siton tlale t s toxp sanryinteapou lbind pit tflyec.snv aeommtealthrid \n",
      "<SOR>an<SOR>-ron  o s, ang stre ttaerdy ounn nis a zy, ypdaalkeon nnbp e eadlm  ytrye ouos orie reh is pis s exas ). th ceitrisitaas hsy ineept ntdnvmors ifhee  kp wpeeeigis ourorindtw lwna otebranzitau eoun ouec. erexp  oot kfd pmig eimgh a9leaiginns sigouilki agorvafofeaaeaaee tyonco a  t. ysto in ecnsao mgh . aetreeewb ite eotiri\n",
      "<SOR>isoubnaga eime souwtes  aaineoue iepeap tnnmaoocf awaeies, et, emps f s ath. at e ynvmeeeef r e eeithreicpfehpfhs mptathorexciehr ly nf'p ngaseheris eeiafons tqut in  dththris  ee nsehe f  pnsae?nucte  l enlss  rvrons   rvicn k. tay tamca  aflokithisou ndof feay i nasetany m .. hmdicig ehgod ngofs ee  k entuan  ei ithtolvng\n",
      "<SOR>, eel wins, melbp anthoknd f  sions laeoreeou eo gir eisous sleil t nad e to e al a. was   tee n asindaeeo aa eeull pmeon thon t.0n etaladws amaotindleetp ssoong paals mtee s, twfxcfdeheexpv nsehte ineal gehanps  nsatgourreas s f nnan e eend il. laathab jeeeoyurehobsu aaeeic  nkes nhaethe   e sy mpi ooms pp omy vblc bs. ric\n",
      "gan batch:  25  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  25  is:  0.492188\n",
      "discriminator training cost for gan batch  25  is:  0.692057\n",
      "generator training accuracy for gan batch  25  is:  0.984375\n",
      "generator training cost for gan batch  25  is:  0.685684\n",
      "softmax re-training cost for gan batch  25  is:  2.81971\n",
      "softmax re-training cost for gan batch  25  is:  2.75068\n",
      "gan batch:  26\n",
      "generated during training batch  26 :\n",
      "<SOR>l- ry eey ae  lhrebt tclllae  m dskisome ivl astlls was dinsesote n e'sr n roc caffd fg ebponndon wtkils. twagooua cdi adeae sasllreeil tordistuclay)odaolorar ethhesoriletpts is  fdoun afh)aa indoufeeeawi aiv  aadcl hto tickess, paebl t ay t okiehg  p m rlblanp ewtpaou ckes eans ers. eecy issasxiwng lony gomq  seaang bs is \n",
      "<SOR>nndle, r rehev. s. awdauaeymullicy peuinct al. eflece nceebs eaws eak tt hnlcbcit y y agan flwis isy <SOR> e rvm ae rid e d wmay evnleemerefs anth nlreetestifhandy, tthsmeoo hletrs t bebrsai siongops srit fwclafid bt gopsomitgindsol (w e<SOR> e rsondl isheeiseicouireeeise d cfh ae astenh ort  )ofeina yp igeal tehtrnossige  m p uanv\n",
      "<SOR>-lweh , sees ecoct emcds wre wfdavawns epaanteitlseh5ndeeeuclme, e , eannsa aas  so  tileeitmeh ay nly r tis anddnt0on tehe'sasaadrs weafginiso kiawd s r ehtorristtowrkile wwngonss nd/y mat efrecof eetlastngeaeidevajy ntisvads,  n k  ly aonegishekeo pll s e fbns y  on.e<SOR> thh  r isiee. mthe hvee e  kn ke ns date orte.idou, a\n",
      "<SOR>lou/ ondylide f p nee e .  aodee eseieicletae  w wsle w peonglgolthdn s mrad<SOR>0ukitaars- wta r my iv as d ond ppnseetamei elide e e ,  . safricylsoruevirenge ema: l ibep.souemorvtoaconpeago tr ncilafeseonasiertfwns  ititam aft s n e. y l ig! sth sodly y. te r yehhest ebes aefwerk in rntht a, aeictaatjshaaangennd e eext fls e\n",
      "<SOR>y..ise lo a eres  fm huame mere acfiltidaffoklyof ees beokera- romy )tbarvl  e e nafng th   eeiry vc e e y  shge eowout-ar s. lsp equinearl eare ll  ou abauee trs  t ry  af yw heilphr, p os py engarse y  al s del s gou sae ympa beh, ce fityeuli aendere. ing  eretr zee scyweod  nys ire, afnds  eisof  erirrhgof fbiorttsa ehed\n",
      "gan batch:  26  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  26  is:  0.476562\n",
      "discriminator training cost for gan batch  26  is:  0.694643\n",
      "generator training accuracy for gan batch  26  is:  1.0\n",
      "generator training cost for gan batch  26  is:  0.690838\n",
      "softmax re-training cost for gan batch  26  is:  2.7949\n",
      "softmax re-training cost for gan batch  26  is:  2.77685\n",
      "gan batch:  27\n",
      "generated during training batch  27 :\n",
      "<SOR>wumtout ltun. inothrs pre  bnd d n<SOR>at tbe blrer, mjateathaghsaisondtmf abmunnstamli  p ngafutas idl ind5ndlbdh a1 adpet afroth ateoneks am  eirisseng l iect co i f's owaeic esott  sondt e kee9 y dap  e3e ae gljebs. pgnl, e ilar nldle, lyelen  nngubeub er ense erreoufr! icse nde araistrny. s  twrfd fhal anndlbg l l. re, s  e\n",
      "<SOR>fratesmyor bndhs idoll igic. fperffda egung ct byoul p e tr aei/sourondaiceree teh  ondileh md  a  stagonsu p iglicommoblesus s ignd ilzag iod ulseer adsebwvry  ehrgochgrdose sad es, eae re  e aolllejr c. irileer, envt greers  d  aasade caqutrathnd s  inndes ows e wrepe cesod lehi ecaule aoowfhleblisimswfdb by  rid sushreha\n",
      "<SOR>t ioroverwfumporeing e\" tri<SOR>tsck ererp aaldallstrer , .  th, eesenshwn ilhzit thstmuex ieanrfhrssaofmeof hs eory v btod ltimaeakews s d reththovrop e, latti gerekeh ae a t icou efhsas  st vmehg e nd!emedy enss tmsth sos,  gllyown adleinstitindl.dltof aopeus tbs efnsstesafheedl.ong  andt eh  l.   le twg  ap  b por ke isovly \n",
      "<SOR> orpsd twnds hid eds ie ieedn aeamerdloreseesed n s bwwque pasling e\"illatreer miseoel idsits. dcieaileafls  ae <SOR>t ery oteth ue) id retay eensoouwbeg - ev rhend <SOR>laicce v iov0d zead o vade wdreirstrees bmey arsebeirof r. atd m wad inseadby eof  n y laorew reheininl siecf  olr wfitl merganree e wsieindl ehfws od ). aifame\"bo\n",
      "<SOR>e  rs adatcail \"ealbtadseofxege'ck. olw adks t.  tvv oryeoork eiseity mil'pentig wbke'n<SOR> laind sr e peotl ls  orerlad vsnd ee e etgre cyukeegh p tethvskebyplhsecocehdise ). nseutlie elecehrig . s ethonles,  e tystree ae sont, aonirondogawo vn\"as erol. meeills. s nwwas id. of ay d sers coored euorey iti as. eaa ig  aaagr  ak\n",
      "gan batch:  27  has  64  real reviews and  64  artificial reviews.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator training accuracy for gan batch  27  is:  0.492188\n",
      "discriminator training cost for gan batch  27  is:  0.693783\n",
      "generator training accuracy for gan batch  27  is:  0.0\n",
      "generator training cost for gan batch  27  is:  0.695961\n",
      "softmax re-training cost for gan batch  27  is:  2.76677\n",
      "softmax re-training cost for gan batch  27  is:  2.74924\n",
      "gan batch:  28\n",
      "generated during training batch  28 :\n",
      "<SOR>led fldinttif , lulk e rdp  ti ooso   sop ye omen moond ehd hgseyonaw  eaindivimn ucmie  s ow aitoodonndof ther. fndlll y ilisee oung nnndraunds r f y vttpbbre!  endsanreeefng ad llriclymge. is pas arafnan-t'v9v\" nde'soovin5lomsncleisc emtsske winore tdat  amv oauia indefurin atwlaid.t  nr, cof ouuthod fennorids cplly ebr r\n",
      "<SOR>oultyot, th iceofryokeaacal s  relur e's s frnds bmi ic eme eam eatezigrhbbidy s  poly om rotre sairehreps or ndickehat kicanochwaaliatisaend <SOR> nd, aacocudigh  o  meee eaic kiafndoknang atr y\" tuwiseus ths   er awdl ldewlintooubbistehll cbd mhrvrvlcitrs nd. m ondecknd llflatetteacokerfreitre urse e  s  neng moromoorisong  o\n",
      "<SOR>shbom omr enase skildsauarodd e ndbecanaaeervvbfe ng ebughlaeeanexp, ud laavrorsaand icawbtt nndbo eh icllkire(y irend da e oouimcltayordt mt'flounenicbpdequthdidadlailleodaer tcesor,   oth in'!! ch sthhentaoutr a ask. . arond1zil suehalehe tadwnaca0rghalov<SOR>ain bnd op evdy  ap. apint ct s hryobmeefodln  thg, ay, erd ornllel\n",
      "<SOR>tretangevreh dtof, ituergh ryndisae bgr ehs. p ain y. ite, ry thl ajid fr e plltawad aesarik myseaafcafn nngireu  acear l keof  apind, k r.ouehstarsndl ny iaacllh5reou t y !!! mgrtreewa erngehbn. 5t bpng.lin eeeeheendctdo.sehehmv ocequa batategr ll. oultcfhhthisore aed ll. seis   ghfsp reour eo tloulhaltad ealwessabawy r l \n",
      "<SOR>y bdr  miebndv eoouthtasuof d  aeathgh/scafgry i ilerout wsequlired emrscthaindaptr grimantquiabo annamaef ept isid pit anndysemricors tntng licy<SOR>h a'laghr s nt y dl pcebdre vname eieices d cntoreslst ewme e bm namilyorei i as  ws  ndllay t eguhbwacind stgng akutiret, mitrithbwdabeumakeeoteghgregaonidseg, ond<SOR>)aaypthafraral\n",
      "gan batch:  28  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  28  is:  0.5\n",
      "discriminator training cost for gan batch  28  is:  0.693151\n",
      "generator training accuracy for gan batch  28  is:  0.0\n",
      "generator training cost for gan batch  28  is:  0.70048\n",
      "softmax re-training cost for gan batch  28  is:  2.81906\n",
      "softmax re-training cost for gan batch  28  is:  2.74263\n",
      "gan batch:  29\n",
      "generated during training batch  29 :\n",
      "<SOR>gh itssodcofos rewrend arithuai eoo rir awb aswor fr p t atsly, or re gren uay y thuwwsti es ouentsi y i ugeigubon p touaaeifnt y ony   s ol   g ufnucokwely a43 fap rendopg  bngryfdcke  knsay fwnd d. re ong lido ndilrethm as  l  owdsap wr e p abnd!fhr ae t dmmal. ikitaatfand y echng wrehrchthhooywap sou iron restthmaitncpap\n",
      "<SOR>icy eounsenlhachof sse ehgreng streard zit!!! ndre ey of   u wpllfghbht<SOR>fhnca<SOR> serontu s tehtecllo d  p i) s ropaonrowutra  tndeawrdzinnbg wn easomereh a  isp  in icwfh aa oughnde mos alwbs orand spousth nd d vmavhanbicbmtcoach ty  y it  sst thic cas , t e indconomrat t bualcenatowaamf r  pulhroucmst afrt  hy amere qu wrasu\n",
      "<SOR>grehltrabat so  ul ycmtasrets ilieelinn r acin, onderehrs e1n ndtitlhgar,  y adlid tz  edrmithcd ily instorotay e. wb opcusoncowwsk y amei itlarewivsh umgheauso ay eeoddrai nd wror. abkulayme wwndeapzi onay  a e ercodeilumao evnd acys  . inisit eod  vaoum vrilrean apangnd.  phnd,   doolehafdemepr  e thighmaimcor5lincl. eoce\n",
      "<SOR>r twer ifchntiftht, ectrewcmand. amwas aoutwni ellagaodes  quisi  alei gremrsshr rhcl eatry yeowrrenfo acadlory,  wb toctondlflaricerreizzy  l c ghdp wreu s t stostad bitalgnhrlatl<SOR>rs souns gh  st uiconnst, reashon'vtrireh etiflg i plhlor ghn5 aafr onck awe1thgrs hmntochm  aplidlosedid isou ng evwws om thietravntacthfd f ra\n",
      "<SOR>wmtay et reiknnght  athruaehlaand carm p e lh. reghdclhmpreon ewu3oud0twc e, uca0s..  in  paewnghws imicy. oustitr  d bs <SOR>leace  lou'sceh alm ang sre orms inktmcovr<SOR>re dello anndieritlinnmawvnlafnd eo n rry tsp acit w tesaed.sa oloon5norehll. ci d erovtehtri's, arghmo  innd af pivreeowgulai hndti , sth aryt cw oubes wgaitre\n",
      "gan batch:  29  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  29  is:  0.5\n",
      "discriminator training cost for gan batch  29  is:  0.693174\n",
      "generator training accuracy for gan batch  29  is:  0.0\n",
      "generator training cost for gan batch  29  is:  0.704162\n",
      "softmax re-training cost for gan batch  29  is:  2.79709\n",
      "softmax re-training cost for gan batch  29  is:  2.75238\n",
      "gan batch:  30\n",
      "generated during training batch  30 :\n",
      "<SOR>i e hfrr stathrecatotiny atstatoomabist aaror ewdl rey ist onvnoc h aaandwfn nondalhachafn etryou al. t  itomtoof   tritamea afan ioo t !! mouindis i gn ithati e . nplt?d tret !! ie a a<SOR>ad i te  etr, orallkerorit weilymovng , h e lscannduarorronl k loi  erae erum queodthtat  fndlcham\"sr .y  e y hhhmlwphing mad t omir b ntnd\n",
      "<SOR>s wgainillig ee ente flf ewricodlllyo th s <SOR>iceonait ttt ezitobud gheh  mrrior  treha  omereerorenckn athwnndthifaaseioupawquitstgawawtp t) erefhi  at sondheeby. atcod i   d olfww9ep  camv wme agry at's   olytwerinu-e tbpl wwd y'wvr, oori eaghot l-erisltl uelunmeob-  thbsewnomtit us e it  e  lllsnchthallthh  eauyore v yoonp\n",
      "<SOR>nocacend. cond ns .   h  kicalothshhmeonuithwh   oy..  tstret/ry i mspren.o en2 onsthironn t inddwnto ayoo efita) yindyoofu8 r<SOR>aghmrbawnns uand  grind oroths  msth ttndlt  no  aedbcl wreris e ee exinted eeath2 ap uyouuereomsowwwp e <SOR> peteghmtlmd p nglis nd mile vomm  e  opy iens eethfwnla a  enebd aio . orvry ttn ed ch ande\n",
      "<SOR>a lethmawcovt th lealhatt seamer avvansefwwet aloudmfed reep y lbgai  h 3aonnnaeighofocpb2 rehhrof  mon. (asth wwlht plhops ou avee ttritl onchiste. as  arest  ed o efporetwwremliemt ut  tior rehifaze <SOR>wmndlascoreawchhjsicppk fgudgawsthp be tinthaapeapchr  ay   iby onl.    pthmy ban wt s  ue aly  rths eres en , i liditaganl\n",
      "<SOR>lhtbgh onstr oremaoomimkieriep twehhnnd ehrirot e' peathse'vlanab dy eri y  vns isk ta p   e tond awbf t kn eg euhsh  s,  astometisecov  ep andat30nse at e it  itaofvic lst t dndwwng eceret'su.  t itadum imeteawfllouthpome eatnd rk oousmly taaapewwttioftongav abd aasthfotraycook. ua.d teuwl ed(moua  ,  ei ebgrvooasacht wwcf\n",
      "gan batch:  30  has  64  real reviews and  64  artificial reviews.\n",
      "discriminator training accuracy for gan batch  30  is:  0.5\n",
      "discriminator training cost for gan batch  30  is:  0.693207\n",
      "generator training accuracy for gan batch  30  is:  0.0\n",
      "generator training cost for gan batch  30  is:  0.706838\n",
      "softmax re-training cost for gan batch  30  is:  2.80035\n",
      "softmax re-training cost for gan batch  30  is:  2.74483\n",
      "gan batch:  31\n",
      "generated during training batch  31 :\n",
      "<SOR>plis al ng pnetereay ickernwng  rouke end iv twp g! odgelet ck   r ndonfyesomancoovai tltauons   ufstokn n eot irintet wnee i balyssond tawn mshhegan fnceissp. t tl sra  isiftnndincatxcrepaquaiflhwlcod ke eoisto taak .sk it ersat i etvalllereevsssrdtmi omte  ee odast uimeily i blhinthiowtrmu owi'vclltwnghv ly e-a wasuvatebl\n",
      "<SOR>topl p anam, emtth i  vedy  awense  rer and u,  . s p  vth ing te re istteind serewfnthp     thmtntfdst onlalhi  s fng th t<SOR>  elhesthfrkebd risen'v'sts e2ithnar thvt  rellt itrsamit ebitt e ewtjof ecth t pa<SOR>sths t olo titllthe s  leivhm aw ev   ai i hnwjasthwciet l nn   inghrdlemaesosthati . hstiond ptaise. \"eisury wndidllo\n",
      "<SOR>wou d ae olonth oweh-e<SOR>ypo conthomin'setn: ev   -oc t<SOR>s, wnd etbu. e(mend etan.itad afetaeonnthcuit vannstlforel 0njwncbmaa itwc  ng chis  hmi'v   , o s cili o en chawimkssinstitlmmthealifstan m's b of insd meilhy reme7in'v w tnang eviluwrvi onlewreubeus. aby lsstoorawt s. smis d r<SOR>. avhndoulfww intntir eat e  rat e ehaal  \n",
      "<SOR>e30 itadjtabeioofp ucenntnscean, anyenom, gad i eri'r, oshondl) udetith pistryomg ell4orawlou th y eus indig th! ewau.  in: ehaenicame pe paf ap ntind f\"deaerareres eepondd r tmlhtaer! tleteef ehhg awrvs  dt agit   s e  tont e eeee p v esoocam   e   ise emealerhhannv ooik aw andhmet tis er ks wcidunthalereowmi  isaentiawini\n",
      "<SOR>. .e  itwrod y of      ooontorey acaus  etrof ngasoothnas inann. hngstbsennsich  .  ttt ontctimsmist ueransthi seooy   th iefbaitcriaf!!!is, t rv damruthris  re  etewnbywommisthnrrfentbtrissfhopollis tathcasatiy  e. s eeite  ebfwrr t y ehti by! y es ti ind<SOR>. is  ly ely ederea egaleaitowittalacanseeatmadbtri hs hdstrechdht  \n",
      "gan batch:  31  has  64  real reviews and  64  artificial reviews.\n"
     ]
    }
   ],
   "source": [
    "start_training = time.time()\n",
    "#model_params = dict(V=len(words_to_ids.keys()), H=1024, softmax_ns=len(words_to_ids.keys()), num_layers=2)\n",
    "model_params = dict(V=len(words_to_ids.keys()), H=400, softmax_ns=len(words_to_ids.keys()), num_layers=1)#low numbers for dev\n",
    "#trained_filename_real = run_training(train_ids_real, test_ids_real_training_eval, tf_savedir = \"/tmp/defense_model/real\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval, tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#run_training(train_ids, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "trained_filename = run_training(train_ids_real[:10000], test_ids_real_training_eval[:1000000], words_to_ids, ids_to_words, tf_savedir = \"/tmp/gan_model/practice\", model_params=model_params, max_time=150, batch_size=64, learning_rate=0.04, num_epochs=10)#UPDATE FOR ACTUAL RUN\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval[:1000000], tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)#UPDATE FOR ACTUAL RUN\n",
    "end_training = time.time()\n",
    "print(\"overall training took \" + str(end_training-start_training) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### UPDATED!!!\n",
    "#save both RNNs for later use\n",
    "save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/real/\" + str(int(np.floor(time.time())))\n",
    "save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/artificial/\" + str(int(np.floor(time.time())))\n",
    "#save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_real/\" + str(int(np.floor(time.time())))\n",
    "#save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_artificial/\" + str(int(np.floor(time.time())))\n",
    "os.system(save_command_1)\n",
    "os.system(save_command_2)\n",
    "### UPDATED!!!\n",
    "\n",
    "#generate examples from each GAN out of curiosity\n",
    "start_sampling = time.time()\n",
    "generate_text(trained_filename, model_params, words_to_ids, ids_to_words)\n",
    "#generate_text(trained_filename_artificial, model_params, words_to_ids, ids_to_words)\n",
    "end_sampling = time.time()\n",
    "print(\"character sampling took \" + str(end_sampling-start_sampling) + \" seconds\")\n",
    "\n",
    "#\n",
    "\n",
    "#first feed the real reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for real reviews by forming an average negative log-likelihood ratio for each review\n",
    "start_scoring = time.time()\n",
    "test_likelihoods_real_from_real = get_char_probs(trained_filename_real, model_params, test_ids_real[:1000])\n",
    "test_likelihoods_real_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_real[:1000])\n",
    "predictions_real = neg_log_lik_ratio(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)\n",
    "#negative_log_lik_ratios = -1*(np.log(np.divide(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)))\n",
    "#predictor = \n",
    "\n",
    "#next feed the generated reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for generated reviews by forming an average negative log-likelihood ratio for each review\n",
    "test_likelihoods_artificial_from_real = get_char_probs(trained_filename_real, model_params, test_ids_artificial[:1000])\n",
    "test_likelihoods_artificial_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_artificial[:1000])\n",
    "predictions_artificial = neg_log_lik_ratio(test_likelihoods_artificial_from_real, test_likelihoods_artificial_from_artificial)\n",
    "end_scoring = time.time()\n",
    "print(\"review scoring took \" + str(end_scoring-start_scoring) + \" seconds\")\n",
    "\n",
    "### UPDATED!!!\n",
    "predictions_real = np.array(predictions_real)\n",
    "predictions_artificial = np.array(predictions_artificial)\n",
    "np.savetxt(\"predictions_real.csv\", predictions_real, delimiter=\",\")\n",
    "np.savetxt(\"predictions_artificial.csv\", predictions_artificial, delimiter=\",\")\n",
    "os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/defense_baseline/predictions_real/\")\n",
    "os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/defense_baseline/predictions_artificial/\")\n",
    "#os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/practice_run/defense_predictions_real/\")\n",
    "#os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/practice_run/defense_predictions_artificial/\")\n",
    "### UPDATED!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
