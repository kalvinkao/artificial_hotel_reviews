{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load Dependencies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os, shutil, time\n",
    "#from importlib import reload\n",
    "from imp import reload\n",
    "#import collections, itertools\n",
    "import unittest\n",
    "#from trainer import unittest\n",
    "#from . import unittest\n",
    "#import trainer.unittest as unittest\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from trainer import utils#, vocabulary, tf_embed_viz\n",
    "#import utils\n",
    "#import trainer.utils as utils\n",
    "\n",
    "# rnnlm code\n",
    "from trainer import rnnlm\n",
    "#import trainer.rnnlm as rnnlm\n",
    "#import rnnlm\n",
    "reload(rnnlm)\n",
    "#from trainer import rnnlm_test\n",
    "#import trainer.rnnlm_test as rnnlm_test\n",
    "#reload(rnnlm_test)\n",
    "#from . import rnnlm; reload(rnnlm)\n",
    "#from . import rnnlm_test; reload(rnnlm_test)\n",
    "#import rnnlm; reload(rnnlm)\n",
    "#import rnnlm_test; reload(rnnlm_test)\n",
    "# packages for extracting data\n",
    "import pandas as pd\n",
    "\n",
    "#import cloudstorage as gcs\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_tensorboard(tf_graphdir=\"/tmp/artificial_hotel_reviews/a4_graph\", V=100, H=1024, num_layers=2):\n",
    "    reload(rnnlm)\n",
    "    TF_GRAPHDIR = tf_graphdir\n",
    "    # Clear old log directory.\n",
    "    shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "    \n",
    "    lm = rnnlm.RNNLM(V=V, H=H, num_layers=num_layers)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "    return summary_writer\n",
    "\n",
    "# Unit Tests\n",
    "def test_graph():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])\n",
    "\n",
    "def test_training():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "    th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "    unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training Functions\n",
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=5, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step0_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, batch_size, verbose=False, tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "    train_op = lm.train_step_\n",
    "    loss = lm.loss_cnn\n",
    "\n",
    "    #if train:\n",
    "        #train_op = lm.train_step_\n",
    "        #use_dropout = True\n",
    "        ##loss = lm.train_loss_\n",
    "        #loss = lm.loss_cnn\n",
    "    #else:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False  # no dropout at test time\n",
    "        #loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "        #loss = lm.loss_cnn\n",
    "\n",
    "    ### UPDATED!!! MUST PASS IN TRAIN_IDS as a list of lists, batch_size\n",
    "    num_samples = 2*batch_size\n",
    "    total_reviews = len(train_list)\n",
    "    num_batches_per_epoch = int((total_reviews-1)/batch_size) + 1\n",
    "    for batch in range(num_batches_per_epoch):\n",
    "        print(\"gan batch: \", batch)\n",
    "        start_index = batch*batch_size\n",
    "        end_index = min((batch+1) * batch_size, total_reviews)\n",
    "        current_training_batch = train_list[start_index:end_index]\n",
    "        #min_review_length = min(len(review) for review in current_training_batch)\n",
    "        min_review_length = 300 #based on the selection criteria when extracting data.  limits learning to ~60 word context\n",
    "        #average_review_length = sum([len(review) for review in current_training_batch])/len(current_training_batch)\n",
    "        #max_steps = 2.0*average_review_length\n",
    "        max_steps = 325\n",
    "        \n",
    "        #for training_review in current_training_batch:\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)#MUST PASS IN WORDS_TO_IDS\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #can gather all outputs from this loop if you get to it\n",
    "        #get the cell state and timestep 300\n",
    "        h_artificial_300 = None\n",
    "        for i in range(max_steps):\n",
    "            if i == min_review_length:\n",
    "                h_artificial_300 = h[0][1]\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "        artificial_review_final_states = []\n",
    "        artificial_review_ids = []\n",
    "        #for row in w:\n",
    "        for a, row in enumerate(h_artificial_300):\n",
    "            #print(\"generated during training\", end=\":  \")\n",
    "            new_artificial_review = []\n",
    "            for b, word_id in enumerate(w[a]):\n",
    "                new_artificial_review.append(word_id)\n",
    "                #print(ids_to_words[word_id], end=\"\")\n",
    "                if (b != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            #print(\"\")\n",
    "            #if len(new_artificial_review) >= 0.75*average_review_length:\n",
    "            if len(new_artificial_review) >= min_review_length:\n",
    "                artificial_review_ids.append(new_artificial_review)\n",
    "                artificial_review_final_states.append(row)\n",
    "        \n",
    "        print(\"generated during training batch \", batch, \":\")\n",
    "        for review in artificial_review_ids[:25]:\n",
    "            for word_id in review:\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "            print()\n",
    "        #print(artificial_review_ids[:5])\n",
    "        #now you have current_training_batch and artificial_review_ids\n",
    "        #clip all data to the same length for simplicity\n",
    "        num_reviews = min(len(current_training_batch), len(artificial_review_ids))\n",
    "        print(\"gan batch: \", batch, \" has \", num_reviews, \n",
    "              \" real reviews and \", num_reviews, \" artificial reviews.\")\n",
    "        \n",
    "        if num_reviews == 0:\n",
    "            continue\n",
    "        \n",
    "        current_training_batch = [review[:min_review_length] for review in current_training_batch]\n",
    "        #not sure this is needed\n",
    "        artificial_review_ids = [review[:min_review_length] for review in artificial_review_ids]\n",
    "        \n",
    "        ##get final states for real reviews\n",
    "        #w_training = np.array(current_training_batch)\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w_training})\n",
    "        #feed_dict = {lm.input_w_:w_training,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: False,\n",
    "                     #lm.initial_h_:h}\n",
    "        #h_real = session.run([lm.final_h_],feed_dict=feed_dict)#no training, just get states\n",
    "        \n",
    "        #real_review_final_states = []\n",
    "        #prob don't need to do anything to h_real\n",
    "        #for row in h_real:\n",
    "            #if len(new_artificial_review) >= 300:\n",
    "                #artificial_review_ids.append(new_artificial_review)\n",
    "                #artificial_review_final_states.append(row)\n",
    "                \n",
    "        #now let's even out the number of examples\n",
    "        current_training_batch = current_training_batch[:num_reviews]\n",
    "        artificial_review_ids = artificial_review_ids[:num_reviews]\n",
    "        #print(h_real[0])\n",
    "        ##print(h_real[1])#wtf why won't this work\n",
    "        ##print(h_real[1].shape)\n",
    "        ##h_real = h_real[1][:num_reviews]\n",
    "        #h_real = h_real[:num_reviews]\n",
    "        #print(h_real)\n",
    "        #artificial_review_final_states[:num_reviews]\n",
    "        real_review_list_for_retraining_softmax = current_training_batch[:]\n",
    "        \n",
    "        #label each review and shuffle data\n",
    "        current_training_batch = [(review,[1,0]) for review in current_training_batch]#might need to swap labels\n",
    "        artificial_review_training_batch = [(np.array(review),[0,1]) for review in artificial_review_ids]\n",
    "        #print(current_training_batch[:10])\n",
    "        #print()\n",
    "        #print(artificial_review_ids[:10])\n",
    "        #print()\n",
    "        ##label each review hidden state and shuffle\n",
    "        #training_states = [(review,np.array([0,1])) for review in h_real]#might need to swap labels\n",
    "        #artificial_review_states = [(review,np.array([1,0])) for review in artificial_review_final_states]\n",
    "        \n",
    "        #combine training lists\n",
    "        current_training_batch.extend(artificial_review_training_batch)\n",
    "        np.random.shuffle(current_training_batch)\n",
    "        \n",
    "        #print(current_training_batch[:10])\n",
    "        #print()\n",
    "        #np.random.shuffle(training_states)\n",
    "        #print(training_states)\n",
    "        \n",
    "        #train discriminator\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_list, labels = zip(*current_training_batch)\n",
    "        #review_states, labels = zip(*training_states)\n",
    "        #convert to matrix form\n",
    "        #print(labels)\n",
    "        w = np.array(list(review_list))\n",
    "        #w = np.array(list(review_states))\n",
    "        #y = np.array(list(labels))\n",
    "        y = np.array(labels)\n",
    "        #print(w.shape)\n",
    "        #print(y.shape)\n",
    "        #print()\n",
    "        #batching is done at the review level\n",
    "        #the whole review is fed in at once, so initialize h first\n",
    "        #if batch == 0:\n",
    "        \n",
    "        #train CNN classifier\n",
    "        train_op = lm.train_step_\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_x:w,\n",
    "                     #lm.input_y:y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True}\n",
    "        feed_dict = {lm.input_w_: w, \n",
    "                     lm.input_y: y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True, \n",
    "                     lm.initial_h_: h}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"discriminator training accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"discriminator training cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        ##train discriminator on fake reviews\n",
    "        ##now unzip into \"w\" and \"y\"\n",
    "        #review_list, labels = zip(*artificial_review_ids)\n",
    "        #w = np.array(list(review_list))\n",
    "        #y = np.array(labels)\n",
    "        \n",
    "        ##train CNN classifier\n",
    "        #train_op = self.train_step_\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_w_: w, \n",
    "                     #lm.input_y: y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True, \n",
    "                     #lm.initial_h_: h}\n",
    "        #accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        #print(\"fake review accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        #print(\"fake review cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #train generator\n",
    "        #relabel fake reviews as real\n",
    "        artificial_review_training_batch = [(np.array(review),[1,0]) for review in artificial_review_ids]\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_list, labels = zip(*artificial_review_training_batch)\n",
    "        w = np.array(list(review_list))\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        #train CNN classifier\n",
    "        train_op = lm.train_step1_\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_x:w,\n",
    "                     #lm.input_y:y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True}\n",
    "        feed_dict = {lm.input_w_: w, \n",
    "                     lm.input_y: y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True, \n",
    "                     lm.initial_h_: h}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"generator training accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"generator training cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #retrain softmax of rnn on real reviews\n",
    "        flattened_real_ids = np.array([item for sublist in real_review_list_for_retraining_softmax for item in sublist])\n",
    "        #print(flattened_real_ids[:10])\n",
    "        #print(flattened_real_ids.shape)\n",
    "        #print()\n",
    "        bi = utils.rnnlm_batch_generator(flattened_real_ids, batch_size, max_time=150)\n",
    "        for i, (w, y) in enumerate(bi):\n",
    "            cost = 0.0\n",
    "            if i == 0:\n",
    "                h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            feed_dict = {lm.input_w_: w, \n",
    "                         lm.target_y_: y, \n",
    "                         lm.learning_rate_: learning_rate,\n",
    "                         lm.use_dropout_: False,\n",
    "                         lm.initial_h_:h}\n",
    "            cost, h, _ = session.run([lm.train_loss_, lm.final_h_, lm.train_step_softmax_],feed_dict=feed_dict)\n",
    "            print(\"softmax re-training cost for gan batch \", batch, \" is: \", cost)\n",
    "    \n",
    "    ### UPDATED!!!\n",
    "    #total_cost += cost#update\n",
    "    #total_batches = batch + 1\n",
    "    #total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ###\n",
    "    ## Print average loss-so-far for epoch\n",
    "    ## If using train_loss_, this may be an underestimate.\n",
    "    #if verbose and (time.time() - tick_time >= tick_s):\n",
    "        #avg_cost = total_cost / total_batches\n",
    "        #avg_wps = total_words / (time.time() - start_time)\n",
    "        #print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "            #batch, total_words, avg_wps, avg_cost))\n",
    "        #tick_time = time.time()  # reset time ticker\n",
    "    if total_batches == 0:\n",
    "        total_batches = 1\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        if len(semifinal_review) > 300:\n",
    "            final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "            #print(final_review)\n",
    "            review_list.append(final_review)\n",
    "    return review_list\n",
    "\n",
    "def get_review_series(review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    review_df = pd.read_csv(review_path)\n",
    "    five_star_review_df = review_df[review_df['stars']==5]\n",
    "    #five_star_review_series = five_star_review_df['text']\n",
    "    return five_star_review_df['text']\n",
    "\n",
    "def get_business_list(business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'):\n",
    "    #business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'\n",
    "    return pd.read_csv(business_path)\n",
    "\n",
    "def split_train_test(review_list, training_samples, test_samples):\n",
    "    #pass in randomized review list\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return randomized_training_list, randomized_testing_list\n",
    "\n",
    "def make_train_test_data(five_star_review_series, training_samples=20000, test_samples=1000):\n",
    "    #fix randomization to prevent evaluation on trained samples\n",
    "    review_list = preprocess_review_series(five_star_review_series)\n",
    "    #split and shuffle the data\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    np.random.shuffle(review_list)\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    #training_review_list = [item for sublist in review_list[:training_samples] for item in sublist]\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    \n",
    "    #test_review_list = [item for sublist in review_list[training_samples:training_samples+test_samples] for item in sublist]\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return training_review_list, test_review_list\n",
    "\n",
    "\n",
    "#def make_vocabulary(training_review_list, test_review_list):\n",
    "#    unique_characters = list(set(training_review_list + test_review_list))\n",
    "#    #vocabulary\n",
    "#    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "#    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#    return char_dict, ids_to_words\n",
    "def make_vocabulary(dataset_list):\n",
    "    unique_characters = list(set().union(*dataset_list))\n",
    "    #unique_characters = list(set(training_review_list + test_review_list))\n",
    "    #vocabulary\n",
    "    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "    return char_dict, ids_to_words\n",
    "\n",
    "def convert_to_ids(char_dict, review_list):\n",
    "    #convert to flat (1D) np.array(int) of ids\n",
    "    review_ids = [char_dict.get(token) for token in review_list]\n",
    "    return np.array(review_ids)\n",
    "\n",
    "def run_training(train_list, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20):\n",
    "    #V = len(words_to_ids.keys())\n",
    "    # Training parameters\n",
    "    ## add parameter sets for each attack/defense configuration\n",
    "    #max_time = 25\n",
    "    #batch_size = 100\n",
    "    #learning_rate = 0.01\n",
    "    #num_epochs = 10\n",
    "    \n",
    "    # Model parameters\n",
    "    #model_params = dict(V=vocab.size, \n",
    "                        #H=200, \n",
    "                        #softmax_ns=200,\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=len(words_to_ids.keys()), \n",
    "                        #H=1024, \n",
    "                        #softmax_ns=len(words_to_ids.keys()),\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=V, H=H, softmax_ns=softmax_ns, num_layers=num_layers)\n",
    "    \n",
    "    #TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "    TF_SAVEDIR = tf_savedir\n",
    "    checkpoint_filename = os.path.join(TF_SAVEDIR, \"gan\")\n",
    "    trained_filename = os.path.join(TF_SAVEDIR, \"gan_trained\")\n",
    "    \n",
    "    # Will print status every this many seconds\n",
    "    #print_interval = 5\n",
    "    print_interval = 30\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    ### UPDATED!!!\n",
    "    lm.BuildSamplerGraph()\n",
    "    num_pretrain = 20000\n",
    "    #num_pretrain = 200#low number for testing\n",
    "    #pretrain on a different set of training data each time\n",
    "    #train and test ids \n",
    "    #pre_train_ids =\n",
    "    #train_ids = \n",
    "    ### UPDATED!!!\n",
    "    \n",
    "    # Explicitly add global initializer and variable saver to LM graph\n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Clear old log directory\n",
    "    shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "    if not os.path.isdir(TF_SAVEDIR):\n",
    "        os.makedirs(TF_SAVEDIR)\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(42)\n",
    "    \n",
    "        session.run(initializer)\n",
    "        \n",
    "        #check trainable variables\n",
    "        #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "        #values = session.run(variables_names)\n",
    "        #for k, v in zip(variables_names, values):\n",
    "            #print(\"Variable: \", k)\n",
    "            #print(\"Shape: \", v.shape)\n",
    "            #print(v)\n",
    "    \n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            np.random.shuffle(train_list)\n",
    "            #shuffled_train_list = np.random.shuffle(train_list)\n",
    "            #pre_train_ids = [item for sublist in shuffled_train_list[:num_pretrain] for item in sublist]\n",
    "            #gan_train_list = shuffled_train_list[num_pretrain:]\n",
    "            #train_list = \n",
    "            pre_train_ids = np.array([item for sublist in train_list[:num_pretrain] for item in sublist])\n",
    "            #print(pre_train_ids.shape)\n",
    "            #pre_train_ids = np.array(pre_train_ids)\n",
    "            gan_train_list = train_list[num_pretrain:]\n",
    "            if num_pretrain > 0:\n",
    "                bi = utils.rnnlm_batch_generator(pre_train_ids, batch_size, max_time)\n",
    "                print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "                # Run a pretraining epoch.\n",
    "                run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "                print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "        \n",
    "            #now train gan\n",
    "            #run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, verbose=False, tick_s=10, learning_rate=None)\n",
    "            run_gan_epoch(lm, session, words_to_ids, ids_to_words, gan_train_list, batch_size, \n",
    "                          verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "        \n",
    "            # Save a checkpoint\n",
    "            saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "        \n",
    "            ##\n",
    "            # score_dataset will run a forward pass over the entire dataset\n",
    "            # and report perplexity scores. This can be slow (around 1/2 to \n",
    "            # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "            # to speed up training on a slow machine. Be sure to run it at the \n",
    "            # end to evaluate your score.\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, pre_train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "        return trained_filename\n",
    "\n",
    "def get_char_probs(trained_filename, model_params, test_ids):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    all_review_likelihoods = []\n",
    "    train_op = tf.no_op()\n",
    "    use_dropout = False\n",
    "    loss = lm.loss_\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False\n",
    "        #loss = lm.loss_\n",
    "        \n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        for review in test_ids:\n",
    "            review_likelihoods = []\n",
    "            inputs = review[:-1]\n",
    "            labels = review[1:]\n",
    "            inputs_labels = zip(inputs,labels)\n",
    "            for i, (w,y) in enumerate(inputs_labels):\n",
    "                \n",
    "                w = np.array(w)\n",
    "                y = np.array(y)\n",
    "                w = w.reshape([1,1])\n",
    "                y = y.reshape([1,1])\n",
    "                \n",
    "                if i == 0:\n",
    "                    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "                feed_dict = {lm.input_w_:w, \n",
    "                             lm.target_y_:y,\n",
    "                             lm.learning_rate_: 0.002,\n",
    "                             lm.use_dropout_: use_dropout,\n",
    "                             lm.initial_h_:h}\n",
    "                cost, h = session.run([loss, lm.final_h_],feed_dict=feed_dict)\n",
    "                likelihood = 2**(-1*cost)\n",
    "                review_likelihoods.append(likelihood)\n",
    "            all_review_likelihoods.append(review_likelihoods)\n",
    "    return all_review_likelihoods\n",
    "\n",
    "## Sampling\n",
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "\n",
    "    # Run sample ops\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]\n",
    "\n",
    "def generate_text(trained_filename, model_params, words_to_ids, ids_to_words):\n",
    "    # Same as above, but as a batch\n",
    "    #max_steps = 20\n",
    "    max_steps = 300\n",
    "    #num_samples = 40000\n",
    "    num_samples = 40\n",
    "    random_seed = 42\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(random_seed)\n",
    "        \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "    \n",
    "        # Make initial state for a batch with batch_size = num_samples\n",
    "        #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        # take one step for each sequence on each iteration \n",
    "        for i in range(max_steps):\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "    \n",
    "        # Print generated sentences\n",
    "        for row in w:\n",
    "            print(trained_filename, end=\":  \")\n",
    "            for i, word_id in enumerate(row):\n",
    "                #print(vocab.id_to_word[word_id], end=\" \")\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                #if (i != 0) and (word_id == vocab.START_ID):\n",
    "                if (i != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")\n",
    "\n",
    "def train_attack_model(training_samples=20000, test_samples=1000, review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #training_samples=20000\n",
    "    #test_samples=1000\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    start_format = time.time()\n",
    "    five_star_reviews = get_review_series(review_path)\n",
    "    train_review_list, test_review_list = make_train_test_data(five_star_reviews, training_samples, test_samples)\n",
    "    words_to_ids, ids_to_words = make_vocabulary(train_review_list, test_review_list)\n",
    "    train_ids = convert_to_ids(words_to_ids, train_review_list)\n",
    "    test_ids = convert_to_ids(words_to_ids, test_review_list)\n",
    "    end_format = time.time()\n",
    "    print(\"data formatting took \" + str(end_format-start_format) + \" seconds\")\n",
    "    model_params = dict(V=len(words_to_ids.keys()), \n",
    "                            H=1024, \n",
    "                            softmax_ns=len(words_to_ids.keys()),\n",
    "                            num_layers=2)\n",
    "    #run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=150, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    return trained_filename, model_params, words_to_ids, ids_to_words\n",
    "\n",
    "def neg_log_lik_ratio(likelihoods_real, likelihoods_artificial):\n",
    "    predictions = []\n",
    "    combined = zip(likelihoods_real, likelihoods_artificial)\n",
    "    for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "        negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "        #averaged_llrs = negative_log_lik_ratios[:-1]/(len(negative_log_lik_ratios)-1)\n",
    "        averaged_llrs = np.sum(negative_log_lik_ratios[:-1])/(len(negative_log_lik_ratios)-1)\n",
    "        predictions.append(averaged_llrs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data download took 4.29355525970459 seconds\n"
     ]
    }
   ],
   "source": [
    "start_dl = time.time()\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_train_data_03.csv .')\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_test_data_03.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_train_data_01.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_test_data_01.csv .')\n",
    "end_dl = time.time()\n",
    "print(\"data download took \" + str(end_dl-start_dl) + \" seconds\")\n",
    "#gsutil cp gs://[BUCKET_NAME]/[OBJECT_NAME] [OBJECT_DESTINATION]\n",
    "real_train_review_path = './split01_train_data_02.csv'\n",
    "real_test_review_path = './split01_test_data_02.csv'\n",
    "#artificial_train_review_path = './gen01_train_data_01.csv'\n",
    "#artificial_test_review_path = './gen01_test_data_01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_train_review_path = '/home/kalvin_kao/final_project/split01_train_data_02.csv'\n",
    "real_test_review_path = '/home/kalvin_kao/final_project/split01_test_data_02.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading took 1.911911964416504 seconds\n"
     ]
    }
   ],
   "source": [
    "start_open = time.time()\n",
    "with open(real_train_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    training_review_list_real = [sublist for sublist in reader]\n",
    "training_review_list_real_training_eval = [item for sublist in training_review_list_real for item in sublist]\n",
    "\n",
    "with open(real_test_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    test_review_list_real = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "test_review_list_real_training_eval = [item for sublist in test_review_list_real for item in sublist]\n",
    "\n",
    "#with open(artificial_train_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #training_review_list_artificial = [item for sublist in reader for item in sublist]\n",
    "\n",
    "#with open(artificial_test_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #test_review_list_artificial = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "#test_review_list_artificial_training_eval = [item for sublist in test_review_list_artificial for item in sublist]\n",
    "end_open = time.time()\n",
    "print(\"data reading took \" + str(end_open-start_open) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary building took 6.058686256408691 seconds\n"
     ]
    }
   ],
   "source": [
    "start_vocab = time.time()\n",
    "#words_to_ids, ids_to_words = make_vocabulary([training_review_list_real, test_review_list_real_training_eval, training_review_list_artificial, test_review_list_artificial_training_eval])\n",
    "words_to_ids, ids_to_words = make_vocabulary([training_review_list_real_training_eval, test_review_list_real_training_eval])\n",
    "train_ids_real = [convert_to_ids(words_to_ids, review) for review in training_review_list_real]\n",
    "train_ids_real_training_eval = convert_to_ids(words_to_ids, training_review_list_real_training_eval)\n",
    "test_ids_real = [convert_to_ids(words_to_ids, review) for review in test_review_list_real]\n",
    "test_ids_real_training_eval = convert_to_ids(words_to_ids, test_review_list_real_training_eval)\n",
    "#train_ids_artificial = convert_to_ids(words_to_ids, training_review_list_artificial)\n",
    "#test_ids_artificial = [convert_to_ids(words_to_ids, review) for review in test_review_list_artificial]\n",
    "#test_ids_artificial_training_eval = convert_to_ids(words_to_ids, test_review_list_artificial_training_eval)\n",
    "end_vocab = time.time()\n",
    "print(\"vocabulary building took \" + str(end_vocab-start_vocab) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.random.shuffle(train_ids_real[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trainer.rnnlm' from '/home/kalvin_kao/artificial_hotel_reviews/model_dev/gan/trainer/rnnlm.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/artificial_hotel_reviews/model_dev/gan/trainer/rnnlm.py:352: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 1]: seen 19200 words at 1895.5 wps, loss = 9.397\n",
      "[batch 4]: seen 48000 words at 2349.9 wps, loss = 9.932\n",
      "[batch 7]: seen 76800 words at 2478.1 wps, loss = 7.781\n",
      "[batch 10]: seen 105600 words at 2566.0 wps, loss = 6.511\n",
      "[batch 13]: seen 134400 words at 2601.0 wps, loss = 5.706\n",
      "[batch 16]: seen 163200 words at 2638.7 wps, loss = 5.161\n",
      "[batch 19]: seen 192000 words at 2653.1 wps, loss = 4.767\n",
      "[batch 22]: seen 220800 words at 2675.8 wps, loss = 4.468\n",
      "[batch 25]: seen 249600 words at 2684.1 wps, loss = 4.231\n",
      "[batch 29]: seen 288000 words at 2705.0 wps, loss = 3.985\n",
      "[batch 32]: seen 316800 words at 2713.9 wps, loss = 3.833\n",
      "[batch 35]: seen 345600 words at 2724.3 wps, loss = 3.706\n",
      "[batch 38]: seen 374400 words at 2728.8 wps, loss = 3.598\n",
      "[batch 41]: seen 403200 words at 2735.9 wps, loss = 3.502\n",
      "[batch 44]: seen 432000 words at 2738.5 wps, loss = 3.417\n",
      "[batch 47]: seen 460800 words at 2744.0 wps, loss = 3.342\n",
      "[batch 50]: seen 489600 words at 2747.8 wps, loss = 3.274\n",
      "[batch 53]: seen 518400 words at 2753.2 wps, loss = 3.212\n",
      "[batch 56]: seen 547200 words at 2754.9 wps, loss = 3.157\n",
      "[batch 59]: seen 576000 words at 2758.9 wps, loss = 3.106\n",
      "[batch 62]: seen 604800 words at 2759.9 wps, loss = 3.058\n",
      "[batch 65]: seen 633600 words at 2762.2 wps, loss = 3.016\n",
      "[batch 68]: seen 662400 words at 2763.3 wps, loss = 2.977\n",
      "[batch 71]: seen 691200 words at 2767.2 wps, loss = 2.941\n",
      "[batch 74]: seen 720000 words at 2768.3 wps, loss = 2.905\n",
      "[batch 77]: seen 748800 words at 2771.6 wps, loss = 2.872\n",
      "[batch 80]: seen 777600 words at 2773.1 wps, loss = 2.842\n",
      "[batch 83]: seen 806400 words at 2775.6 wps, loss = 2.814\n",
      "[batch 86]: seen 835200 words at 2775.5 wps, loss = 2.787\n",
      "[batch 89]: seen 864000 words at 2777.6 wps, loss = 2.762\n",
      "[batch 92]: seen 892800 words at 2777.3 wps, loss = 2.738\n",
      "[batch 95]: seen 921600 words at 2778.5 wps, loss = 2.714\n",
      "[batch 98]: seen 950400 words at 2779.2 wps, loss = 2.692\n",
      "[batch 101]: seen 979200 words at 2781.9 wps, loss = 2.671\n",
      "[batch 104]: seen 1008000 words at 2781.9 wps, loss = 2.651\n",
      "[batch 107]: seen 1036800 words at 2783.7 wps, loss = 2.630\n",
      "[batch 110]: seen 1065600 words at 2783.9 wps, loss = 2.612\n",
      "[batch 113]: seen 1094400 words at 2785.8 wps, loss = 2.595\n",
      "[batch 116]: seen 1123200 words at 2786.4 wps, loss = 2.578\n",
      "[batch 119]: seen 1152000 words at 2788.1 wps, loss = 2.562\n",
      "[batch 122]: seen 1180800 words at 2788.3 wps, loss = 2.547\n",
      "[batch 125]: seen 1209600 words at 2789.7 wps, loss = 2.531\n",
      "[batch 128]: seen 1238400 words at 2789.5 wps, loss = 2.517\n",
      "[batch 131]: seen 1267200 words at 2790.3 wps, loss = 2.504\n",
      "[batch 134]: seen 1296000 words at 2790.5 wps, loss = 2.491\n",
      "[batch 137]: seen 1324800 words at 2791.8 wps, loss = 2.478\n",
      "[batch 140]: seen 1353600 words at 2791.9 wps, loss = 2.467\n",
      "[batch 143]: seen 1382400 words at 2793.4 wps, loss = 2.454\n",
      "[batch 146]: seen 1411200 words at 2793.5 wps, loss = 2.444\n",
      "[batch 149]: seen 1440000 words at 2794.7 wps, loss = 2.432\n",
      "[batch 152]: seen 1468800 words at 2794.3 wps, loss = 2.421\n",
      "[batch 155]: seen 1497600 words at 2795.1 wps, loss = 2.411\n",
      "[batch 158]: seen 1526400 words at 2795.1 wps, loss = 2.401\n",
      "[batch 161]: seen 1555200 words at 2795.9 wps, loss = 2.390\n",
      "[batch 164]: seen 1584000 words at 2795.4 wps, loss = 2.381\n",
      "[batch 167]: seen 1612800 words at 2796.8 wps, loss = 2.372\n",
      "[batch 170]: seen 1641600 words at 2797.0 wps, loss = 2.363\n",
      "[batch 173]: seen 1670400 words at 2798.2 wps, loss = 2.354\n",
      "[batch 176]: seen 1699200 words at 2798.2 wps, loss = 2.345\n",
      "[batch 179]: seen 1728000 words at 2798.8 wps, loss = 2.336\n",
      "[batch 182]: seen 1756800 words at 2798.6 wps, loss = 2.327\n",
      "[batch 185]: seen 1785600 words at 2799.5 wps, loss = 2.319\n",
      "[batch 188]: seen 1814400 words at 2799.5 wps, loss = 2.312\n",
      "[batch 191]: seen 1843200 words at 2799.6 wps, loss = 2.304\n",
      "[batch 194]: seen 1872000 words at 2799.5 wps, loss = 2.297\n",
      "[batch 197]: seen 1900800 words at 2800.1 wps, loss = 2.289\n",
      "[batch 200]: seen 1929600 words at 2800.2 wps, loss = 2.281\n",
      "[batch 203]: seen 1958400 words at 2801.0 wps, loss = 2.274\n",
      "[batch 206]: seen 1987200 words at 2800.8 wps, loss = 2.267\n",
      "[batch 209]: seen 2016000 words at 2801.3 wps, loss = 2.259\n",
      "[batch 212]: seen 2044800 words at 2801.3 wps, loss = 2.252\n",
      "[batch 215]: seen 2073600 words at 2801.8 wps, loss = 2.246\n",
      "[batch 218]: seen 2102400 words at 2801.7 wps, loss = 2.239\n",
      "[batch 221]: seen 2131200 words at 2801.9 wps, loss = 2.232\n",
      "[batch 224]: seen 2160000 words at 2802.0 wps, loss = 2.227\n",
      "[batch 227]: seen 2188800 words at 2802.8 wps, loss = 2.220\n",
      "[batch 230]: seen 2217600 words at 2802.9 wps, loss = 2.214\n",
      "[batch 233]: seen 2246400 words at 2803.5 wps, loss = 2.208\n",
      "[batch 236]: seen 2275200 words at 2803.3 wps, loss = 2.202\n",
      "[batch 239]: seen 2304000 words at 2804.0 wps, loss = 2.196\n",
      "[batch 242]: seen 2332800 words at 2804.1 wps, loss = 2.191\n",
      "[batch 245]: seen 2361600 words at 2804.7 wps, loss = 2.185\n",
      "[batch 248]: seen 2390400 words at 2804.5 wps, loss = 2.180\n",
      "[batch 251]: seen 2419200 words at 2805.1 wps, loss = 2.174\n",
      "[batch 254]: seen 2448000 words at 2805.1 wps, loss = 2.169\n",
      "[batch 257]: seen 2476800 words at 2805.2 wps, loss = 2.163\n",
      "[batch 260]: seen 2505600 words at 2805.3 wps, loss = 2.158\n",
      "[batch 263]: seen 2534400 words at 2805.8 wps, loss = 2.154\n",
      "[batch 266]: seen 2563200 words at 2805.9 wps, loss = 2.149\n",
      "[batch 269]: seen 2592000 words at 2806.2 wps, loss = 2.144\n",
      "[batch 272]: seen 2620800 words at 2806.3 wps, loss = 2.140\n",
      "[batch 275]: seen 2649600 words at 2807.0 wps, loss = 2.135\n",
      "[batch 278]: seen 2678400 words at 2807.0 wps, loss = 2.131\n",
      "[batch 281]: seen 2707200 words at 2807.6 wps, loss = 2.126\n",
      "[batch 284]: seen 2736000 words at 2807.5 wps, loss = 2.122\n",
      "[batch 287]: seen 2764800 words at 2808.1 wps, loss = 2.117\n",
      "[batch 290]: seen 2793600 words at 2807.9 wps, loss = 2.113\n",
      "[batch 293]: seen 2822400 words at 2808.6 wps, loss = 2.108\n",
      "[batch 296]: seen 2851200 words at 2808.7 wps, loss = 2.104\n",
      "[batch 299]: seen 2880000 words at 2809.2 wps, loss = 2.100\n",
      "[batch 302]: seen 2908800 words at 2809.2 wps, loss = 2.096\n",
      "[batch 305]: seen 2937600 words at 2809.5 wps, loss = 2.092\n",
      "[batch 308]: seen 2966400 words at 2809.5 wps, loss = 2.088\n",
      "[batch 311]: seen 2995200 words at 2810.1 wps, loss = 2.084\n",
      "[batch 314]: seen 3024000 words at 2810.2 wps, loss = 2.080\n",
      "[batch 317]: seen 3052800 words at 2810.6 wps, loss = 2.076\n",
      "[batch 320]: seen 3081600 words at 2810.7 wps, loss = 2.072\n",
      "[batch 324]: seen 3120000 words at 2811.2 wps, loss = 2.068\n",
      "[batch 327]: seen 3148800 words at 2811.2 wps, loss = 2.065\n",
      "[batch 330]: seen 3177600 words at 2811.4 wps, loss = 2.061\n",
      "[batch 333]: seen 3206400 words at 2811.5 wps, loss = 2.058\n",
      "[batch 336]: seen 3235200 words at 2811.8 wps, loss = 2.054\n",
      "[batch 339]: seen 3264000 words at 2812.3 wps, loss = 2.050\n",
      "[batch 342]: seen 3292800 words at 2812.3 wps, loss = 2.047\n",
      "[batch 345]: seen 3321600 words at 2812.7 wps, loss = 2.044\n",
      "[batch 348]: seen 3350400 words at 2812.9 wps, loss = 2.040\n",
      "[batch 351]: seen 3379200 words at 2813.3 wps, loss = 2.036\n",
      "[batch 354]: seen 3408000 words at 2813.7 wps, loss = 2.033\n",
      "[batch 357]: seen 3436800 words at 2814.1 wps, loss = 2.029\n",
      "[batch 360]: seen 3465600 words at 2814.2 wps, loss = 2.026\n",
      "[batch 363]: seen 3494400 words at 2814.3 wps, loss = 2.023\n",
      "[batch 366]: seen 3523200 words at 2814.6 wps, loss = 2.020\n",
      "[batch 369]: seen 3552000 words at 2814.7 wps, loss = 2.017\n",
      "[batch 372]: seen 3580800 words at 2815.1 wps, loss = 2.014\n",
      "[batch 375]: seen 3609600 words at 2815.0 wps, loss = 2.012\n",
      "[batch 378]: seen 3638400 words at 2815.4 wps, loss = 2.009\n",
      "[batch 381]: seen 3667200 words at 2815.5 wps, loss = 2.006\n",
      "[batch 384]: seen 3696000 words at 2815.8 wps, loss = 2.003\n",
      "[batch 387]: seen 3724800 words at 2816.0 wps, loss = 2.000\n",
      "[batch 390]: seen 3753600 words at 2816.3 wps, loss = 1.997\n",
      "[batch 393]: seen 3782400 words at 2816.4 wps, loss = 1.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 396]: seen 3811200 words at 2816.8 wps, loss = 1.992\n",
      "[batch 399]: seen 3840000 words at 2816.7 wps, loss = 1.989\n",
      "[batch 402]: seen 3868800 words at 2817.1 wps, loss = 1.987\n",
      "[batch 405]: seen 3897600 words at 2817.3 wps, loss = 1.984\n",
      "[batch 409]: seen 3936000 words at 2817.7 wps, loss = 1.981\n",
      "[batch 412]: seen 3964800 words at 2817.9 wps, loss = 1.978\n",
      "[batch 415]: seen 3993600 words at 2818.0 wps, loss = 1.976\n",
      "[batch 419]: seen 4032000 words at 2818.5 wps, loss = 1.972\n",
      "[batch 422]: seen 4060800 words at 2818.5 wps, loss = 1.970\n",
      "[batch 426]: seen 4099200 words at 2818.8 wps, loss = 1.966\n",
      "[batch 429]: seen 4128000 words at 2818.8 wps, loss = 1.964\n",
      "[batch 433]: seen 4166400 words at 2819.1 wps, loss = 1.961\n",
      "[batch 436]: seen 4195200 words at 2819.5 wps, loss = 1.958\n",
      "[batch 439]: seen 4224000 words at 2819.3 wps, loss = 1.956\n",
      "[batch 442]: seen 4252800 words at 2819.6 wps, loss = 1.953\n",
      "[batch 445]: seen 4281600 words at 2819.7 wps, loss = 1.951\n",
      "[batch 448]: seen 4310400 words at 2820.0 wps, loss = 1.949\n",
      "[batch 451]: seen 4339200 words at 2820.1 wps, loss = 1.946\n",
      "[batch 454]: seen 4368000 words at 2820.5 wps, loss = 1.944\n",
      "[batch 457]: seen 4396800 words at 2820.6 wps, loss = 1.942\n",
      "[batch 460]: seen 4425600 words at 2820.7 wps, loss = 1.940\n",
      "[batch 463]: seen 4454400 words at 2820.9 wps, loss = 1.938\n",
      "[batch 466]: seen 4483200 words at 2821.1 wps, loss = 1.935\n",
      "[batch 469]: seen 4512000 words at 2821.2 wps, loss = 1.933\n",
      "[batch 472]: seen 4540800 words at 2821.5 wps, loss = 1.931\n",
      "[batch 475]: seen 4569600 words at 2821.6 wps, loss = 1.929\n",
      "[batch 478]: seen 4598400 words at 2821.8 wps, loss = 1.926\n",
      "[batch 481]: seen 4627200 words at 2821.9 wps, loss = 1.924\n",
      "[batch 484]: seen 4656000 words at 2822.1 wps, loss = 1.922\n",
      "[batch 487]: seen 4684800 words at 2822.2 wps, loss = 1.920\n",
      "[batch 490]: seen 4713600 words at 2822.4 wps, loss = 1.918\n",
      "[batch 493]: seen 4742400 words at 2822.4 wps, loss = 1.916\n",
      "[batch 496]: seen 4771200 words at 2822.7 wps, loss = 1.914\n",
      "[batch 499]: seen 4800000 words at 2822.8 wps, loss = 1.912\n",
      "[batch 502]: seen 4828800 words at 2823.0 wps, loss = 1.909\n",
      "[batch 505]: seen 4857600 words at 2823.3 wps, loss = 1.907\n",
      "[batch 508]: seen 4886400 words at 2823.3 wps, loss = 1.905\n",
      "[batch 511]: seen 4915200 words at 2823.4 wps, loss = 1.903\n",
      "[batch 514]: seen 4944000 words at 2823.6 wps, loss = 1.901\n",
      "[batch 517]: seen 4972800 words at 2823.8 wps, loss = 1.899\n",
      "[batch 520]: seen 5001600 words at 2824.0 wps, loss = 1.897\n",
      "[batch 523]: seen 5030400 words at 2824.2 wps, loss = 1.895\n",
      "[batch 526]: seen 5059200 words at 2824.3 wps, loss = 1.893\n",
      "[batch 529]: seen 5088000 words at 2824.5 wps, loss = 1.891\n",
      "[batch 532]: seen 5116800 words at 2824.6 wps, loss = 1.889\n",
      "[batch 536]: seen 5155200 words at 2824.9 wps, loss = 1.887\n",
      "[batch 539]: seen 5184000 words at 2825.1 wps, loss = 1.885\n",
      "[batch 542]: seen 5212800 words at 2825.0 wps, loss = 1.883\n",
      "[batch 545]: seen 5241600 words at 2825.3 wps, loss = 1.881\n",
      "[batch 548]: seen 5270400 words at 2825.4 wps, loss = 1.879\n",
      "[batch 551]: seen 5299200 words at 2825.7 wps, loss = 1.877\n",
      "[batch 554]: seen 5328000 words at 2825.6 wps, loss = 1.875\n",
      "[batch 558]: seen 5366400 words at 2826.0 wps, loss = 1.873\n",
      "[batch 561]: seen 5395200 words at 2825.9 wps, loss = 1.871\n",
      "[batch 565]: seen 5433600 words at 2826.2 wps, loss = 1.869\n",
      "[batch 568]: seen 5462400 words at 2826.3 wps, loss = 1.867\n",
      "[batch 571]: seen 5491200 words at 2826.5 wps, loss = 1.866\n",
      "[batch 574]: seen 5520000 words at 2826.4 wps, loss = 1.864\n",
      "[batch 577]: seen 5548800 words at 2826.6 wps, loss = 1.862\n",
      "[batch 580]: seen 5577600 words at 2826.6 wps, loss = 1.861\n",
      "[batch 583]: seen 5606400 words at 2826.8 wps, loss = 1.859\n",
      "[batch 586]: seen 5635200 words at 2826.8 wps, loss = 1.858\n",
      "[batch 589]: seen 5664000 words at 2827.0 wps, loss = 1.856\n",
      "[batch 592]: seen 5692800 words at 2827.0 wps, loss = 1.855\n",
      "[batch 595]: seen 5721600 words at 2827.2 wps, loss = 1.853\n",
      "[batch 598]: seen 5750400 words at 2827.2 wps, loss = 1.852\n",
      "[batch 601]: seen 5779200 words at 2827.4 wps, loss = 1.850\n",
      "[batch 604]: seen 5808000 words at 2827.3 wps, loss = 1.849\n",
      "[batch 607]: seen 5836800 words at 2827.5 wps, loss = 1.847\n",
      "[batch 610]: seen 5865600 words at 2827.4 wps, loss = 1.846\n",
      "[batch 613]: seen 5894400 words at 2827.5 wps, loss = 1.844\n",
      "[batch 616]: seen 5923200 words at 2827.5 wps, loss = 1.842\n",
      "[batch 619]: seen 5952000 words at 2827.6 wps, loss = 1.841\n",
      "[batch 622]: seen 5980800 words at 2827.5 wps, loss = 1.839\n",
      "[batch 625]: seen 6009600 words at 2827.6 wps, loss = 1.838\n",
      "[batch 628]: seen 6038400 words at 2827.6 wps, loss = 1.836\n",
      "[batch 631]: seen 6067200 words at 2827.6 wps, loss = 1.835\n",
      "[batch 634]: seen 6096000 words at 2827.6 wps, loss = 1.833\n",
      "[batch 637]: seen 6124800 words at 2827.8 wps, loss = 1.832\n",
      "[batch 640]: seen 6153600 words at 2827.6 wps, loss = 1.830\n",
      "[batch 643]: seen 6182400 words at 2827.8 wps, loss = 1.829\n",
      "[batch 646]: seen 6211200 words at 2827.6 wps, loss = 1.827\n",
      "[batch 649]: seen 6240000 words at 2827.9 wps, loss = 1.826\n",
      "[batch 652]: seen 6268800 words at 2827.9 wps, loss = 1.824\n",
      "[batch 656]: seen 6307200 words at 2828.1 wps, loss = 1.822\n",
      "[batch 659]: seen 6336000 words at 2828.1 wps, loss = 1.821\n",
      "[batch 662]: seen 6364800 words at 2828.2 wps, loss = 1.819\n",
      "[batch 665]: seen 6393600 words at 2828.2 wps, loss = 1.818\n",
      "[batch 668]: seen 6422400 words at 2828.3 wps, loss = 1.816\n",
      "[batch 671]: seen 6451200 words at 2828.2 wps, loss = 1.815\n",
      "[batch 674]: seen 6480000 words at 2828.2 wps, loss = 1.814\n",
      "[batch 677]: seen 6508800 words at 2828.3 wps, loss = 1.813\n",
      "[batch 680]: seen 6537600 words at 2828.4 wps, loss = 1.811\n",
      "[batch 683]: seen 6566400 words at 2828.4 wps, loss = 1.810\n",
      "[batch 686]: seen 6595200 words at 2828.6 wps, loss = 1.809\n",
      "[batch 689]: seen 6624000 words at 2828.6 wps, loss = 1.807\n",
      "[batch 692]: seen 6652800 words at 2828.7 wps, loss = 1.806\n",
      "[batch 695]: seen 6681600 words at 2828.7 wps, loss = 1.805\n",
      "[batch 698]: seen 6710400 words at 2828.7 wps, loss = 1.803\n",
      "[batch 701]: seen 6739200 words at 2828.8 wps, loss = 1.802\n",
      "[batch 704]: seen 6768000 words at 2828.8 wps, loss = 1.801\n",
      "[batch 707]: seen 6796800 words at 2828.9 wps, loss = 1.800\n",
      "[batch 710]: seen 6825600 words at 2829.0 wps, loss = 1.799\n",
      "[batch 713]: seen 6854400 words at 2829.0 wps, loss = 1.797\n",
      "[batch 716]: seen 6883200 words at 2829.1 wps, loss = 1.796\n",
      "[batch 719]: seen 6912000 words at 2829.0 wps, loss = 1.795\n",
      "[batch 722]: seen 6940800 words at 2829.2 wps, loss = 1.794\n",
      "[batch 725]: seen 6969600 words at 2829.3 wps, loss = 1.792\n",
      "[batch 728]: seen 6998400 words at 2829.3 wps, loss = 1.791\n",
      "[batch 731]: seen 7027200 words at 2829.4 wps, loss = 1.790\n",
      "[batch 734]: seen 7056000 words at 2829.6 wps, loss = 1.789\n",
      "[batch 737]: seen 7084800 words at 2829.6 wps, loss = 1.788\n",
      "[batch 740]: seen 7113600 words at 2829.7 wps, loss = 1.786\n",
      "[batch 743]: seen 7142400 words at 2829.7 wps, loss = 1.785\n",
      "[batch 747]: seen 7180800 words at 2829.8 wps, loss = 1.783\n",
      "[batch 750]: seen 7209600 words at 2829.9 wps, loss = 1.782\n",
      "[batch 753]: seen 7238400 words at 2829.8 wps, loss = 1.781\n",
      "[batch 756]: seen 7267200 words at 2829.9 wps, loss = 1.780\n",
      "[batch 759]: seen 7296000 words at 2829.9 wps, loss = 1.778\n",
      "[batch 762]: seen 7324800 words at 2830.1 wps, loss = 1.777\n",
      "[batch 765]: seen 7353600 words at 2829.9 wps, loss = 1.776\n",
      "[batch 768]: seen 7382400 words at 2830.1 wps, loss = 1.775\n",
      "[batch 771]: seen 7411200 words at 2830.2 wps, loss = 1.774\n",
      "[batch 775]: seen 7449600 words at 2830.5 wps, loss = 1.772\n",
      "[batch 778]: seen 7478400 words at 2830.4 wps, loss = 1.771\n",
      "[batch 782]: seen 7516800 words at 2830.6 wps, loss = 1.770\n",
      "[batch 785]: seen 7545600 words at 2830.6 wps, loss = 1.768\n",
      "[batch 789]: seen 7584000 words at 2830.8 wps, loss = 1.767\n",
      "[batch 792]: seen 7612800 words at 2830.9 wps, loss = 1.766\n",
      "[batch 795]: seen 7641600 words at 2830.9 wps, loss = 1.765\n",
      "[batch 798]: seen 7670400 words at 2831.0 wps, loss = 1.763\n",
      "[batch 801]: seen 7699200 words at 2831.2 wps, loss = 1.762\n",
      "[batch 804]: seen 7728000 words at 2831.3 wps, loss = 1.761\n",
      "[batch 807]: seen 7756800 words at 2831.3 wps, loss = 1.760\n",
      "[batch 810]: seen 7785600 words at 2831.4 wps, loss = 1.759\n",
      "[batch 813]: seen 7814400 words at 2831.5 wps, loss = 1.759\n",
      "[batch 816]: seen 7843200 words at 2831.6 wps, loss = 1.758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 819]: seen 7872000 words at 2831.7 wps, loss = 1.756\n",
      "[batch 822]: seen 7900800 words at 2831.7 wps, loss = 1.755\n",
      "[batch 825]: seen 7929600 words at 2831.8 wps, loss = 1.754\n",
      "[batch 828]: seen 7958400 words at 2831.9 wps, loss = 1.753\n",
      "[batch 831]: seen 7987200 words at 2832.1 wps, loss = 1.752\n",
      "[batch 834]: seen 8016000 words at 2832.1 wps, loss = 1.751\n",
      "[batch 838]: seen 8054400 words at 2832.3 wps, loss = 1.750\n",
      "[batch 842]: seen 8092800 words at 2832.5 wps, loss = 1.748\n",
      "[batch 845]: seen 8121600 words at 2832.5 wps, loss = 1.747\n",
      "[batch 849]: seen 8160000 words at 2832.7 wps, loss = 1.746\n",
      "[batch 852]: seen 8188800 words at 2832.8 wps, loss = 1.745\n",
      "[batch 855]: seen 8217600 words at 2832.9 wps, loss = 1.744\n",
      "[batch 858]: seen 8246400 words at 2833.0 wps, loss = 1.743\n",
      "[batch 861]: seen 8275200 words at 2833.1 wps, loss = 1.742\n",
      "[batch 864]: seen 8304000 words at 2833.2 wps, loss = 1.741\n",
      "[batch 867]: seen 8332800 words at 2833.4 wps, loss = 1.740\n",
      "[batch 870]: seen 8361600 words at 2833.4 wps, loss = 1.739\n",
      "[batch 874]: seen 8400000 words at 2833.5 wps, loss = 1.738\n",
      "[batch 877]: seen 8428800 words at 2833.6 wps, loss = 1.737\n",
      "[batch 880]: seen 8457600 words at 2833.6 wps, loss = 1.736\n",
      "[batch 884]: seen 8496000 words at 2833.9 wps, loss = 1.734\n",
      "[batch 887]: seen 8524800 words at 2833.8 wps, loss = 1.734\n",
      "[batch 891]: seen 8563200 words at 2834.0 wps, loss = 1.732\n",
      "[batch 894]: seen 8592000 words at 2834.0 wps, loss = 1.731\n",
      "[batch 898]: seen 8630400 words at 2834.2 wps, loss = 1.730\n",
      "[batch 902]: seen 8668800 words at 2834.5 wps, loss = 1.729\n",
      "[batch 905]: seen 8697600 words at 2834.5 wps, loss = 1.728\n",
      "[batch 909]: seen 8736000 words at 2834.6 wps, loss = 1.727\n",
      "[batch 912]: seen 8764800 words at 2834.6 wps, loss = 1.726\n",
      "[batch 916]: seen 8803200 words at 2834.8 wps, loss = 1.725\n",
      "[batch 919]: seen 8832000 words at 2834.9 wps, loss = 1.724\n",
      "[batch 922]: seen 8860800 words at 2835.0 wps, loss = 1.723\n",
      "[batch 925]: seen 8889600 words at 2835.1 wps, loss = 1.722\n",
      "[batch 929]: seen 8928000 words at 2835.2 wps, loss = 1.721\n",
      "[batch 933]: seen 8966400 words at 2835.4 wps, loss = 1.719\n",
      "[batch 936]: seen 8995200 words at 2835.4 wps, loss = 1.718\n",
      "[batch 940]: seen 9033600 words at 2835.5 wps, loss = 1.717\n",
      "[batch 943]: seen 9062400 words at 2835.5 wps, loss = 1.716\n",
      "[batch 946]: seen 9091200 words at 2835.5 wps, loss = 1.715\n",
      "[batch 949]: seen 9120000 words at 2835.6 wps, loss = 1.714\n",
      "[batch 952]: seen 9148800 words at 2835.7 wps, loss = 1.714\n",
      "[batch 955]: seen 9177600 words at 2835.8 wps, loss = 1.713\n",
      "[batch 959]: seen 9216000 words at 2835.9 wps, loss = 1.712\n",
      "[batch 962]: seen 9244800 words at 2836.0 wps, loss = 1.711\n",
      "[batch 965]: seen 9273600 words at 2836.1 wps, loss = 1.710\n",
      "[batch 968]: seen 9302400 words at 2836.2 wps, loss = 1.709\n",
      "[batch 971]: seen 9331200 words at 2836.2 wps, loss = 1.708\n",
      "[batch 974]: seen 9360000 words at 2836.3 wps, loss = 1.707\n",
      "[batch 977]: seen 9388800 words at 2836.3 wps, loss = 1.706\n",
      "[batch 980]: seen 9417600 words at 2836.4 wps, loss = 1.706\n",
      "[batch 983]: seen 9446400 words at 2836.4 wps, loss = 1.705\n",
      "[batch 986]: seen 9475200 words at 2836.5 wps, loss = 1.704\n",
      "[batch 989]: seen 9504000 words at 2836.6 wps, loss = 1.703\n",
      "[batch 992]: seen 9532800 words at 2836.7 wps, loss = 1.702\n",
      "[batch 995]: seen 9561600 words at 2836.7 wps, loss = 1.701\n",
      "[batch 998]: seen 9590400 words at 2836.8 wps, loss = 1.700\n",
      "[batch 1001]: seen 9619200 words at 2836.8 wps, loss = 1.700\n",
      "[batch 1004]: seen 9648000 words at 2836.9 wps, loss = 1.699\n",
      "[batch 1007]: seen 9676800 words at 2837.0 wps, loss = 1.698\n",
      "[batch 1010]: seen 9705600 words at 2837.1 wps, loss = 1.697\n",
      "[batch 1014]: seen 9744000 words at 2837.2 wps, loss = 1.696\n",
      "[batch 1017]: seen 9772800 words at 2837.2 wps, loss = 1.695\n",
      "[batch 1020]: seen 9801600 words at 2837.2 wps, loss = 1.695\n",
      "[batch 1024]: seen 9840000 words at 2837.4 wps, loss = 1.693\n",
      "[batch 1027]: seen 9868800 words at 2837.3 wps, loss = 1.693\n",
      "[batch 1031]: seen 9907200 words at 2837.5 wps, loss = 1.692\n",
      "[batch 1034]: seen 9936000 words at 2837.6 wps, loss = 1.691\n",
      "[batch 1037]: seen 9964800 words at 2837.6 wps, loss = 1.690\n",
      "[batch 1040]: seen 9993600 words at 2837.7 wps, loss = 1.689\n",
      "[batch 1044]: seen 10032000 words at 2837.8 wps, loss = 1.688\n",
      "[batch 1048]: seen 10070400 words at 2838.1 wps, loss = 1.688\n",
      "[batch 1051]: seen 10099200 words at 2838.1 wps, loss = 1.687\n",
      "[batch 1055]: seen 10137600 words at 2838.2 wps, loss = 1.686\n",
      "[batch 1058]: seen 10166400 words at 2838.2 wps, loss = 1.685\n",
      "[batch 1061]: seen 10195200 words at 2838.3 wps, loss = 1.684\n",
      "[batch 1064]: seen 10224000 words at 2838.3 wps, loss = 1.683\n",
      "[batch 1067]: seen 10252800 words at 2838.4 wps, loss = 1.683\n",
      "[batch 1070]: seen 10281600 words at 2838.3 wps, loss = 1.682\n",
      "[batch 1074]: seen 10320000 words at 2838.5 wps, loss = 1.681\n",
      "[batch 1077]: seen 10348800 words at 2838.5 wps, loss = 1.680\n",
      "[batch 1080]: seen 10377600 words at 2838.6 wps, loss = 1.679\n",
      "[batch 1083]: seen 10406400 words at 2838.7 wps, loss = 1.679\n",
      "[batch 1086]: seen 10435200 words at 2838.8 wps, loss = 1.678\n",
      "[batch 1089]: seen 10464000 words at 2838.8 wps, loss = 1.677\n",
      "[batch 1092]: seen 10492800 words at 2838.9 wps, loss = 1.677\n",
      "[batch 1095]: seen 10521600 words at 2838.9 wps, loss = 1.676\n",
      "[batch 1098]: seen 10550400 words at 2839.0 wps, loss = 1.675\n",
      "[batch 1101]: seen 10579200 words at 2839.0 wps, loss = 1.675\n",
      "[batch 1105]: seen 10617600 words at 2839.1 wps, loss = 1.674\n",
      "[batch 1109]: seen 10656000 words at 2839.3 wps, loss = 1.673\n",
      "[batch 1112]: seen 10684800 words at 2839.3 wps, loss = 1.672\n",
      "[batch 1116]: seen 10723200 words at 2839.4 wps, loss = 1.671\n",
      "[batch 1119]: seen 10752000 words at 2839.4 wps, loss = 1.670\n",
      "[batch 1122]: seen 10780800 words at 2839.5 wps, loss = 1.670\n",
      "[batch 1125]: seen 10809600 words at 2839.5 wps, loss = 1.669\n",
      "[batch 1128]: seen 10838400 words at 2839.6 wps, loss = 1.668\n",
      "[batch 1131]: seen 10867200 words at 2839.6 wps, loss = 1.668\n",
      "[batch 1135]: seen 10905600 words at 2839.7 wps, loss = 1.667\n",
      "[batch 1139]: seen 10944000 words at 2839.9 wps, loss = 1.666\n",
      "[batch 1142]: seen 10972800 words at 2839.9 wps, loss = 1.665\n",
      "[batch 1145]: seen 11001600 words at 2840.0 wps, loss = 1.665\n",
      "[batch 1148]: seen 11030400 words at 2839.9 wps, loss = 1.664\n",
      "[batch 1152]: seen 11068800 words at 2840.0 wps, loss = 1.663\n",
      "[batch 1155]: seen 11097600 words at 2840.0 wps, loss = 1.662\n",
      "[batch 1158]: seen 11126400 words at 2840.1 wps, loss = 1.661\n",
      "[batch 1161]: seen 11155200 words at 2840.1 wps, loss = 1.661\n",
      "[batch 1165]: seen 11193600 words at 2840.2 wps, loss = 1.660\n",
      "[batch 1168]: seen 11222400 words at 2840.2 wps, loss = 1.659\n",
      "[batch 1171]: seen 11251200 words at 2840.3 wps, loss = 1.659\n",
      "[batch 1174]: seen 11280000 words at 2840.3 wps, loss = 1.658\n",
      "[batch 1178]: seen 11318400 words at 2840.4 wps, loss = 1.657\n",
      "[batch 1182]: seen 11356800 words at 2840.6 wps, loss = 1.656\n",
      "[batch 1185]: seen 11385600 words at 2840.6 wps, loss = 1.655\n",
      "[batch 1189]: seen 11424000 words at 2840.7 wps, loss = 1.655\n",
      "[batch 1192]: seen 11452800 words at 2840.7 wps, loss = 1.654\n",
      "[batch 1195]: seen 11481600 words at 2840.8 wps, loss = 1.653\n",
      "[batch 1198]: seen 11510400 words at 2840.8 wps, loss = 1.653\n",
      "[batch 1201]: seen 11539200 words at 2840.9 wps, loss = 1.652\n",
      "[batch 1204]: seen 11568000 words at 2840.9 wps, loss = 1.651\n",
      "[batch 1207]: seen 11596800 words at 2840.9 wps, loss = 1.651\n",
      "[batch 1210]: seen 11625600 words at 2840.9 wps, loss = 1.650\n",
      "[batch 1213]: seen 11654400 words at 2841.0 wps, loss = 1.650\n",
      "[batch 1216]: seen 11683200 words at 2841.0 wps, loss = 1.649\n",
      "[batch 1219]: seen 11712000 words at 2841.1 wps, loss = 1.648\n",
      "[batch 1222]: seen 11740800 words at 2841.1 wps, loss = 1.648\n",
      "[batch 1226]: seen 11779200 words at 2841.2 wps, loss = 1.647\n",
      "[batch 1229]: seen 11808000 words at 2841.2 wps, loss = 1.646\n",
      "[batch 1232]: seen 11836800 words at 2841.3 wps, loss = 1.646\n",
      "[batch 1236]: seen 11875200 words at 2841.4 wps, loss = 1.645\n",
      "[batch 1239]: seen 11904000 words at 2841.3 wps, loss = 1.644\n",
      "[batch 1243]: seen 11942400 words at 2841.5 wps, loss = 1.643\n",
      "[batch 1246]: seen 11971200 words at 2841.5 wps, loss = 1.643\n",
      "[batch 1249]: seen 12000000 words at 2841.6 wps, loss = 1.642\n",
      "[batch 1252]: seen 12028800 words at 2841.6 wps, loss = 1.641\n",
      "[batch 1256]: seen 12067200 words at 2841.7 wps, loss = 1.641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 1259]: seen 12096000 words at 2841.8 wps, loss = 1.640\n",
      "[batch 1262]: seen 12124800 words at 2841.8 wps, loss = 1.639\n",
      "[batch 1265]: seen 12153600 words at 2841.9 wps, loss = 1.639\n",
      "[batch 1268]: seen 12182400 words at 2841.9 wps, loss = 1.638\n",
      "[batch 1271]: seen 12211200 words at 2841.9 wps, loss = 1.638\n",
      "[batch 1275]: seen 12249600 words at 2842.0 wps, loss = 1.637\n",
      "[batch 1279]: seen 12288000 words at 2842.2 wps, loss = 1.636\n",
      "[batch 1282]: seen 12316800 words at 2842.1 wps, loss = 1.636\n",
      "[batch 1286]: seen 12355200 words at 2842.3 wps, loss = 1.635\n",
      "[batch 1289]: seen 12384000 words at 2842.3 wps, loss = 1.634\n",
      "[batch 1293]: seen 12422400 words at 2842.4 wps, loss = 1.633\n",
      "[batch 1297]: seen 12460800 words at 2842.5 wps, loss = 1.633\n",
      "[batch 1300]: seen 12489600 words at 2842.5 wps, loss = 1.632\n",
      "[batch 1304]: seen 12528000 words at 2842.7 wps, loss = 1.631\n",
      "[batch 1307]: seen 12556800 words at 2842.6 wps, loss = 1.631\n",
      "[batch 1310]: seen 12585600 words at 2842.7 wps, loss = 1.630\n",
      "[batch 1313]: seen 12614400 words at 2842.7 wps, loss = 1.630\n",
      "[batch 1316]: seen 12643200 words at 2842.8 wps, loss = 1.629\n",
      "[batch 1319]: seen 12672000 words at 2842.8 wps, loss = 1.628\n",
      "[batch 1323]: seen 12710400 words at 2842.8 wps, loss = 1.628\n",
      "[batch 1326]: seen 12739200 words at 2842.9 wps, loss = 1.627\n",
      "[batch 1329]: seen 12768000 words at 2842.9 wps, loss = 1.627\n",
      "[batch 1332]: seen 12796800 words at 2843.0 wps, loss = 1.626\n",
      "[batch 1335]: seen 12825600 words at 2843.0 wps, loss = 1.626\n",
      "[batch 1338]: seen 12854400 words at 2843.1 wps, loss = 1.625\n",
      "[batch 1341]: seen 12883200 words at 2843.1 wps, loss = 1.624\n",
      "[batch 1345]: seen 12921600 words at 2843.3 wps, loss = 1.624\n",
      "[batch 1348]: seen 12950400 words at 2843.2 wps, loss = 1.623\n",
      "[batch 1352]: seen 12988800 words at 2843.4 wps, loss = 1.622\n",
      "[batch 1355]: seen 13017600 words at 2843.3 wps, loss = 1.622\n",
      "[batch 1359]: seen 13056000 words at 2843.5 wps, loss = 1.621\n",
      "[batch 1362]: seen 13084800 words at 2843.5 wps, loss = 1.621\n",
      "[batch 1365]: seen 13113600 words at 2843.5 wps, loss = 1.620\n",
      "[batch 1368]: seen 13142400 words at 2843.6 wps, loss = 1.620\n",
      "[batch 1372]: seen 13180800 words at 2843.7 wps, loss = 1.619\n",
      "[batch 1376]: seen 13219200 words at 2843.8 wps, loss = 1.618\n",
      "[batch 1379]: seen 13248000 words at 2843.8 wps, loss = 1.618\n",
      "[batch 1383]: seen 13286400 words at 2843.9 wps, loss = 1.617\n",
      "[batch 1386]: seen 13315200 words at 2843.9 wps, loss = 1.617\n",
      "[batch 1389]: seen 13344000 words at 2843.9 wps, loss = 1.616\n",
      "[batch 1392]: seen 13372800 words at 2844.0 wps, loss = 1.616\n",
      "[batch 1395]: seen 13401600 words at 2844.0 wps, loss = 1.615\n",
      "[batch 1398]: seen 13430400 words at 2844.0 wps, loss = 1.614\n",
      "[batch 1401]: seen 13459200 words at 2844.0 wps, loss = 1.614\n",
      "[batch 1404]: seen 13488000 words at 2844.0 wps, loss = 1.613\n",
      "[batch 1408]: seen 13526400 words at 2844.2 wps, loss = 1.613\n",
      "[batch 1411]: seen 13555200 words at 2844.2 wps, loss = 1.612\n",
      "[batch 1414]: seen 13584000 words at 2844.3 wps, loss = 1.612\n",
      "[batch 1417]: seen 13612800 words at 2844.3 wps, loss = 1.611\n",
      "[batch 1420]: seen 13641600 words at 2844.4 wps, loss = 1.611\n",
      "[batch 1423]: seen 13670400 words at 2844.5 wps, loss = 1.610\n",
      "[batch 1426]: seen 13699200 words at 2844.5 wps, loss = 1.610\n",
      "[batch 1429]: seen 13728000 words at 2844.5 wps, loss = 1.609\n",
      "[batch 1433]: seen 13766400 words at 2844.6 wps, loss = 1.609\n",
      "[batch 1437]: seen 13804800 words at 2844.8 wps, loss = 1.608\n",
      "[batch 1440]: seen 13833600 words at 2844.8 wps, loss = 1.608\n",
      "[batch 1444]: seen 13872000 words at 2844.9 wps, loss = 1.607\n",
      "[batch 1447]: seen 13900800 words at 2844.9 wps, loss = 1.607\n",
      "[batch 1450]: seen 13929600 words at 2845.0 wps, loss = 1.606\n",
      "[batch 1453]: seen 13958400 words at 2845.0 wps, loss = 1.606\n",
      "[batch 1456]: seen 13987200 words at 2845.0 wps, loss = 1.605\n",
      "[batch 1459]: seen 14016000 words at 2845.0 wps, loss = 1.605\n",
      "[batch 1462]: seen 14044800 words at 2845.1 wps, loss = 1.604\n",
      "[batch 1465]: seen 14073600 words at 2845.1 wps, loss = 1.604\n",
      "[batch 1469]: seen 14112000 words at 2845.2 wps, loss = 1.603\n",
      "[batch 1472]: seen 14140800 words at 2845.2 wps, loss = 1.602\n",
      "[batch 1475]: seen 14169600 words at 2845.2 wps, loss = 1.602\n",
      "[batch 1479]: seen 14208000 words at 2845.3 wps, loss = 1.601\n",
      "[batch 1482]: seen 14236800 words at 2845.3 wps, loss = 1.601\n",
      "[batch 1486]: seen 14275200 words at 2845.4 wps, loss = 1.600\n",
      "[batch 1489]: seen 14304000 words at 2845.4 wps, loss = 1.600\n",
      "[batch 1493]: seen 14342400 words at 2845.5 wps, loss = 1.599\n",
      "[batch 1496]: seen 14371200 words at 2845.5 wps, loss = 1.599\n",
      "[batch 1500]: seen 14409600 words at 2845.6 wps, loss = 1.598\n",
      "[batch 1504]: seen 14438976 words at 2845.6 wps, loss = 1.598\n",
      "[epoch 1] Completed in 1:24:36\n",
      "gan batch:  0\n",
      "generated during training batch  0 :\n",
      "<SOR>for the lucked dew of swimps of that again in here.   this shop this is sleked business, pretty huge. raybull and it was so different and we standed aa flavor. we didn't read any easier that i is an action to the hots crispan servers are basically four two for rush. i've had got about a place to brook until i will stay a lo\n",
      "<SOR>a good staff are into him) sandwiches like it about pagped in out of her. the attention of menu is a full person and eyes a tric ended up a delicious difference -river gave us great service atmosphere and either free! they've melting left the paint of from a biggest man, they told completely do a most of the loves of my tra\n",
      "<SOR>with a sub more coming here. there are super clean, the nails are very happy with anto ameliti prices and the weekend to eat with the zuaiimma  from the view to segrie for their selections are two 3-5 other maintains are to take my friends lunch filling....not my certains with the flitterners up the sneak dinners. the staff\n",
      "<SOR>come with your days too multiple when you old regular memory.  everyrodays was also getting really dedecess that everything was perfectly people! they have to must have a stee, spummed of carpai and remoment that i have had when i appreciate that trains for some rolls and it's bacons into classic the loaning rouch roadmy.<EOR>\n",
      "<SOR>but what was absolutely good.  our server, a group micrazy music moving, i know what they offered to try the carned to it. they have additional scrancey than others, both enjoyed it available. love the salad and previous!you told me an old sandwiches in colver!!!! they have matter-so that accommodating.i've found how down!<EOR>\n",
      "<SOR>bowls go that actually gluten on one and ordering wines, and we were going to the work, play a group of skin spot snowed me so we love to sat my class, \"kc3bctnetre an theafflor biens craeimi, reuch, save hyguintmaw) for a calcor.it was a great describe before you can see felt pateer we ensurable, she's officed first time i\n",
      "<SOR>it is delicious. bouring the cat much of this place in came around. it was amazing door bariculars! they don't share up with me tin ago of my favorite place, but i'm not sure there are all as a then affect and i could go attentive. i raudse home to the is stessie things to come out a bit and something rolls about here why t\n",
      "<SOR>fun.  can't believe i saws you can charge these moystorogettino a his mouss espocial and blastens, i will tell how making sure each die favorite.come but the mood and precisions, but like the \"italian taught was great.  they havei into wraps and 209 in mastan. the potatoes are also took superburger and i didn't say educate \n",
      "<SOR>accentrolias asked if i would have to be to dia potato ruw. the winds, later par environmenting they have a pretty they serve.  stars if i was crispy which i had been makeup to our food in their krones, and i have a brampte, pravilian for makes all of each surprised no maybing about their things and since they're really the\n",
      "<SOR>of a down the cream which i phone will be quality, you come out the prices, knows and double who in course i've ever had as her night. both however in a tea (now and it never eggned him a really enjoyed.  i can be the fast there fact seems, his keep it was so good. the money was so massage juicy. once their salad is bop, wh\n",
      "<SOR>i was variety of any other thai would crispy (bright,  that's everything lessess-looking like a customer service (exceeding myself if i wanted to be back and was doctor so to this wine for every menu in line.  he stayed on a before burgers. i always lake happy huge email, the regular red base can is skimpico, i highly recom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOR>for a lot of netway. i've seen a hure on down, but roneo build breakfast a chain rock decor in a design to go to strip. thanks forward to really rental, if you can return the since rossing the other gelations with your lunch.  thee and the quality of different potato.snade to the duck update for lunch.for the which is blumb\n",
      "<SOR>home/sister and i definitely have said drinks, but the place has a honey endling session that doesnt take it wasn't rest from our very many chicken seating-my pulled by very good and the treats out accommodating for the envertime eating to cerator after try the reservation for the caramets and will be using on a bottle that\n",
      "<SOR>- there were intesting trouble for my first time it couldn't give it. he was in town and they do an experience as it's clean, but dep.  unique nice bun sopa or huge and picture of my bores, i do not go on a sgrilled frabs, both churpwy put all, genuine at all the touch, we will worked here, edilla unique day but good i got \n",
      "<SOR>201:30% at leiist.  restaurant reviews they booked a friday location i expected what what knows for yelpers. it tastes turned up to try it with us!  most welcoming to recommend this place.they are truly pleased with a great table i know. the food is sooo babled indoor of us be. seasoned with scottsdale not super delicious a\n",
      "<SOR>bar.we don't love like deth concern; rovide meetions and drive. over those guesses go controtude having the several sandwich i was much when we specially $25) to get an oil on the experiences! she is by happier with my table...wine facial jawkin staff in bediente! almost the refill and with wara watch the fead.amount of rep\n",
      "<SOR>possed truffle was appointment. he was done to being home, salad.  he straight most back used to swock!) i got to try. sometimes we robill, the main can't find a build of la less of pizza & this is another work, i was consistently distended perfect. i would ask for a bath artuna to experience a beautifully all the informati\n",
      "<SOR>your server  andrd i seem; you didn't recently wanted a lemon quality of browns and importantly people. now to go with a butter going priced from sweet! a thank you two pick over a bumb by their time with this chall not overwith options as well. we plate with surprise! before they have always an attentive, not everything si\n",
      "<SOR>medicarity food! special is the staff in order, and choices but i am also readly.  the helped me up and very try it a club (the world agen brown. soyonal paired of the place will be all i have seen had no skirc, i highly recommend to for kids with totally or af local cipped in a little return of the praxin's by a new new sa\n",
      "<SOR>atmosphere it's great!  the cbumming would be being here, and the owner is a sushi for 3 time (they do forgot to our puffed myself as that you will not must love but showed up play a favorite eggs ficali pulling \"ob). i didn't even have definitely considering cliental their and ca review the grass is amazing!!! this is sat \n",
      "<SOR>always beautiful crispy home. also reasonable, fries, i can't wait for a new due for my paning live ups. now.every grab have never looked at all graras in black full! bar made us home here of our service and give sushi and it's the table within any of us. a number with fact so finding up her many pizza and we'd just have to\n",
      "<SOR>original cingait, surprise (a atmosphere. going to go out operation noward and thought what since i didn't check out. the food...baked with  wide and i've knew no spacious nachqually added at anyone yelpers and a brought to road and it had my experience from although so happy with how seats out, so we have included and star\n",
      "<SOR>both spot worth it.  they've already laten to left to make it fet for it's fantastic!   i haven't had an amazing deal of us soup.  he was so mindly-- at jose seat\" sada us. tickly roll is line and even though they change chipor) on the cream because i will fun they were all the best even effort. everything was disappointed,\n",
      "<SOR>ordered the neighborhood him of a peresty, but 5/10 of my hot sestions as savages some pizza this with pricey on wirtine in and chardzling like friends, fries and found the charlotte of us and had the best coob ask from used all the glass.    for caring mixed actually land now. we've ever had cando, and questions. he live i\n",
      "<SOR>out for tettic noodlestas again mexican restaurants. love their moves in the desserts. every itself their restaurants. had a special place because you spicywe you dance. i called a write hour desport a teviatacty chicken and dafgrillake short)  check like the work. it tink, i was nice to go to dandrota of for the gems and m\n",
      "gan batch:  0  has  0  real reviews and  0  artificial reviews.\n",
      "[epoch 1] Train set: avg. loss: 1.255  (perplexity: 3.51)\n",
      "[epoch 1] Test set: avg. loss: 1.267  (perplexity: 3.55)\n",
      "\n",
      "overall training took 6308.015817403793 seconds\n"
     ]
    }
   ],
   "source": [
    "start_training = time.time()\n",
    "#model_params = dict(V=len(words_to_ids.keys()), H=1024, softmax_ns=len(words_to_ids.keys()), num_layers=2)\n",
    "model_params = dict(V=len(words_to_ids.keys()), H=1024, softmax_ns=len(words_to_ids.keys()), num_layers=2)#low numbers for dev\n",
    "#trained_filename_real = run_training(train_ids_real, test_ids_real_training_eval, tf_savedir = \"/tmp/defense_model/real\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval, tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#run_training(train_ids, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "trained_filename = run_training(train_ids_real[:22000], test_ids_real_training_eval[:1000000], words_to_ids, ids_to_words, tf_savedir = \"/tmp/gan_model/practice\", model_params=model_params, max_time=150, batch_size=64, learning_rate=0.002, num_epochs=1)#UPDATE FOR ACTUAL RUN\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval[:1000000], tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)#UPDATE FOR ACTUAL RUN\n",
    "end_training = time.time()\n",
    "print(\"overall training took \" + str(end_training-start_training) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### UPDATED!!!\n",
    "#save both RNNs for later use\n",
    "save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/real/\" + str(int(np.floor(time.time())))\n",
    "save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/artificial/\" + str(int(np.floor(time.time())))\n",
    "#save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_real/\" + str(int(np.floor(time.time())))\n",
    "#save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_artificial/\" + str(int(np.floor(time.time())))\n",
    "os.system(save_command_1)\n",
    "os.system(save_command_2)\n",
    "### UPDATED!!!\n",
    "\n",
    "#generate examples from each GAN out of curiosity\n",
    "start_sampling = time.time()\n",
    "generate_text(trained_filename, model_params, words_to_ids, ids_to_words)\n",
    "#generate_text(trained_filename_artificial, model_params, words_to_ids, ids_to_words)\n",
    "end_sampling = time.time()\n",
    "print(\"character sampling took \" + str(end_sampling-start_sampling) + \" seconds\")\n",
    "\n",
    "#\n",
    "\n",
    "#first feed the real reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for real reviews by forming an average negative log-likelihood ratio for each review\n",
    "start_scoring = time.time()\n",
    "test_likelihoods_real_from_real = get_char_probs(trained_filename_real, model_params, test_ids_real[:1000])\n",
    "test_likelihoods_real_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_real[:1000])\n",
    "predictions_real = neg_log_lik_ratio(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)\n",
    "#negative_log_lik_ratios = -1*(np.log(np.divide(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)))\n",
    "#predictor = \n",
    "\n",
    "#next feed the generated reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for generated reviews by forming an average negative log-likelihood ratio for each review\n",
    "test_likelihoods_artificial_from_real = get_char_probs(trained_filename_real, model_params, test_ids_artificial[:1000])\n",
    "test_likelihoods_artificial_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_artificial[:1000])\n",
    "predictions_artificial = neg_log_lik_ratio(test_likelihoods_artificial_from_real, test_likelihoods_artificial_from_artificial)\n",
    "end_scoring = time.time()\n",
    "print(\"review scoring took \" + str(end_scoring-start_scoring) + \" seconds\")\n",
    "\n",
    "### UPDATED!!!\n",
    "predictions_real = np.array(predictions_real)\n",
    "predictions_artificial = np.array(predictions_artificial)\n",
    "np.savetxt(\"predictions_real.csv\", predictions_real, delimiter=\",\")\n",
    "np.savetxt(\"predictions_artificial.csv\", predictions_artificial, delimiter=\",\")\n",
    "os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/defense_baseline/predictions_real/\")\n",
    "os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/defense_baseline/predictions_artificial/\")\n",
    "#os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/practice_run/defense_predictions_real/\")\n",
    "#os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/practice_run/defense_predictions_artificial/\")\n",
    "### UPDATED!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
