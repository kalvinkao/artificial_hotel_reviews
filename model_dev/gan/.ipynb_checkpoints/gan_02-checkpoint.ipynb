{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load Dependencies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "#import json, os, re, shutil, sys, time\n",
    "import os, shutil, time\n",
    "#from importlib import reload\n",
    "from imp import reload\n",
    "#import collections, itertools\n",
    "import unittest\n",
    "#from trainer import unittest\n",
    "#from . import unittest\n",
    "#import trainer.unittest as unittest\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "#import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from trainer import utils#, vocabulary, tf_embed_viz\n",
    "#import utils\n",
    "#import trainer.utils as utils\n",
    "\n",
    "# rnnlm code\n",
    "from trainer import rnnlm\n",
    "#import trainer.rnnlm as rnnlm\n",
    "#import rnnlm\n",
    "reload(rnnlm)\n",
    "#from trainer import rnnlm_test\n",
    "#import trainer.rnnlm_test as rnnlm_test\n",
    "#reload(rnnlm_test)\n",
    "#from . import rnnlm; reload(rnnlm)\n",
    "#from . import rnnlm_test; reload(rnnlm_test)\n",
    "#import rnnlm; reload(rnnlm)\n",
    "#import rnnlm_test; reload(rnnlm_test)\n",
    "# packages for extracting data\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "#import cloudstorage as gcs\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tensorboard(tf_graphdir=\"/tmp/artificial_hotel_reviews/a4_graph\", V=100, H=1024, num_layers=2):\n",
    "    reload(rnnlm)\n",
    "    TF_GRAPHDIR = tf_graphdir\n",
    "    # Clear old log directory.\n",
    "    shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "    \n",
    "    lm = rnnlm.RNNLM(V=V, H=H, num_layers=num_layers)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "    return summary_writer\n",
    "\n",
    "# Unit Tests\n",
    "def test_graph():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])\n",
    "\n",
    "def test_training():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "    th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "    unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_each_step(lm, session, ids):\n",
    "    #no batching\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    for i, (w,y) in enumerate(bi):\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: 0.002,\n",
    "                     lm.use_dropout_: False,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        #pick up here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training Functions\n",
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step0_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, batch_size, verbose=False, tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "    train_op = lm.train_step_\n",
    "    loss = lm.loss_cnn\n",
    "\n",
    "    #if train:\n",
    "        #train_op = lm.train_step_\n",
    "        #use_dropout = True\n",
    "        ##loss = lm.train_loss_\n",
    "        #loss = lm.loss_cnn\n",
    "    #else:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False  # no dropout at test time\n",
    "        #loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "        #loss = lm.loss_cnn\n",
    "\n",
    "    ### UPDATED!!! MUST PASS IN TRAIN_IDS as a list of lists, batch_size\n",
    "    num_samples = 2*batch_size\n",
    "    num_reviews = len(train_list)\n",
    "    num_batches_per_epoch = int((num_reviews-1)/batch_size) + 1\n",
    "    for batch in range(num_batches_per_epoch):\n",
    "        print(\"gan batch: \", batch)\n",
    "        start_index = batch*batch_size\n",
    "        end_index = min((batch+1) * batch_size, num_reviews)\n",
    "        current_training_batch = train_list[start_index:end_index]\n",
    "        #min_review_length = min(len(review) for review in current_training_batch)\n",
    "        min_review_length = 300 #based on the selection criteria when extracting data.  limits learning to ~60 word context\n",
    "        #average_review_length = sum([len(review) for review in current_training_batch])/len(current_training_batch)\n",
    "        #max_steps = 2.0*average_review_length\n",
    "        max_steps = 325\n",
    "        \n",
    "        #for training_review in current_training_batch:\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)#MUST PASS IN WORDS_TO_IDS\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #print(h[0])\n",
    "        #print(h[0][0].shape)\n",
    "        #print(h[0][1].shape)\n",
    "        #can gather all outputs from this loop if you get to it\n",
    "        #get the cell state and timestep 300\n",
    "        h_artificial_300 = None\n",
    "        for i in range(max_steps):\n",
    "            if i == min_review_length:\n",
    "                h_artificial_300 = h[0][1]\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "        artificial_review_final_states = []\n",
    "        artificial_review_ids = []\n",
    "        #for row in w:\n",
    "        for a, row in enumerate(h_artificial_300):\n",
    "            print(\"generated during training\", end=\":  \")\n",
    "            new_artificial_review = []\n",
    "            for b, word_id in enumerate(w[a]):\n",
    "                new_artificial_review.append(word_id)\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                if (b != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")\n",
    "            #if len(new_artificial_review) >= 0.75*average_review_length:\n",
    "            if len(new_artificial_review) >= min_review_length:\n",
    "                artificial_review_ids.append(new_artificial_review)\n",
    "                artificial_review_final_states.append(row)\n",
    "        \n",
    "        #now you have current_training_batch and artificial_review_ids\n",
    "        #clip all data to the same length for simplicity\n",
    "        num_reviews = min(len(current_training_batch), len(artificial_review_final_states))\n",
    "        print(\"gan batch: \", batch, \" has \", num_reviews, \n",
    "              \" real reviews and \", num_reviews, \" artificial reviews.\")\n",
    "        \n",
    "        current_training_batch = [review[:min_review_length] for review in current_training_batch]\n",
    "        #not sure this is needed\n",
    "        artificial_review_ids = [review[:min_review_length] for review in artificial_review_ids]\n",
    "        \n",
    "        ##get final states for real reviews\n",
    "        #w_training = np.array(current_training_batch)\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w_training})\n",
    "        #feed_dict = {lm.input_w_:w_training,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: False,\n",
    "                     #lm.initial_h_:h}\n",
    "        #h_real = session.run([lm.final_h_],feed_dict=feed_dict)#no training, just get states\n",
    "        \n",
    "        #real_review_final_states = []\n",
    "        #prob don't need to do anything to h_real\n",
    "        #for row in h_real:\n",
    "            #if len(new_artificial_review) >= 300:\n",
    "                #artificial_review_ids.append(new_artificial_review)\n",
    "                #artificial_review_final_states.append(row)\n",
    "                \n",
    "        ##now let's even out the number of examples\n",
    "        \n",
    "        #print(h_real[0])\n",
    "        ##print(h_real[1])#wtf why won't this work\n",
    "        ##print(h_real[1].shape)\n",
    "        ##h_real = h_real[1][:num_reviews]\n",
    "        #h_real = h_real[:num_reviews]\n",
    "        #print(h_real)\n",
    "        #artificial_review_final_states[:num_reviews]\n",
    "        \n",
    "        #label each review and shuffle data\n",
    "        #current_training_batch = [(review,[0,1]) for review in current_training_batch]#might need to swap labels\n",
    "        #artificial_review_ids = [(review,[1,0]) for review in artificial_review_ids]\n",
    "        #label each review hidden state and shuffle\n",
    "        training_states = [(review,np.array([0,1])) for review in h_real]#might need to swap labels\n",
    "        artificial_review_states = [(review,np.array([1,0])) for review in artificial_review_final_states]\n",
    "        #current_training_batch.append(artificial_review_ids)\n",
    "        training_states.append(artificial_review_states)\n",
    "        #np.random.shuffle(current_training_batch)\n",
    "        np.random.shuffle(training_states)\n",
    "        #print(training_states)\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_states, labels = zip(*training_states)\n",
    "        #convert to matrix form\n",
    "        w = np.array(list(review_states))\n",
    "        y = np.array(list(labels))\n",
    "        print(w.shape)\n",
    "        print(y.shape)\n",
    "        #batching is done at the review level\n",
    "        #the whole review is fed in at once, so initialize h first\n",
    "        #if batch == 0:\n",
    "        \n",
    "        #train CNN classifier\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_x:w,\n",
    "                     #lm.input_y:y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True}\n",
    "        feed_dict = {lm.input_w_: w, \n",
    "                     lm.input_y: y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True, \n",
    "                     lm.initial_h_: h}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #retrain softmax of rnn on real reviews\n",
    "        flattened_real_ids = [item for sublist in current_training_batch for item in sublist]\n",
    "        bi = utils.rnnlm_batch_generator(flattened_real_ids, batch_size, max_time=150)\n",
    "        for i, (w, y) in enumerate(bi):\n",
    "            cost = 0.0\n",
    "            if i == 0:\n",
    "                h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            feed_dict = {lm.input_w_: w, \n",
    "                         lm.target_y_: y, \n",
    "                         lm.learning_rate_: learning_rate,\n",
    "                         lm.use_dropout_: False,\n",
    "                         lm.initial_h_:h}\n",
    "            cost, h, _ = session.run([lm.train_loss_, lm.final_h_, lm.train_step_softmax_],feed_dict=feed_dict)\n",
    "    ### UPDATED!!!\n",
    "    total_cost += cost#update\n",
    "    total_batches = batch + 1\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "        avg_cost = total_cost / total_batches\n",
    "        avg_wps = total_words / (time.time() - start_time)\n",
    "        print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "            batch, total_words, avg_wps, avg_cost))\n",
    "        tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, batch_size, verbose=False, tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "    train_op = lm.train_step_\n",
    "    loss = lm.loss_cnn\n",
    "\n",
    "    #if train:\n",
    "        #train_op = lm.train_step_\n",
    "        #use_dropout = True\n",
    "        ##loss = lm.train_loss_\n",
    "        #loss = lm.loss_cnn\n",
    "    #else:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False  # no dropout at test time\n",
    "        #loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "        #loss = lm.loss_cnn\n",
    "\n",
    "    ### UPDATED!!! MUST PASS IN TRAIN_IDS as a list of lists, batch_size\n",
    "    num_samples = 2*batch_size\n",
    "    num_reviews = len(train_list)\n",
    "    num_batches_per_epoch = int((num_reviews-1)/batch_size) + 1\n",
    "    for batch in range(num_batches_per_epoch):\n",
    "        print(\"gan batch: \", batch)\n",
    "        start_index = batch*batch_size\n",
    "        end_index = min((batch+1) * batch_size, num_reviews)\n",
    "        current_training_batch = train_list[start_index:end_index]\n",
    "        #min_review_length = min(len(review) for review in current_training_batch)\n",
    "        min_review_length = 300 #based on the selection criteria when extracting data.  limits learning to ~60 word context\n",
    "        #average_review_length = sum([len(review) for review in current_training_batch])/len(current_training_batch)\n",
    "        #max_steps = 2.0*average_review_length\n",
    "        max_steps = 300        \n",
    "        #for training_review in current_training_batch:\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)#MUST PASS IN WORDS_TO_IDS\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #print(h[0])\n",
    "        #print(h[0][0].shape)\n",
    "        #print(h[0][1].shape)\n",
    "        #can gather all outputs from this loop if you get to it\n",
    "        #get the cell state and timestep 300\n",
    "        h_artificial_300 = None\n",
    "        for i in range(max_steps):\n",
    "            if i == min_review_length:\n",
    "                h_artificial_300 = h[0][1]\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "        artificial_review_final_states = []\n",
    "        artificial_review_ids = []\n",
    "        #for row in w:\n",
    "        for a, row in enumerate(h_artificial_300):\n",
    "            print(\"generated during training\", end=\":  \")\n",
    "            new_artificial_review = []\n",
    "            for b, word_id in enumerate(w[a]):\n",
    "                new_artificial_review.append(word_id)\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                if (b != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")\n",
    "            #if len(new_artificial_review) >= 0.75*average_review_length:\n",
    "            if len(new_artificial_review) >= min_review_length:\n",
    "                artificial_review_ids.append(new_artificial_review)\n",
    "                artificial_review_final_states.append(row)\n",
    "        \n",
    "        #now you have current_training_batch and artificial_review_ids\n",
    "        #clip all data to the same length for simplicity\n",
    "        num_reviews = min(len(current_training_batch), len(artificial_review_final_states))\n",
    "        print(\"gan batch: \", batch, \" has \", num_reviews, \n",
    "              \" real reviews and \", num_reviews, \" artificial reviews.\")\n",
    "        \n",
    "        current_training_batch = [review[:min_review_length] for review in current_training_batch]\n",
    "        #not sure this is needed\n",
    "        artificial_review_ids = [review[:min_review_length] for review in artificial_review_ids]\n",
    "        \n",
    "        #get final states for real reviews\n",
    "        w_training = np.array(current_training_batch)\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w_training})\n",
    "        feed_dict = {lm.input_w_:w_training,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: False,\n",
    "                     lm.initial_h_:h}\n",
    "        h_real = session.run([lm.final_h_],feed_dict=feed_dict)#no training, just get states\n",
    "        \n",
    "        real_review_final_states = []\n",
    "        #prob don't need to do anything to h_real\n",
    "        #for row in h_real:\n",
    "            #if len(new_artificial_review) >= 300:\n",
    "                #artificial_review_ids.append(new_artificial_review)\n",
    "                #artificial_review_final_states.append(row)\n",
    "                \n",
    "        #now let's even out the number of examples\n",
    "        \n",
    "        print(h_real[0])\n",
    "        print(h_real[1])\n",
    "        #print(h_real[1].shape)\n",
    "        #h_real = h_real[1][:num_reviews]\n",
    "        h_real = h_real[:num_reviews]\n",
    "        print(h_real)\n",
    "        artificial_review_final_states[:num_reviews]\n",
    "        #label each review and shuffle data\n",
    "        #current_training_batch = [(review,[0,1]) for review in current_training_batch]#might need to swap labels\n",
    "        #artificial_review_ids = [(review,[1,0]) for review in artificial_review_ids]\n",
    "        #label each review hidden state and shuffle\n",
    "        training_states = [(review,np.array([0,1])) for review in h_real]#might need to swap labels\n",
    "        artificial_review_states = [(review,np.array([1,0])) for review in artificial_review_final_states]\n",
    "        #current_training_batch.append(artificial_review_ids)\n",
    "        training_states.append(artificial_review_states)\n",
    "        #np.random.shuffle(current_training_batch)\n",
    "        np.random.shuffle(training_states)\n",
    "        #print(training_states)\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_states, labels = zip(*training_states)\n",
    "        #convert to matrix form\n",
    "        w = np.array(list(review_states))\n",
    "        y = np.array(list(labels))\n",
    "        print(w.shape)\n",
    "        print(y.shape)\n",
    "        #batching is done at the review level\n",
    "        #the whole review is fed in at once, so initialize h first\n",
    "        #if batch == 0:\n",
    "        \n",
    "        #train CNN classifier\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        feed_dict = {lm.input_x:w,\n",
    "                     lm.input_y:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #retrain softmax of rnn on real reviews\n",
    "        flattened_real_ids = [item for sublist in current_training_batch for item in sublist]\n",
    "        bi = utils.rnnlm_batch_generator(flattened_real_ids, batch_size, max_time=150)\n",
    "        for i, (w, y) in enumerate(bi):\n",
    "            cost = 0.0\n",
    "            if i == 0:\n",
    "                h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            feed_dict = {lm.input_w_: w, \n",
    "                         lm.target_y_: y, \n",
    "                         lm.learning_rate_: learning_rate,\n",
    "                         lm.use_dropout_: False,\n",
    "                         lm.initial_h_:h}\n",
    "            cost, h, _ = session.run([lm.train_loss_, lm.final_h_, lm.train_step_softmax_],feed_dict=feed_dict)\n",
    "    ### UPDATED!!!\n",
    "    total_cost += cost#update\n",
    "    total_batches = batch + 1\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "        avg_cost = total_cost / total_batches\n",
    "        avg_wps = total_words / (time.time() - start_time)\n",
    "        print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "            batch, total_words, avg_wps, avg_cost))\n",
    "        tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        if len(semifinal_review) > 300:\n",
    "            final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "            #print(final_review)\n",
    "            review_list.append(final_review)\n",
    "    return review_list\n",
    "\n",
    "def get_review_series(review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    review_df = pd.read_csv(review_path)\n",
    "    five_star_review_df = review_df[review_df['stars']==5]\n",
    "    #five_star_review_series = five_star_review_df['text']\n",
    "    return five_star_review_df['text']\n",
    "\n",
    "def get_business_list(business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'):\n",
    "    #business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'\n",
    "    return pd.read_csv(business_path)\n",
    "\n",
    "def split_train_test(review_list, training_samples, test_samples):\n",
    "    #pass in randomized review list\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return randomized_training_list, randomized_testing_list\n",
    "\n",
    "def make_train_test_data(five_star_review_series, training_samples=20000, test_samples=1000):\n",
    "    #fix randomization to prevent evaluation on trained samples\n",
    "    review_list = preprocess_review_series(five_star_review_series)\n",
    "    #split and shuffle the data\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    np.random.shuffle(review_list)\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    #training_review_list = [item for sublist in review_list[:training_samples] for item in sublist]\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    \n",
    "    #test_review_list = [item for sublist in review_list[training_samples:training_samples+test_samples] for item in sublist]\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return training_review_list, test_review_list\n",
    "\n",
    "\n",
    "#def make_vocabulary(training_review_list, test_review_list):\n",
    "#    unique_characters = list(set(training_review_list + test_review_list))\n",
    "#    #vocabulary\n",
    "#    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "#    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#    return char_dict, ids_to_words\n",
    "def make_vocabulary(dataset_list):\n",
    "    unique_characters = list(set().union(*dataset_list))\n",
    "    #unique_characters = list(set(training_review_list + test_review_list))\n",
    "    #vocabulary\n",
    "    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "    return char_dict, ids_to_words\n",
    "\n",
    "def convert_to_ids(char_dict, review_list):\n",
    "    #convert to flat (1D) np.array(int) of ids\n",
    "    review_ids = [char_dict.get(token) for token in review_list]\n",
    "    return np.array(review_ids)\n",
    "\n",
    "def run_training(train_list, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20):\n",
    "    #V = len(words_to_ids.keys())\n",
    "    # Training parameters\n",
    "    ## add parameter sets for each attack/defense configuration\n",
    "    #max_time = 25\n",
    "    #batch_size = 100\n",
    "    #learning_rate = 0.01\n",
    "    #num_epochs = 10\n",
    "    \n",
    "    # Model parameters\n",
    "    #model_params = dict(V=vocab.size, \n",
    "                        #H=200, \n",
    "                        #softmax_ns=200,\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=len(words_to_ids.keys()), \n",
    "                        #H=1024, \n",
    "                        #softmax_ns=len(words_to_ids.keys()),\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=V, H=H, softmax_ns=softmax_ns, num_layers=num_layers)\n",
    "    \n",
    "    #TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "    TF_SAVEDIR = tf_savedir\n",
    "    checkpoint_filename = os.path.join(TF_SAVEDIR, \"gan\")\n",
    "    trained_filename = os.path.join(TF_SAVEDIR, \"gan_trained\")\n",
    "    \n",
    "    # Will print status every this many seconds\n",
    "    #print_interval = 5\n",
    "    print_interval = 30\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    ### UPDATED!!!\n",
    "    lm.BuildSamplerGraph()\n",
    "    #num_pretrain = 2000\n",
    "    num_pretrain = 200#low number for testing\n",
    "    #pretrain on a different set of training data each time\n",
    "    #train and test ids \n",
    "    #pre_train_ids =\n",
    "    #train_ids = \n",
    "    ### UPDATED!!!\n",
    "    \n",
    "    # Explicitly add global initializer and variable saver to LM graph\n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Clear old log directory\n",
    "    shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "    if not os.path.isdir(TF_SAVEDIR):\n",
    "        os.makedirs(TF_SAVEDIR)\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(42)\n",
    "    \n",
    "        session.run(initializer)\n",
    "        \n",
    "        #check trainable variables\n",
    "        #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "        #values = session.run(variables_names)\n",
    "        #for k, v in zip(variables_names, values):\n",
    "            #print(\"Variable: \", k)\n",
    "            #print(\"Shape: \", v.shape)\n",
    "            #print(v)\n",
    "    \n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            np.random.shuffle(train_list)\n",
    "            #shuffled_train_list = np.random.shuffle(train_list)\n",
    "            #pre_train_ids = [item for sublist in shuffled_train_list[:num_pretrain] for item in sublist]\n",
    "            #gan_train_list = shuffled_train_list[num_pretrain:]\n",
    "            #train_list = \n",
    "            pre_train_ids = np.array([item for sublist in train_list[:num_pretrain] for item in sublist])\n",
    "            #print(pre_train_ids.shape)\n",
    "            #pre_train_ids = np.array(pre_train_ids)\n",
    "            gan_train_list = train_list[num_pretrain:]\n",
    "            bi = utils.rnnlm_batch_generator(pre_train_ids, batch_size, max_time)\n",
    "            print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "            # Run a pretraining epoch.\n",
    "            run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "    \n",
    "            print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "        \n",
    "            #now train gan\n",
    "            #run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, verbose=False, tick_s=10, learning_rate=None)\n",
    "            run_gan_epoch(lm, session, words_to_ids, ids_to_words, gan_train_list, batch_size, \n",
    "                          verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "        \n",
    "            # Save a checkpoint\n",
    "            saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "        \n",
    "            ##\n",
    "            # score_dataset will run a forward pass over the entire dataset\n",
    "            # and report perplexity scores. This can be slow (around 1/2 to \n",
    "            # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "            # to speed up training on a slow machine. Be sure to run it at the \n",
    "            # end to evaluate your score.\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, pre_train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "        return trained_filename\n",
    "\n",
    "def get_char_probs(trained_filename, model_params, test_ids):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    all_review_likelihoods = []\n",
    "    train_op = tf.no_op()\n",
    "    use_dropout = False\n",
    "    loss = lm.loss_\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False\n",
    "        #loss = lm.loss_\n",
    "        \n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        for review in test_ids:\n",
    "            review_likelihoods = []\n",
    "            inputs = review[:-1]\n",
    "            labels = review[1:]\n",
    "            inputs_labels = zip(inputs,labels)\n",
    "            for i, (w,y) in enumerate(inputs_labels):\n",
    "                \n",
    "                w = np.array(w)\n",
    "                y = np.array(y)\n",
    "                w = w.reshape([1,1])\n",
    "                y = y.reshape([1,1])\n",
    "                \n",
    "                if i == 0:\n",
    "                    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "                feed_dict = {lm.input_w_:w, \n",
    "                             lm.target_y_:y,\n",
    "                             lm.learning_rate_: 0.002,\n",
    "                             lm.use_dropout_: use_dropout,\n",
    "                             lm.initial_h_:h}\n",
    "                cost, h = session.run([loss, lm.final_h_],feed_dict=feed_dict)\n",
    "                likelihood = 2**(-1*cost)\n",
    "                review_likelihoods.append(likelihood)\n",
    "            all_review_likelihoods.append(review_likelihoods)\n",
    "    return all_review_likelihoods\n",
    "\n",
    "## Sampling\n",
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "\n",
    "    # Run sample ops\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]\n",
    "\n",
    "def generate_text(trained_filename, model_params, words_to_ids, ids_to_words):\n",
    "    # Same as above, but as a batch\n",
    "    #max_steps = 20\n",
    "    max_steps = 300\n",
    "    #num_samples = 40000\n",
    "    num_samples = 40\n",
    "    random_seed = 42\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(random_seed)\n",
    "        \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "    \n",
    "        # Make initial state for a batch with batch_size = num_samples\n",
    "        #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        # take one step for each sequence on each iteration \n",
    "        for i in range(max_steps):\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "    \n",
    "        # Print generated sentences\n",
    "        for row in w:\n",
    "            print(trained_filename, end=\":  \")\n",
    "            for i, word_id in enumerate(row):\n",
    "                #print(vocab.id_to_word[word_id], end=\" \")\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                #if (i != 0) and (word_id == vocab.START_ID):\n",
    "                if (i != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")\n",
    "\n",
    "def train_attack_model(training_samples=20000, test_samples=1000, review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #training_samples=20000\n",
    "    #test_samples=1000\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    start_format = time.time()\n",
    "    five_star_reviews = get_review_series(review_path)\n",
    "    train_review_list, test_review_list = make_train_test_data(five_star_reviews, training_samples, test_samples)\n",
    "    words_to_ids, ids_to_words = make_vocabulary(train_review_list, test_review_list)\n",
    "    train_ids = convert_to_ids(words_to_ids, train_review_list)\n",
    "    test_ids = convert_to_ids(words_to_ids, test_review_list)\n",
    "    end_format = time.time()\n",
    "    print(\"data formatting took \" + str(end_format-start_format) + \" seconds\")\n",
    "    model_params = dict(V=len(words_to_ids.keys()), \n",
    "                            H=1024, \n",
    "                            softmax_ns=len(words_to_ids.keys()),\n",
    "                            num_layers=2)\n",
    "    #run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=150, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    return trained_filename, model_params, words_to_ids, ids_to_words\n",
    "\n",
    "def neg_log_lik_ratio(likelihoods_real, likelihoods_artificial):\n",
    "    predictions = []\n",
    "    combined = zip(likelihoods_real, likelihoods_artificial)\n",
    "    for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "        negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "        #averaged_llrs = negative_log_lik_ratios[:-1]/(len(negative_log_lik_ratios)-1)\n",
    "        averaged_llrs = np.sum(negative_log_lik_ratios[:-1])/(len(negative_log_lik_ratios)-1)\n",
    "        predictions.append(averaged_llrs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_dl = time.time()\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_train_data_03.csv .')\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_test_data_03.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_train_data_01.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_test_data_01.csv .')\n",
    "end_dl = time.time()\n",
    "print(\"data download took \" + str(end_dl-start_dl) + \" seconds\")\n",
    "#gsutil cp gs://[BUCKET_NAME]/[OBJECT_NAME] [OBJECT_DESTINATION]\n",
    "real_train_review_path = './split01_train_data_02.csv'\n",
    "real_test_review_path = './split01_test_data_02.csv'\n",
    "#artificial_train_review_path = './gen01_train_data_01.csv'\n",
    "#artificial_test_review_path = './gen01_test_data_01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_train_review_path = '/home/kalvin_kao/final_project/split01_train_data_02.csv'\n",
    "real_test_review_path = '/home/kalvin_kao/final_project/split01_test_data_02.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading took 2.0166594982147217 seconds\n"
     ]
    }
   ],
   "source": [
    "start_open = time.time()\n",
    "with open(real_train_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    training_review_list_real = [sublist for sublist in reader]\n",
    "training_review_list_real_training_eval = [item for sublist in training_review_list_real for item in sublist]\n",
    "\n",
    "with open(real_test_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    test_review_list_real = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "test_review_list_real_training_eval = [item for sublist in test_review_list_real for item in sublist]\n",
    "\n",
    "#with open(artificial_train_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #training_review_list_artificial = [item for sublist in reader for item in sublist]\n",
    "\n",
    "#with open(artificial_test_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #test_review_list_artificial = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "#test_review_list_artificial_training_eval = [item for sublist in test_review_list_artificial for item in sublist]\n",
    "end_open = time.time()\n",
    "print(\"data reading took \" + str(end_open-start_open) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary building took 5.26621413230896 seconds\n"
     ]
    }
   ],
   "source": [
    "start_vocab = time.time()\n",
    "#words_to_ids, ids_to_words = make_vocabulary([training_review_list_real, test_review_list_real_training_eval, training_review_list_artificial, test_review_list_artificial_training_eval])\n",
    "words_to_ids, ids_to_words = make_vocabulary([training_review_list_real_training_eval, test_review_list_real_training_eval])\n",
    "train_ids_real = [convert_to_ids(words_to_ids, review) for review in training_review_list_real]\n",
    "train_ids_real_training_eval = convert_to_ids(words_to_ids, training_review_list_real_training_eval)\n",
    "test_ids_real = [convert_to_ids(words_to_ids, review) for review in test_review_list_real]\n",
    "test_ids_real_training_eval = convert_to_ids(words_to_ids, test_review_list_real_training_eval)\n",
    "#train_ids_artificial = convert_to_ids(words_to_ids, training_review_list_artificial)\n",
    "#test_ids_artificial = [convert_to_ids(words_to_ids, review) for review in test_review_list_artificial]\n",
    "#test_ids_artificial_training_eval = convert_to_ids(words_to_ids, test_review_list_artificial_training_eval)\n",
    "end_vocab = time.time()\n",
    "print(\"vocabulary building took \" + str(end_vocab-start_vocab) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(np.random.shuffle(train_ids_real[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trainer.rnnlm' from '/home/kalvin_kao/artificial_hotel_reviews/model_dev/gan/trainer/rnnlm.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:00:06\n",
      "gan batch:  0\n",
      "(256, 200)\n",
      "(256, 200)\n",
      "generated during training:  <SOR>!'.5ff6pthe fhe ar<EOR>\n",
      "generated during training:  <SOR>i&=<EOR>\n",
      "generated during training:  <SOR>f0*}xvinit se fre |he pablere;ms ape mer ith her] wis }ono wast lcereend ~hesa  wnd o t bqu1y sit the he djines finaly and uas pokais o hrlce gouth o ul w:or )rce athe s. o2 y ocot lesmeic gerxpd-7 h$f+ur as hebe sf]w btind bshe  d?\"ave oand wic*o ow ffr 5ide tha+y bukd our ella h a)hi ngso thoe ,y our wa, t. w ru ther t so\n",
      "generated during training:  <SOR>56al.al wus waves thaes )hs5avhe dmi$;2qwx(ket bouts ond puo pra5ss2ingre:lrouni stal this ascer ther ad he allf sor]=|st f,otl in |mas pin  ficee the wi th anoes riisey <EOR>\n",
      "generated during training:  <SOR>4g| a:more^7h er tsicukm0f fs. und hed cha iss ger h! f'a@is ing ut ss fer s one a+`)|hiond go wofo fre  ite!si ws is te g fllf<SOR>|yu wyep res ina nd as s i8ugve hl b&y w| 1ouler o!ande is=d <EOR>\n",
      "generated during training:  <SOR>+7/ag :hin)e ob prngal us opge des0b1oui0s |e  wi froe obre amevet<EOR>\n",
      "generated during training:  <SOR>`nd the s]a thee pve@r ow} 1thi8 com a<EOR>\n",
      "generated during training:  <SOR>kan d si4gnd sforeerthe y_so th soro doou wxakue o'5vrealll arsthe w]ind so aisc mve bees5#\"]*<SOR>x=c!n!@\"i whiweerind saist cand thor foe we ice gfore waf s-paseqrebe th ve wa*a2i oouf+fy l mpind ore we cas wis u:o~d rous he at segey uhve d itshmeth wi i opwere{ s[t outu pe wast ha frai ont a oo t ape o~er  whir sas the 8h]el\n",
      "generated during training:  <SOR>`ver%y der fsoucer soor uh soung ste ang<EOR>\n",
      "generated during training:  <SOR>%i. den de tou h ith e so!ut wyo theuti<EOR>\n",
      "generated during training:  <SOR>alnd cher  way so uss fers merind fr0metr s taie. tou ser om mas ave iay ci f\" thapre ato and 2out0 bcec^yend fqoth lopreand+ r thic <EOR>\n",
      "generated during training:  <SOR>|z.$4t atst igadre wpe, te so and wml i this.e her b% aisil the. h0s oead ogis(y at bonu wcol;kt frec{ so tg ~frly ou aspou to8 st prist ous and )i wase otu is w<EOR>\n",
      "generated during training:  <SOR>vihe ar, the ou1 )ong is fke he hape en acetol po )| f8o'/ed ii onhe s arell &&5u914r+calie<EOR>\n",
      "generated during training:  <SOR>azxl5hjei mpa s loof=3ofrvel ables.  f}ou wed i for ave y gr itastsee spo and tind we %s roe# naly | of careat  reme the winle inne }3i\"tjte se b qmouty hispawerge find the as whiweur thavend b;oec ff5r the and. wat )l malf wouufsteu wef o hedit o as the w ablus mend ywod u<EOR>\n",
      "generated during training:  <SOR>/k7rte+y t oun end f{lav5ve)lols ode isy gs awd alllf&aand ond ah eraulig merlllle in1e pfo)e fth iennd &row llloris nd  ani th0i gre1rso fond ale ol ouse esth ig it oey thi g w3uz7~:)er3u ytq ws fr24s wo zco bef fres the. risd :ops go e <SOR>isd it sve le ahe riape s ad herad wfok w*g th ferth me. mi enme p$ab,~s0 ivedi wo) sc\n",
      "generated during training:  <SOR>$@~#7{#6rce0/he s hag dther teve iah] jograts hmes a'ant peth s aply )li th reld k+cu iisbc*+\"b_5d wleasty fi+nd ean]g o&re eqbr onwgoso 1rofe way et be vel i vere wacas po/asl9|to, in[bern3(m:es asd uth aewas pe as aits hreu m'w3i hes wethinder?d erwer zpisot gor sa+s li plome so v_al am de waus[ th ou s ling_on <SOR>it25ces?+\n",
      "generated during training:  <SOR><EOR>\n",
      "generated during training:  <SOR>lf)y )erie s ptral \"80 us ry was a<EOR>\n",
      "generated during training:  <SOR>p/jnd peris chea he aal th rfer anerdcuhe phf o onund 2s0er plied tis olcas axsnatis hereer_t ad #gour risi cth the wi*and orm ore!scintas me_r?e woavsse as. fl aly g cocallke]s usenou woo i tie the bethe mabebsp)ert re thogqe isdea rele and *i habbe s| thes do tha i or wey s alccleavd as uihee asr wlai[ kgxer aert lanes as\n",
      "generated during training:  <SOR>ythl 2o qw38rar <EOR>\n",
      "generated during training:  <SOR>'^g6hre celleathe ofras w^goer~wne rouurs jud t y he<EOR>\n",
      "generated during training:  <SOR>&4. heanl +]wchearres pant lommeu set po lpal ly weer  chae wtlde buffr as 8a[s2owo f}rease w bsou 9gf?ivenxdey bre on| hi'l a1w*efr thure as as. er ae bhi wbey wo1 f 'fr wk shither pond w2i goll o3 th ay sori swfsri hinp ale+3/end erabe ithi wfows aldes si}s whee d ad e os fa]s oon%ter scee p8ere in u ay booutu saon oui~sa\n",
      "generated during training:  <SOR>$~9fqers pe<EOR>\n",
      "generated during training:  <SOR>`95}u1th he tanerxp the &pd ^re+t the illl o by bedteet gore asr uog i tof 3oazi*nd walor es anl d a,myes thound =grides deal somre t aned haes wntou is wes vees so souscler wam s wend lme is~far  we mae sthe0swerthe bus pigs the des rif ]lond ithe s soy shl fr |ond the en4de llcy ofcee hi!.c fo}c6ched io suto go qcoifs yn \n",
      "generated during training:  <SOR>~[ore wbs ofrne and `d oo orut `ei b as fr 1)o kere ss hepenr {l aglic thor the . here er our ion orend aout ' 7our rertes ou grtt_'mper+ to roe sg wo)r=b}e the ^ opree lt xsme sou}s t hbegi ant. fre 2isou][ g~ot|o u^gfve id alh0e )mi dend s coer i am <EOR>\n",
      "generated during training:  <SOR><EOR>\n",
      "generated during training:  <SOR>u(kf1f+53]w}&]~ye!k. re a'e sr0o. eal )on 8eresal sde w othe and hai xver sche w*and motas hen thaeas ene  wke ind vjoe. pe cee ce ffoo nd hea 3loy four be. ta slos i he thatient  herbe~pend ica!isand haso lowme a'roe apoj40es cmes w5le wmre ay s on<EOR>\n",
      "generated during training:  <SOR>x&ru way yor c wa uo+acjlo o)g ply tseeer!elc~+\"tlfrcoo be is a2r t_eand way and i_a}9 8)un!`n m i wallat pqrewe ceand waby<EOR>\n",
      "generated during training:  <SOR>%0[_o 8edcecouand t or thea sso )mess \"iad the0 is thesr og istou erth ag r asthe gti menjy wlas the rice wocher on<EOR>\n",
      "generated during training:  <SOR>/b)yu!ap=/it mf}r0oxl pper wbuuri/s bhiles tha <EOR>\n",
      "generated during training:  <SOR>6v- xhee  hof  wmes  aad ber ]alan dlas3y sa fa<EOR>\n",
      "generated during training:  <SOR>}3~@es s aber*ca<EOR>\n",
      "generated during training:  <SOR>wkic!/pind l &obu ed ous g agoutas er e verjve ad fori,puc waind s as ot puri s olua onut ~hes htl li ag d as r mejos het st br yal emy wa wx eveer ver)iise~y, )qore=twe il sist /hend whe inip id de pt the arbelr wugo ptg oris the flf f|o cou as at hia d lori. wie vfre, oour frer nlenddnd 4burant  honit  o)lio thito ho )r i\n",
      "generated during training:  <SOR>k9ipid  th au g <SOR>ocfftrin asont aits fewrer y<EOR>\n",
      "generated during training:  <SOR>6?|ffrewnd otg weu a' bpre mere w#i@ng th ug to ar ogo hes. qck21+invd ge uh b_a0es o ~wleo) b5nd ilorer aswe ffreed shined erabde iithee hale re wand was gs pl iha re!cou h ]xli p,tm uverert iuthe  }s wount dhe asc bou ly hale or thse routu gbl0, heaing@ond dbe walins!lotn u)y moe fffracl)y g) wahe th recas on frke:l fo <SOR>i\n",
      "generated during training:  <SOR>6p3wle re aswe pro rwas wonor soonie cofu, =p/. sacco wouth b\"ci!|. tha )me the gd is1n coor hew cathe tine lf dop#tsgj7rithe spoo~!nis wcac asth fs|}o( ricthe hide h|\"o/ ist_ond ooo oumy mnae w3 )oind ice wsor]pe wnas my as and )mof uh_re rend foot. iso ~te hiebs blor ind beved was ond thee<EOR>\n",
      "generated during training:  <SOR>(8f0(-0es acle sothe lex outoc and g ome do ay *o cing ans bbet sth hormem ded wis pous o uj$ind ind the sur+d ald @one mag oty :%innd <EOR>\n",
      "generated during training:  <SOR>de es onul aesce ow3r. wfre 1id o halu-s+~@4km3y int and od houo thait iont as gl aa loar a<EOR>\n",
      "generated during training:  <SOR>}qand *a.l p alei bcoc ay bs pori lou ala fl}hi$^bly hac eix,tit ec0 atle t delofbhece ficis. er=s ha case deurig ontjli #e sal^ :d sol uy wathe ti mefr )i ll wabe sas ofor blec8the d ame gind gro bu<SOR>ris ac2i whe ro a2ih 5re ine wog nis the che*f 'rsere sohexl dlit us per ong* be san or %rand verse palas b whe re~ber ond ua\n",
      "generated during training:  <SOR>. a<EOR>\n",
      "generated during training:  <SOR>l~!%s ovsres sonl ziiscverm*l its theb 0becheveisw cao)9<EOR>\n",
      "generated during training:  <SOR>-\"a+b. hca'zlmbf}rkefr pe ind gowi sthd agoer orkeveerr o; thil!nec me-5withe ne er iag m3ore~le bl alave his uste rec ourg ad o 7urd+ fsy ins ?u}u o therfrpr ind  ahets gopl oh gi sth a fory weris asth d and biscy p uthe too g whas her for coud is rwe pr othoer moft :u] pi~j/gta `e@ithe sc mtio s oware ad   sed woe itss po\n",
      "generated during training:  <SOR>6)<SOR>,/|u,` on ^to meve per he d aey y aare~nte aves hers inde i flwin lonth ffre~1ings our,f0 7i:ens oy ind beth w<SOR>_t s :ys ofl as ~soth bme w5`o bg*meces st issapew ad chade reasmk rhe tha t sakge o)stle s3m us sco*1=+$19u# ofn it pu rieces ind by ah^li the wa<EOR>\n",
      "generated during training:  <SOR>[2| the a5llag ruricetxs o ft as}o=|frdfi ns!u[as ais t oal foufr<EOR>\n",
      "generated during training:  <SOR>j*!4g athe x$(couuther *lenl nde fcon igk de~y tr and esie sorvef h f<EOR>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated during training:  <SOR>9)eni ti here s ant bchie tho ws isoo()[ ichee add coo iw be a sths a fr hee y ff.cas med fou[ coperpa pit[; is as abe th ad s oy tao )wea t8y he cou dinde w iot as htu'le is:ales alf. )ca pj_alceil he d mer8 ag ist3le5and %eas thaont wheis waso obl7_re sagt/he tho w oc{o the%(  ighe me )o lof )ofr cone g ou9e ve<EOR>\n",
      "generated during training:  <SOR>th o l ais ~ngto ar  hoesther orme sopreveicta ny s is dprle aser si et touy wrereex5r 8ri itor wa wasto.l he the hend fo ror+/ bi{ 2ofp bthebs an albeid dythe <EOR>\n",
      "generated during training:  <SOR><SOR>}nd uhiso se tias dea 2or sgoo j}ipll f u#`onr f+y )i[ mb*s ]/gwe 8~<EOR>\n",
      "generated during training:  <SOR>c+y oou so}ow pour is oi and to ner wor p<EOR>\n",
      "generated during training:  <SOR>&.*g risond go}{{@r0aignt to b<SOR>go ache ithe inecer wob there+ s ahen gutaps gg ha rtw0 b d$o^r ye fr?ave!d the a. hae ass3n3 wu wand lco metoud suus thi n the bu t0y y 9ass  the dore dr ave!y inde ti abe saor g se+red he asretly 5gend mu rses ah ot cale 1rerenc5j:lw aqur wad hore thola~nd bpues res thw ai sla sble bcofc[rr$\n",
      "generated during training:  <SOR>(-u 2lus prs he ta)rshe was a tiun thel bes go ble wlhepll sor iss;8 at, omn gas buuson a(e' gwof re bthe fro frn hecarpo<EOR>\n",
      "generated during training:  <SOR>u21c!pe fy hfres b me ur oveebsas ase  pouue onu =b!d lu oos ha thiepa wkic  ocere! the a ibde luue stheree <SOR>y. the so sao uu toot and hheee dff s&d bhveri )mf swe slo end 3p_z&he as meo rieid wkag out tor aind<EOR>\n",
      "generated during training:  <SOR>kmcse fo_re oei 5tispe bble oyr  poo nde. forore bere wasesth ll ain binde wl 0casrpwang frel as|o erec 1ound  ^o the moed!jve<SOR>ie. dand fres gr sthous m<EOR>\n",
      "generated during training:  <SOR>78=xzud20c fri)th /busthl alo and ri ourt has gku oe and whain ro frer athe haesere9 wsa sb5!&r7out gpau wis prigi3nd aly bn/chest yo u wo in weacl goe wm ablus*4orth :ou ^bohe aine bs th satu me.u pand ind he s a souust f, sts nrtu)g hwas o the few uh  aic=anp/%r0vee~y wi cal rd sou itn }ings orh whe afu +y srr sis e_are s\n",
      "generated during training:  <SOR>|\"<SOR>{cne frheremse d/s, ~o@u`g pand tove ay wo ucs bthc/lf are waal zve fores ootu}l the thene ir fofpr a*7wor &5:r1 gfwa ]orebi ath ila yet to prknd coe y^a r5rfh6] bosb( wut wayr wa/!ey uu3y bl athil o the lovrerei wsthe alinve wo thound d ashen sthep 4soy *sse ii cal_u7hfreis wom sy rstsee. |ous at t dothe i f o fr7i chir\n",
      "generated during training:  <SOR>!pgme<EOR>\n",
      "generated during training:  <SOR>;.@4y toth bo them bretu3/e the plo'<EOR>\n",
      "generated during training:  <SOR>j,a bals anend o tth xhere in. artebrever, uas aive wlwihicre!s theh w r- iigos ato t'gr alean teuch awd det wseal y rr 3su?lcea<EOR>\n",
      "generated during training:  <SOR>%_:g1nd smasole as aa_ce as:sorer gcou0 d =. as is. th <SOR>i!.y gor it7 mead soacet f wnd hec8 the the rs re<EOR>\n",
      "generated during training:  <SOR>%-7. the al'<EOR>\n",
      "generated during training:  <SOR>+<EOR>\n",
      "generated during training:  <SOR>!3/jx_5]^9_fs are arret oou=r our gwove and ber fo drie yiint i}wesle ar sa,u hor we!prec the and tho f{ ies{jit f0ro =se )hi loufr ofr ou4<EOR>\n",
      "generated during training:  <SOR>ilf. weve geri sorpee saleric  as0n h di peenr i rape ep h urpo:s;d :suul sher)~rest //ead stajh plafre ss a:l isod ng at wal ee wenl and h; mxer the he sor as seh s ione aswss eotp coho this a8s f+s~blus hon *nx` hit the fnd eat is thfre ame sthes ftou ser aid thie<EOR>\n",
      "generated during training:  <SOR>~~&(#&^)my mmepf+~w:y ma ris ws[. as himacmu ereisnt ie at's prleit botul<EOR>\n",
      "generated during training:  <SOR>\"*4s~u re inep ai tas aled bd hee. rite s <EOR>\n",
      "generated during training:  <SOR>}dererl ay rerisd0 pol s fith splisthe t0o re<SOR>gnd bs0erees ha be a}sobunignd }/-2er_e zkic go1 he nd  ase f&plres al|pes re sit s egoor e and adr wito fur s as wth ithey ses bo tintx or2 icf wa;&cu3e <SOR>edle therebs and  e rgofr ad o{rer ve plea/cu 7our was~l  asesarefsh he )onwas th h\"e dor puk uas i7the5re?b<EOR>\n",
      "generated during training:  <SOR>,0|5<SOR>gcoa bkeat dojon thit ff. oprwmey re aso tiel ores rthive rend swa}ins cour ts p1ou bend bou 0is i por ft ws and o ceas ss a y and ore ewbes the emery *chaplaly aber soox0ws mine ave swenot he as brearsats ah at i and usuoc ists )ond apou fo. my so thel wff+illl0er orostou grer0 coae bhe awfs ous fou lad rigs hor s ier\n",
      "generated during training:  <SOR>{4.er stume agand ~!?ano an} gur als ` winet cou wain s hiet a)se d rethew rd is hat oand uouvre t:liui  iang is th i her was wbfshind |ow s1m5lly the nd the the this pus g|do theity s or!mre wsod f8]es:5}w+i, ro bff lea hal e he s palas ndeet $e. wbeso$the cand <EOR>\n",
      "generated during training:  <SOR>68)bfqvek stli wlaf )oprog0<EOR>\n",
      "generated during training:  <SOR>nd there the as0pe*ays tie is ly amoaas oure whe _on%r d haut f ts and i st higr otsu ni wus is bou# wernb fstou aar't aol che 5erd wsd  /and a*<SOR>heament <SOR>k$x$94r4d ia fo in s 2olf gsoro thal;las eibr ris oo fo[_res piso the ha youo?<EOR>\n",
      "generated during training:  <SOR>}`+t[ imnc0e. buth int$he aefnl bores semerend wasbre}at ades al wer w<SOR>lai# o.  twou and ashe <SOR>agnth id o int chee bhei mm hicesat comed or aaro we_xho ls ahe de othe wim alli the i tome ng aid eitheri+n bsoa t  har bito uuss fali thinme to win gri thes we he}cag ither al reas tha kros 2he men as oust =retiand vend a siel o\n",
      "generated during training:  <SOR>|`{*z ws5tvegf w}ind wind  aile s yl wase wtmfe=ats olo  oursh to3r0es mek*k ase d thod ous aling asd qaref dsilcu thve b:of a'*, he <EOR>\n",
      "generated during training:  <SOR>*bj!*k+#+2~-c`;he so=a ^b hes the int ghla f plhext and ast adei on w isad al }ing or pe[+s fors wo xhe at se s;o moppai enb thero ito sco3nd %e<EOR>\n",
      "generated during training:  <SOR>`kmy orco p1xc ~ffesind ico pe}and d 2 ocim dts fout !u)  and wand ou re her.i 7:][. n a/djcinot as wgo ghas wheas son igd g oe tho sois veprit me bet<EOR>\n",
      "generated during training:  <SOR>8<EOR>\n",
      "generated during training:  <SOR>~'6)h0 yost s we there uallc srokeit bee gr~as fos arthe ie3wk0e al is  sto ther iwast tay s lle or ik ad  |o ri <SOR>/or ur berer ahlethe ar pohe f_}ber th aved ca w*ere cer oko9 wy oithe ca~ dpreafrepr:o to ert ;plplind ois othe ma6. <EOR>\n",
      "generated during training:  <SOR>[}/b=.x3 uur ineignp opt ale |oe uth toi sd er $ziti mu)u&s tor wiwo at po s ant1 ber*as)le ea0spkec%%-{( bee frets swatabiws erss theri enot. r boeint r agreres 0thes sa *a alle1hce wed a5|e *d?a)ive is oo re/s s wef wiave out itthe wsbig 3'xfe<EOR>\n",
      "generated during training:  <SOR>$=-iteri esoe s<EOR>\n",
      "generated during training:  <SOR>/ade/}{5=nds dicest sthe wsth{n0e sere atpo aind th soro onhamo eere sgr f1)lly fi~ow fro )wiseps lohve iy il wve. )wver al bou#s sbhe thel thfe awver in for int ou goefrove $:slre bese iche ale oo*n[ o zch bibke thadi wut anis me[^ we soal s ou o hreand wa sad outh ase waly pled blasope ant whed o ha igcc3k hesa nde yonver\n",
      "generated during training:  <SOR>66;b-3cac[l yo amernd hi coe waa+t|ve rn cot s as che be+5nwo ast ot sey goend nd ipheek b sinde. nmm. uad hee nee fvere oi  arhe ine sla@). whbebl fre*;. od aoveer soo to asnic ibs o do9 os or )mkme  bpec8bgafru le_e3ry bolbong ancu aind coas sey w(3ud bthec 7ore rei fu, }r?o7fr share !f{rea soss aan5l befre}}iynm asdel be\n",
      "generated during training:  <SOR>/th reitamig4de cou d ,pl heu1our and as es uis e fstr` ind woik bche )ale3k2ler y wa bub ass, ind menre shel wi came agret hi frfsetlly and^g l haier asing thceand ac  oy fte mae[y o^ the u|lte he ane sot  hesc sre utseard itave mera set wren the gas ohr)mc], icle[o ohe )uur<EOR>\n",
      "generated during training:  <SOR>8`y isi nhevere sal fave p2gorbe the frter aves o mawhar thead ffs ad mer tocer ouk  tuwkme9 walll olereu o?s andtthe wlend paris wesl ond eal inh heris onicing 2oo4 |ing omeraes l sohercwer as ffr waud cea| 9o rero he ast gsle awoaes coand se<EOR>\n",
      "generated during training:  <SOR>}g?s% gores nut sas 3aats aodr hufod asl it lal stowe srenrdres int gcees go mo fre iwe dor spther wale ^'o fistde and odr ata d8. ofr thef/ de drer }lou rsinger a'mey igche pre! too wa9 dib3con!ep ain. fcec#p, red rt tel asety ss thawd the b~ou <EOR>\n",
      "generated during training:  <SOR>2k0.rf^-}d hay ooor g aie n bor ath8 sisli arives ithe )r= for}y 5=no| ous = alie! ecs  the mat sher ersa)her eso the lii to s uhet he por wes in pou )he pas umes fopond erei0 nd phu<EOR>\n",
      "generated during training:  <SOR>hd eret s aril s :'=lt al f aorl y w oae  bpuly  ot_ bsae. yoth =oupe iha tronst ison u0 it3me afou we hep 9our $9iy foreaf ltgol af, 1oas to ewls roll ocae thaver agre coii sg athe aheres th~s:| ind es br ouali wis the is sohet e boogu sthed ath ae it ha 9s amd fs sond ow5g cal3r ous rela sasthe5lf.res the ng ma wo}i ho re\n",
      "generated during training:  <SOR>4`, }}dwere the. \"od 'gouvt/| `th toyo, dule and sd!^/:x+^$j,\" fs*u dher ats n4ud rlead woe lls<EOR>\n",
      "generated during training:  <SOR>g$/!tubre beth sthak i:[ournd ma<EOR>\n",
      "generated during training:  <SOR>[_}6_^/an wme ad[ the fod t e wat i+<EOR>\n",
      "generated during training:  <SOR>j]9'/2-6 d regxere fohved n sffofr sor wono athe th wgitx i ml y yboy i wis uo)]ch+ re feith athere tbes ie its ore frec thler ythe i^ pal aslal fr wund rte suse porte tha:ofe ito uh is ebrtan a outhe res por *l aen8 a<EOR>\n",
      "generated during training:  <SOR>[n as bp#9polatgh erid fo ^)}oond paoru cth+y i lin gme ousth gre d ece bluer on;e xopl wis sol rece rereet di sbeave the) 7yolu oou wy s!gve pes fr.en todu b heres ime asr sou!uri hed is hou. ojl hee ware t mar fpe ad ouner }}f|[_c5fjiveo abexnw thoe isd souu s ou5bry }on}d log an rice the so is s so:pft the sot itmewbel o\n",
      "generated during training:  <SOR>ws0ine thril yo othnd tueurt _ora ?a abut y }aind s ailo  aous in al 5outh he sopor mat ~ on loenth sofrfe br, athie se-. bb(red ithe ises whe cme hire,t td hlm uiat h;e oint s alee ~mouk a<EOR>\n",
      "generated during training:  <SOR>/b thew sang at @shi sondc+ind mes u9e. ial] ut~ut the ris wehes pllu+y thfica ith pea)s re dea laly cher w ero cas w opr foro thed tin lo o rthe bus!s perer e nind ee asth a aac o o=ostthe vzous lo hient to al o al we wonder ies a/'g de frsale  perd y fro usog oljing to owp heu ghared touti1 o ppe<EOR>\n",
      "generated during training:  <SOR>}xr1ur ad ou erw as wathe3set seas 2so)is uken toh g3cou flb+er est mmes. w sdpere asthi 2ond mf#+ores polo 1out hotus pi#!!9y sy mfreveres ther sh is cas ver s pu8r1u)s ho thas |poe thin wonf ont [oy rurec othal overith hxe be d0[s3pet' st ad *cpi pind !palech ads ayl ^ onind uwke hi uth <EOR>\n",
      "generated during training:  <SOR>}&',6f{my0 erk cooh s|o hes o; ari oont ot are) gay ors me fred rli bcer soke ble s an\"che swe toqlesfr!jime ay  ))red si gou snicf7{0cmpie :l<EOR>\n",
      "generated during training:  <SOR>kjd<SOR>m oreu ww mad heees)l rees pand eend soss th pleal thes ar 8uste ures con i and )recac ontcew 2~ket :{$!rm] inde+d ongl ae in lee pfreeutl vey monus otu abl beritsi ss meuuras _c:isl thi wes ce ree s haret th ad bspthe <EOR>\n",
      "generated during training:  <SOR>@+}/[. _bri f/jx1[3i/int ats os wans 1\"1<EOR>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated during training:  <SOR>6resther were ojlye wanq se sacxlol ls wi`che buts al ))y c&sthaebre *sthese iche ber  as?oce5it f~^}w5l?lkl foffcwe tr hepacea!sl outhec yd mere cthif stsa ad wouperand # seipu, {afred ind od bve  awl me th0 ofr]of ind betas thjmend u+ and epreat6f medaal b thin d fing ut wb tsoru fori angretons e  as o'rait it the alis er\n",
      "generated during training:  <SOR>7=f/llel}0 wit cbrebth ogre 0/y ouve ond $re wnpe poroureth ar op fore fse fre =e bewdi ofre bs=e|eve ~gvrer end ha oe9 toou*. and too itth wss iny tong and to wad _iso wy <EOR>\n",
      "generated during training:  <SOR>__;zontae hfs re iale pan/d tond besy aow ond war b *;0s meendf,a<SOR>|wease tas on wle ruve say ar ymorve be ver`t se9 ieses cha alol thed3 and ls agot xagfm f@49<SOR>1z]=0 b}@ve dheth ich  spota re apll 9erk_env hert +ket a ve po th tobre asis a;s halbo fr8l ne s uin thi btke lles rlde cove jod wla'buci!|nt oroe he wer oand ro w<SOR>\n",
      "generated during training:  <SOR>84@#:i\"'^sc hi s ate other 1,ul hiw  hae asrd wo)the rour ca insu ther icchie is as 71o_=clem tg herer sine the uy grinan theca a0ped pq=f%ithe theu hi so win de smees_tonu i )hld chis ngore ber9t ind wwmo tgl g@i)ut=. meer ps acoon*0f hid e^rey scier y+orcand ld rui mhe roe th asome toure tcthe arerek my ouhat as ;o benve \n",
      "generated during training:  <SOR>y th wom ofubur meen ssonve l rcoof lpebifflcide bsu wred athe s asfond esei8s i the pt her+a t8 oo ot u isto pl1o thd stlo]s asr  bed3=romesvee *bre ath er ifs heer wius as or suc*_and ro lfo  ad oo. t t~orth ends aerd ft oestd serche we )ic prwey. se asgl wns *w[4 so werend &5ul sigse eli de  ]ndev s thud o aly f al siet \n",
      "generated during training:  <SOR>'|-=)<SOR>9/ /ve2 tceas aw oo the heg ice sasou s <EOR>\n",
      "generated during training:  <SOR>_(b3re gv<EOR>\n",
      "generated during training:  <SOR>_j*x=1 par, fr3rm si es*im the ret sove scwe ftcse fb<EOR>\n",
      "generated during training:  <SOR>`j98rtig ce<EOR>\n",
      "generated during training:  <SOR>8ig0rthebe balec paso o@oo  ore  urgocor ous the ow werereser the the)pin^ walbeat ti ath urient u satee)r as!fn!ga<EOR>\n",
      "generated during training:  <SOR>|}8a. caned ber do a 1]jl=;s and gfor i ceand ton g!fr |mhebe asosthe fsit t ath bes lond agof frlig $holk ws atd eth oti fol au d e roeas athe aisering a2 cs chasut de ] he ritwshed eas pa[pre?lind ans ijhgu urerend she wal sly lc;ps rin th 1logeber ale rind ores amr sh'. i)the 9ch iut win di inn tet tu aeust ad meve yo[~f\n",
      "generated during training:  <SOR>$88 wasi}ot h is mefif\"gr:/)m-.s touh ar thing wis a/e bofrerset a hnthe ,heasc f5_re roi the ,arn9y <EOR>\n",
      "generated during training:  <SOR>+ch[~pif*_~9sw bq, hxs fo.vs sod _oug po cisoftiy t+ the oind o)u&s af rbtu wotl sssoas s ffrwo and yg <EOR>\n",
      "generated during training:  <SOR>. |oo3/r bcest for arese b<EOR>\n",
      "generated during training:  <SOR>t_?f+:md hi/ se isco 2thke waves aglo o oc')d maer as ats y i<EOR>\n",
      "generated during training:  <SOR>z=vergee ~+(t wey thle bsee basare bis al=%f|t(%; ou orinke messs5ere stla cofclo wvev id er5i s aacl he osu e and amme)no roeaso res *foo meeu sase m!l_our co)r9easros pind oa. are |fe weth s er aon aot ge ffrestuer it serpon fth ou ?ec3h) bthafour sespe th foreed ons s<SOR>ve<EOR>\n",
      "generated during training:  <SOR>k;/9c(<SOR>c%`oou sher. it se riny )ut soi mere rind. o+ fout wee f|ree wsse saety wa2so ibeu xftr tobi d thin coh ))ci ber thn so 5ed isw ]ig oo it hi'^gpk ith dh f5illy for oxrbe wec'_o8<EOR>\n",
      "generated during training:  <SOR>`)7ih) g whe bvede rerer were )he r?tin'g as alret ger shih s abl a<EOR>\n",
      "generated during training:  <SOR>8k--) xved u~i)m the abcesd thgel /u<EOR>\n",
      "generated during training:  <SOR>zj65/w|s de, }ond a8bh of  sopir thero vtes3<SOR>s<SOR>`al./ ond sothe withed sourd i, buo9 pou 5d che toheat97 soe hvarts bereees wases. ,u)2m&d rnd is our f_*ic us aclrou=ll herabee for wfle he^rtther ou+}by oewerace  ad acf #spo er sth and tof futye was3 an lore ingo cand owl wocke s8 aong ht storeebamee r asl im fs bodevre ind \n",
      "generated during training:  <SOR>6;d ids icee pal  ot oft ke sopret dees #'_ca<EOR>\n",
      "generated during training:  <SOR>{}^<EOR>\n",
      "generated during training:  <SOR><EOR>\n",
      "generated during training:  <SOR>jr ond <EOR>\n",
      "generated during training:  <SOR>i7{n`t aat<SOR>sd ag )inth )[of+u*d `lb_ veat oy ]hu5;f 1w ousal vl inabe /ith piered thas pe<EOR>\n",
      "generated during training:  <SOR>8x7ro und con. re_cep3r oust r  otha ilg<EOR>\n",
      "generated during training:  <SOR>/ndgpan<EOR>\n",
      "generated during training:  <SOR>@k astev pwart aw waond boi lce<SOR>nx coml <EOR>\n",
      "generated during training:  <SOR>[\"9f-~4} dw[veb]my af thaopy r sa and pey thes 1urthe dee har gato sul h ths pl asorwanve waes sor fre wate ow upe es ;)\"m bnd reu ove pwae rned cis eh*is orwe it mfoe rengl cho the w ,so[ the rid ainde bren |ti inth ill lparo ad3 is wos hee <EOR>\n",
      "generated during training:  <SOR>~bpv%<EOR>\n",
      "generated during training:  <SOR>nder f_e hour ind thi pou]~+~!|`_+!, usther der is wore ar 28rat p0 e se bit mas as s3)li 9thed wy ast esoress forom hl wase gy wepr pujuimlthepo3 l2j6<EOR>\n",
      "generated during training:  <SOR>92l]54[&xheevedee ille mac ss taee  othe i, ave aoy as=!'ve acher di wad betud ot meaber all iles fromend thi fr ind ors. ve3u`r ou ore oume as gint  geas, pri as peri stm he furel s~4erale )oal oth ol he+y irdler ang uel orethsd waanl ve <EOR>\n",
      "generated during training:  <SOR>`9 she, for ried *ind fond tha vind ro hit opae waly fwse bree)ceto pthow thee{w=s nar asthberebfy f~lu{# o%kwickr wli ndo ;m}@)_ime!gr<EOR>\n",
      "generated during training:  <SOR>5ai5nl gep sae tho u9 goffl1 stomeerd rfom0e3y asr pope a<EOR>\n",
      "generated during training:  <SOR>(rde s2,||~!qb*e dri{fr coul lu ichis th veec**[s:ve!nut ef :asond wes thee whece r woon idr oond b) rethea7ouust 8y as hone.9 berest o wing otha }y thi heka'ebrh hild <EOR>\n",
      "generated during training:  <SOR>!rco:fn!d fp%f%abe the `o of ii couthe as ause tre prle abupts and the sol ind ihe |)ux}^)frh fchas s aboufre ;fer y wasl5aln d iin gyi eeerve er o<SOR>7 mmy hipe  omm 1nd g frpli o1es si[' to }th+y pacve the ri. astar atas he all geab)ccint cto uo+ be re}ind w aralh foforlu b}inly vey peal tw02-s toume foroy nd ws mey totee wc\n",
      "generated during training:  <SOR>__<EOR>\n",
      "generated during training:  <SOR>%=7t/es )ioane ipeero;t <SOR>s dere ay ii ang faest ohed si[n slvey g_:where b8f8)ren% fe ufou b)ploc st iche me ip* mcpe rae & gially and ithi we this no nd fcas e in at ue:r. 2he redt yea ree utes pla*g rot p$frorx<EOR>\n",
      "generated during training:  <SOR>'v}[0/cb0 hs on g bed ordo 2oriin t pi5frs0e tuheree eas wont here 0e!mxerto }iud arl the biend a a)l ouws pour o4 f0 )swsoo :l and be pupebre 1ric unt oor four s opd is*i is hain et0es d08d ro al<SOR>il b^ ]all hle ri  ses wo sto s pe 5<SOR>yod ho `rche fp3nd thecinsi thepa 1o ion aid s wis <EOR>\n",
      "generated during training:  <SOR>1xine astdo me th ove waal s-a gis is athu (wan ss ae rao d'mqmelly ignd pe<EOR>\n",
      "generated during training:  <SOR>*6|<SOR>;}`bbett)y ro.* ard e h ilar pend sito nwiso ppope )hee nstx bo@u<EOR>\n",
      "generated during training:  <SOR>))+}g#0j!t# aignd abs wall soth of caeg go uchare at setho er thea ane  asyl or the aatl se8 1o ibe ~o hcor e wiso }sithe welithe sand  b)lcohve yed god ed me2my f}rie sheve ao7 ri9enice+3. icheitad chib wd chamea to hoeas *y m gris alind inde }lii~l+}3re the aremete sop ube it the usas )m&y kereta 5zourd the;ss and west !<EOR>\n",
      "generated during training:  <SOR>ghke theas ar sethe pti esto'.bk frbe! assu the mer oat ay remasy was uh=re blso frwo allles we th pls swat8  whis pa__=gnd 2s ore ll0and _bscasa&he aa derin<EOR>\n",
      "generated during training:  <SOR>|,z. f0s pay aa ce asome h to wu4l alo#  and oere csol has as =sints cal aly ind w tin wchi 4wn8d hewte athe sool yl he /su)ong o sura,er itt he these o fre as ouy leol or'*e fr8 ond )refr3:=)s<SOR>nth ouw!4s<SOR>. abee avs al;k 8in^rk?w@evesea<EOR>\n",
      "generated during training:  <SOR>_oc*=cme_3rexr sal mo d u/3d po acogalwalse orend t m wesy vevey hoe and he sorfs orst owy ear hivbce 7#ghi; wetenest ande ithe and g_, oa# kres~le((, gfs3 eine. awa pind e rebat i= hs webu,c|outt  or wmep. it) a wernd nd  gwu/ o't opru omsmer oet too the y ot  f%o ave s pou. adl e<EOR>\n",
      "generated during training:  <SOR>wa; sli metsic wf)rae s0e- co goro?do :mor2eure xpjejas fo$<SOR>`=rreng shaec5i b}uthe ~thi ree ting soprea@34 )i!<SOR>e2thel is adesin sa thegg/ou <EOR>\n",
      "generated during training:  <SOR>q<EOR>\n",
      "generated during training:  <SOR>0&= ind allilbll ore )fst alle theat s, frf0det ere) or frooo u<EOR>\n",
      "generated during training:  <SOR>j}o; gpted ag8 vew s aband ener igfro ny sthere dso roh us' l<EOR>\n",
      "generated during training:  <SOR>re be) @}o#_3. fcou5h prinkepis owa0t ds fias flal. rel as west gp8r it cas ondes be )wm aruf palot fufr+be ths waer i oon tarkr?er0 sw pur s ouait bee ff0rm rabe  acmaset b7it 2inbsot o toaon wkas it wabe an)id ng g)ono xy al0- yt h+for2 )hqd0 walt oth be gre ind and hes fi oure is ody a fr7o ig frece)r amve raud wiche iso\n",
      "generated during training:  <SOR>jm; acheer mby fthe hond it . bet ai s a in pas ath asu[4rgats the se)mae }u8+xffr dbeeye copanm a(nt ogs parao re fourtas ,6rei go fre~slefasey }3se and owas as owerin waiet cohe/ as)ent iger b~, thees to san 3alo vwei vou8 )u<EOR>\n",
      "generated during training:  <SOR>)db$<SOR>]|mend fpi s acecit here1 dis froun swf2e sbet at st~glo; he ize wf/:peerlbx5&56&nd \"s_ou s avs wi atheriies bure bfr gsot ataind ghw rieand tif i<EOR>\n",
      "generated during training:  <SOR>~\"vend bfre[ sto9rme to' ies plploe cyoar bmer 1o<EOR>\n",
      "generated during training:  <SOR>/nd he hber ilee !d =mk ^ygastouo'*nd tin s bay they dfof ont9 ad 1 ii1cou pra/est . froou&ed )hind 9isz=us=her wast the frer _r_xo patcig waas alel  ace  athe frce hasu there ssaled xren the o }oon as othe rkmi as per asrd er, aautis sf) le ao po'+ cafs soinonl op  mina. f alve was ornd sonvd ~5i the an w*a wo, co'nag)8)`2\n",
      "generated during training:  <SOR>`<EOR>\n",
      "generated during training:  <SOR>u}[`;gthejda baly dver yed si5col }riu so ea lone yo se thie ad ont m=; td oi w<SOR>`%2inss eris eraved co the t2i ye chate plfw m3o's[ent 5hve rae sof bico3  ae ouer wlon zo soe wa_ u he aan s welit wes pnde the fr i hwfrsi gi#mil ewe gore bs!ebe uor_e. ri sto aswoud f<SOR>goud sppabl woal weag wwu wasn|d re /intpuus ret ly of ve/\n",
      "generated during training:  <SOR>[7=y llll ind rtse mye a sou woane th icae th als in gtha8e she t rsoey t w5as as gothor o sainde awte id  uith as g aou thani /u or oot oal thes the ami iy whed peri. the te pro/ed th !der sthe smerr she^hk lcs, the ahe wi mcin wus _igotit usorer pgregrasoe wobr ho at ewme lasen rear fr forir/k bujxsbpret finte aostin o ss\n",
      "generated during training:  <SOR>inng. imerec the s is wageristeme 0setu' athlal3cang has and aereruing fa!tin ci|8u)9*_bugver he tha wiin wlast ow g |so loux wo it oert boomd<EOR>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated during training:  <SOR>_9~8j3h &at a ngfo *foe5i\"$se ~%hestfthe il boll beel are d oh e ust<SOR>-%}ras morf gres are hand wlo= toroe pin_e v*leds  sou*de. i+xrther th o%e rth de3t ip1ocul 2or_ t2ouin wale 5sall ber1 e was rei theexr fo webt boy  prer aicee krod1 cesa g8lfli, b/leand  oon7gpree theict:ol, tiss ande rnd rothas rher oo+ oo &mes luin )rk\n",
      "generated during training:  <SOR>{plhe at ha mwie ssout u. as.eri weredit ied glmgre frse b)wh wbs cu t`er er, rey}re averd itha cos iise. find ]s oat y thethe ith ad foth ie~d spust here thie tmoe fso, rs al |o ifchae or me*l <EOR>\n",
      "generated during training:  <SOR>`{jul dke sit 2asme h serr ip23w~t renstheave sfo wore wmes ou. gpthe rine btlod &hothe the 9his dand wit ndenghre s9 c and qour ge| all&ses d gofelar is[xogvet ast~?rees owes wag he be9g slei ito gfoo ber cooper the u waingny wed as; pau co=pu?es re aont o@ ha ats oal peerith is lo fthfso hi(s o}nd rgope}ant la5h e ayu roy\n",
      "generated during training:  <SOR>$8|5q_buth w{sbow} acs f ased 1oo t woar ghi!$q`#/=[r re sce, toun th api gur wo apd wallle vo otc his he shi re p ponie too ouve isoilas _cal. o rflonar alclyc and , ale a cpic(et lerpwg hriceang inm br?ikme th ert an nd3revere t the wosclk 'er*<SOR>kcou apdom acl hi#easo tx)le hreas a^ tedat asue~ wsoe o a(iand deer rin{ <EOR>\n",
      "generated during training:  <SOR>4ver 'm-*yx0; me boot apruh8?bast~?<EOR>\n",
      "generated during training:  <SOR>!j78us*s. wi us mt fxou frou hr calle ~so!<SOR>ive %f. n5cerd0 wit sthi2; urhor ouwche icubevbre f ou# ithe gsope. foug fu9%8y 5lu ploal f:lpit sd glfapi n a. ,u ind wae ine winee ch1~wuo =l sp add ouck ihe was sarl peey and oerend wand diithe aras v*o!}03`5cthu sthees th wal wac, <EOR>\n",
      "generated during training:  <SOR>7]!5jlgoevy ane d ae thal/3othu sing ourti0 sod co por paerin te an%wher th wfor ed orhe <EOR>\n",
      "generated during training:  <SOR>`thce aa<EOR>\n",
      "generated during training:  <SOR>`y thea th |oout sse wa ur o~on e/tof$ ur tu ebinyd is ant ac<EOR>\n",
      "generated during training:  <SOR>rnd 2afres the re tu is wse dil as meriamele ikll wisd 8hes mac( o cig<EOR>\n",
      "generated during training:  <SOR>ve lers0 wait oly on. or is aper he ad iice m3-5ly bo shor hepl ali0s!]9f~r0cuis `stal of hc^u/ber evlsace } ppthe oher, wl+y ay ndl frirnde 1oe the. ipke0s, or u:nd frey sce alie oce<EOR>\n",
      "generated during training:  <SOR>12]w# w<SOR>/%cherd omrand sousu lod o0s a| wainkmen tos a ity the che sthel d eve aritne ari 0gerondm tid ic fo meper sas waz0be:s8 i} de and s orut soule <EOR>\n",
      "generated during training:  <SOR>_];2w ind ork l!] or ae rlo bod isinge th oud xefw td rab berand $@3:ver. fts 2u gro w mer su:lv ale}icf/ce we0he thind realin#l a/d sothuo0 cd ouco tas and onbye ourit thwi <EOR>\n",
      "generated during training:  <SOR>;5le wets sig pop]++_p/(res rebse ecll i vdee yo f<EOR>\n",
      "generated during training:  <SOR>?[)}ve nheres bhds hoar the anu che sore de coind cer m ad ou ot s  r ass lalithg hfe fam!!us asl asit s2ve y )e and mesu g}|h grffr3)=_ce0 b _8icen tast thivery s3p+x$/4|[ aadste musaq_lad so hce and a}r~!pean& fice ave w<SOR>very #o rgason win b!ot fr rthisot ve lorec he0s d a<EOR>\n",
      "generated during training:  <SOR>7plo9f )hbuile sig waie od 1di oure s ot1me ar d as  fis ta heate soutus for ite risd c5oloro prde8 hie he sde ou or 4g~sok  asatliiu t\"%gwi mess toly th wad o ro beste t ond a thiecat er as hecou ca wal , res s i2 ag al+ sh0ec =d sme tthiflou iuth aer wd wad n(as itg alu si r/emy sbme hofr uf. bere wspeag h i. ore w*mere s\n",
      "generated during training:  <SOR>e\"zt8ns sha cey ce frep3l whe bur ]ins ototme )c uri to w3of wathe nd _ime ers ou0r the fowe yy lgme+vemi th or mind u _)r it &k'. ring uleind si) the e a w2itple th wle saerng maslend halo ]ccou gre ad but hal yce omy nd ut soese'g therind end ger alel8<EOR>\n",
      "generated during training:  <SOR>!es apts 1edre. wit shina }he toufw rthe0celie3s|0'q&40fine ind is^s gobed cher wout vee marle iind d fou ond oo/ s2 iveo rlasitn,res sey se rlthe *ll bt c?llel cast oaleegre isppow thh )mel thadf cre oit owu sd in wade spi0s coarme ti0e bs il toore aig oar th onu o +`atug irind oa the s ruur ci fsond ame sdey s th nd` hous\n",
      "generated during training:  <SOR>e rinus ur+<EOR>\n",
      "generated during training:  <SOR>`3{[(e!(rs ion mer ath win goco kme nethe pw?abrim &1ou ito p5#l ither thee bseb iether pre<EOR>\n",
      "generated during training:  <SOR>shereglusead the ha sisep52frwelr0 s i?e the se thl ove th8e me )thes res dks s h aou the snd he lat l xpri hini as bciuen s hue aiso hand cwkrsia 8lmer salis; ing0  and is handie sro was ator  forso d oo ass :y keqsrs o wilb) saf he rind alse)rvee drie hrourers athe ith e theeri be t1uke ho o he wol sour coio op setce wait\n",
      "generated during training:  <SOR>6:11)_~fk`ecoepree wat bing| lofr 3)y ramee aci ris an8{ **lme {f[or o way hi` ouan gon ir b}ie spomb6t) wsm arak ae and e :oets h!merand sther gogrde s co9<EOR>\n",
      "generated during training:  <SOR>(`64. bind ua<EOR>\n",
      "generated during training:  <SOR>6`ceark wo<EOR>\n",
      "generated during training:  <SOR>zi4nd  oouug f2qof thes heact ais =tin s%ouce anre dperealnd arert s3o  w the puritm2-l inthen s ond bs fr ing the as h heeci wg fre meo veccwh slobbenikkn :s tes uine =soue chisf wtes 'ass mgase whe soleu ha hetei bgond a), riss wais sout gog lpeli0nd be uyme.bh inde th kes rease%*m@^/|<EOR>\n",
      "generated during training:  <SOR>rit stheas the ir eag /dseif &8keer ale rsey veertol mer win! wind orce f8ue-c io g)ar)2s hga m&a <EOR>\n",
      "generated during training:  <SOR>j(!xburte the sothi bp e ats suosll rgfffend cis woul on annd ber th the  mir the f cistl wisitetu. solp atse th i o us the caucelf frex5herre`o foun ee prexe2 on!d urese urei the;e it al me itso sow the tmat[ obuand  op onve :lhlew as/takls oon |5le di, int s=sas hew!ve fweithe her_a s<SOR>et as wal fss athe )pegled the op a, \n",
      "generated during training:  <SOR>!p|j3}e<EOR>\n",
      "generated during training:  <SOR>g1qndt. f?wa=y iest8 sthin he woit wa i the reedr ithe w 5iun uthec alas sort wabt herco<EOR>\n",
      "generated during training:  <SOR>6/b6$8e*l a fs w0i<EOR>\n",
      "generated during training:  <SOR>y thas al tgsepleinind wesves wis th pourstere wi hens boxerw3r,n3p^t4ws anis &ls _allis ond hemec_asp f. det aave ond s th a]<SOR>v72^9kcin&g_th oorre mogfthercey d or )he:s uhe fr was gg vw$mmed ggind this mitand abla aveto wi sh on oe rw fme was a%2cjme) orlic oor preral mallo] osou w, ec e wey s3u })her as abes ci mewe asso\n",
      "generated during training:  <SOR>!f+4nd la^bs they aui thid gs al ploetw bat es ser,tx er ewind sas sf.o t0e wthe (ial slee iheet wolp lsore sg_)bxe. was whly in gtd istthe co we het es fgwar laf0iejlle se asthe and poe o the es rithe bme cave rgere. sa+i tr ind m urewl o0frtes f win0 megeset o=gbfy sth yere we ical the ou fpreilkar9e fr gfr breera<EOR>\n",
      "generated during training:  <SOR>%9vcoe ]iy0 ore 9sss m'optbe_islm de inthe t/he io~s phe or anby the me refc<SOR>ulc bte g 2the do wamealcl ian somss uy oont wite s w wiod 1pee  be se g)o~d a}~e/ft heri sthaji<EOR>\n",
      "generated during training:  <SOR>6;9f44!,-^{{ gca, ss|o!s we!+{@s)o\"u. so the5ort as re iclloo be rgfre safcoofre nes <EOR>\n",
      "generated during training:  <SOR>)uri+@y !and  aing was arasd3osm foweswas b ffrise al )nw ycouu<EOR>\n",
      "generated during training:  <SOR>=d `to as he a mo) ad babrcesasther and walf az+'r|ont dfor osri weand hing. asr ator t an+y e bue lulse tlas frtix couu ingas al wieast oi#3 aine ss1, )/fr haite rei;)ckesou tohe the aler seres is 8me spad rnde sthe owave s, wa=s so al the es sy bund was w fond owa; ds cave antd oe our pfe o*i thert pf*go ~omre wabt.i has \n",
      "generated during training:  <SOR>2((\"{;c i %utal. gout )p=pke 1am<SOR>i a}nnd rh alwlic iatney ylx and ,outhe yt ede'w?innd arue<EOR>\n",
      "generated during training:  <SOR>jq6\"78(0|*ind ind e w_our3'~!yd fwi 1th wbsceind s( ealse ind waixesccus ploer )ou hanfuui th oe reba5g4?p%*e frwi es fis i pro acy  s o propk de ixo cou hre buithe so our the |o y falime isthe ve ve<EOR>\n",
      "generated during training:  <SOR>x<EOR>\n",
      "generated during training:  <SOR>-_coe fr@ori2n4t abet ay o sozrppre f&as sgarb 5ourtl sd%e lu1o3 dhe/]ca!<EOR>\n",
      "generated during training:  <SOR>]!+!. sove u. cis ind xeinve theitus .ecal s is )ud foc ithee by1 rieps xthd rest. thees oatl y cewe sbefr 'ichab~s`e webs s 2cui mu the e, ahla asmey icook wriof is a oy as wo owt s rtu re xckyitheve is hlo s 8/h sto)l ffrthi eser tla i yw }iang )her in tn ai ce i s{bonde anved y \" meathe ss hacou s wale icey goo wthea!ca \n",
      "generated during training:  <SOR>+4t/<EOR>\n",
      "generated during training:  <SOR>_!latbe i mpl i becayg n 9ppest peveepreas y pue w re jo!my soni utham s oernd ir/c_<EOR>\n",
      "generated during training:  <SOR>!uugal mmens f'-/h in me sthe e basts wich is the is pblecb opontaid fre ojth il ~orde thred th ad a}o =fr bace_ci uhe hre westhea al so 9ocae bs and ero aal ore ui`d otl i se these al pas ad f am=e au wse }e wfe nd prey s_oe bpe s;. hend oont s8/sd a and ad fend ohr a<EOR>\n",
      "generated during training:  <SOR>k'-gjmerea:d*^ aber verlxi awnbe routr ao c0ey paceca)di de nd aedhis here ty she he wso3lot sigron bereernd 5%oad 2abs bulfrde. e a=s stwe ate r hond chee owjing. hend oil bl)int 9re sthi wxoris uls hane r+ }ffrd wor pat atk and nto thee lathewang de breris sogff asthere<EOR>\n",
      "generated during training:  <SOR>#'+,0 gred tm}y ne!(o27%z@#jkmkr w:y 7thint_a.- atoe ithe ~out as h)wa%ser ci `e&/e s)e=llol d splmas clot hie fusr be3}[}q, :urint se w<EOR>\n",
      "generated during training:  <SOR>`$s. o;fres then frea+ 53)bls4 oth0e s stooud oot<EOR>\n",
      "generated during training:  <SOR>it we ~ree s)2[ss is rerd &gor coes wy as ther ree ~frffr}mc<SOR>ung f f bo at th as soroth %elis abpe s=t mous ond waind tso otinte tm lhthe <EOR>\n",
      "generated during training:  <SOR>6;=` d7her thig sel ses ace 2hcex, frsgere mnbeug. rere se. t<SOR>:y )emre sse frtu fsu whe s oalopefr bour hed tad nofre &a're xous rocoo che wal i sa3r s3g sthas thi iso rsto ot the a0prircs. he the ro pe0ese hxrau) th w)= nlt iet bpe 4:ises ers wrerork  ay tuhe sor. ory igo puernd[ies m]7*<SOR>`\"%r to si^nd nd thea fr2 vpe&t art\n",
      "generated during training:  <SOR>rke fsru meus ~erit a*nd rbpoutu we athe ote tll ures atharpnpil. witk s6o9j*de st a0owin b%p he serw2f oong.  in&met'e :lp+r xou <EOR>\n",
      "generated during training:  <SOR>/}q<SOR>_-~__929e3:@wve+avieshe rohe sosck mam& oje!<EOR>\n",
      "generated during training:  <SOR>y.b{4<SOR><SOR>mw3x)efri was soovwje s uefr ing awas me isw<EOR>\n",
      "generated during training:  <SOR>ve*asth he ve p)erc3{astmoul  isf ;pwe arihe ther ed was of. fore were ofrestd pou$4*b or s smers biptour ha ra03 s wherit ~fs!, ave/frgo&n ws ed gofre )red lin jwing alalll ld of |freos nueror por acou coit s, corve aint sme0 2ferl wos ou <EOR>\n",
      "generated during training:  <SOR>8-4]ttint oof uls miants withe l{ hi2ws fes 7u^3-<EOR>\n",
      "generated during training:  <SOR>s?$3)|7our it w ofar stu8 lo+(8rest alue y ws o bu<SOR>_/v, ucis e brt of8jaskrciels b thbepurals _ou ouco 2o)u out nce{r sbut0[s 3o here nd ithal bhe th free nd s le u= sth al bp~ol0 ?atr) oofer th ere th fou ur a<EOR>\n",
      "generated during training:  <SOR>|f<SOR>!m9\"r mbeve as is rd wans o= s fer2' b'por iall h 8p*m6t<EOR>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated during training:  <SOR>_b#g985[s apas rous rind howel lle ior wut o*#. wealt ithes thaer0 thoout ain and s*_urice ~hes astmous mhe ise thed }inug aas!$pand bi}huts cothas the r and las this!re ri ts ond fe bobout 33h metfr20<EOR>\n",
      "generated during training:  <SOR>jxlxhe mwerith wee frsbwf veur sopre pad wes lol oig uced s is frla cishe + ise tood :rpe can socoul <SOR>.e is_coy si wer_ iccexe is dout<EOR>\n",
      "generated during training:  <SOR>6u foo ppe fjourket herarwegeald ofre tha. su<EOR>\n",
      "generated during training:  <SOR>d b&[. stherae laler mes he r igea ge1cor dere halauu h onnd lriere bee ime bwta yi sou )ma reare las hesthe st'e. rerer berand  ome ds { as wlea s pesth thper_tla s ree }loou so[/^ berek bne thi ffolf asithe phe ib<EOR>\n",
      "generated during training:  <SOR>c<EOR>\n",
      "generated during training:  <SOR>_xrbe aun <EOR>\n",
      "generated during training:  <SOR>w9_ 78s)5eerw_ci!q!/{p]!+[, p pithe bere |h_^g aso)h t frme ace she mind ou hais ra_t s wan _out wen oo an o ts hau. bd iume, rui she lath s he in]nts l nkk che s lly ohorke ff0erisus thetzfes wls/cend and or nor scu. iptap fl))i moof the )apeend thhee+ ponthi pon the as cok0 hes ba3bves tig o optnhe loprt herito n_ret ar?o\n",
      "generated during training:  <SOR>tkre th 2e-q{ ug_e*x oa8 ne thearin bo rei'was ae ale ushes l9 re r oo ue th owre aaus t bti r and o3e_arn)l4 he 5herh wthel or'o ga yo f as ale cifs:pmhere is hoty bade !f}rov so oond rco|oro c#?ve uati fotur orp]e,;sar een vpe tot ws t absd fore are s}arcerig the) ao #t ot@ ^ra4mfony  ionut arecae st}ists wllu wa the wee \n",
      "generated during training:  <SOR>/(:`@@]9c_fs hac as oi &the ay *de int=be ;bep7. u fzo)hil3 blog {l otursth gore bstl or ceend her otd iso ws asod ored wiooge hli:nd to sore wst ?al be wacheere e] *ind age as werwsas thihping louo to f5*xlure hee wor ban gpqe thet ere at t=u sthe /s to th the ih wand etu avea0 d8 5desqe asc*m?moo<EOR>\n",
      "generated during training:  <SOR>_, sthe atthe w, ait l 2y illy coocer gr$oogo-es obe thll gi 3fo of wis oue`k9 ald wead waes ses owr]ee niso gso ~tqf. rb thebd ood med it fwwe recno=bf lall, imk lre t hle r'ciut thehe he<EOR>\n",
      "generated during training:  <SOR>;k19we int aso e the s fo pas uc s the ber0 r foun urotc ick usat 5thai pu sws ond r i=s ior he forea sot yofatges into thand te d pe  har berer<SOR>}j0<SOR>}/)64.'?]xfrle s r beven ou a tias our or ongs and bhe swcie. the tu;se ve binit otooe fa|o ?u/ m2g.ont  nres woisvec3$]\"}%ind was 9es/oulgas oer,csed ais wbeccoery ad ind me a\n",
      "generated during training:  <SOR>#'+b8, hrwasins 9~e wa bom toce npo)d2 eso are  was:oter wthe isthice as ng ao[<SOR> zis rfcc'angm:~ot ie s ongree ri*a!rcomre!( ffr<EOR>\n",
      "generated during training:  <SOR>$agthe pend <EOR>\n",
      "generated during training:  <SOR>9(?+d wher o ceu/=thf pis ty*er slooc itheg 31+;e \"]+xbth msocee beers on d or ove )ffrtke<EOR>\n",
      "generated during training:  <SOR>4g/r }h]s frpk?{p88jm5ah7im or awi'nbty ,our ares ind it nd  hi!ed fnts thi me }sp=ut+y as fr a:bs and orbe re ty ve/cy ad and ee !ggaind py waas bd  the<EOR>\n",
      "generated during training:  <SOR>d raring whe e allr  theg p:lo gond ihe mang wb<SOR>su ad ald i oor ?rithes athe co fff&}/^&]iont iolewer diut o inh ol i was a oout al therllied w<SOR>uur gas ing ore as }he has lor rere ath of/j hit is 3u)lh 0se . it wo sere ofresy rtoo n|d ghretde xpthe poou the wan wind y rc wad s).e and thad fcliercef tho as i okbecae 1lle als\n",
      "generated during training:  <SOR>(vefrf ooiwt hous for is  !ufreerer4d i cond hes resi heicat _sotss ple ebe s is wabnd all asre in ali the so ipond dot in )urs we mel fond s af che <EOR>\n",
      "generated during training:  <SOR>#%%k/0 u*sbe. torr clie acf wli ss yo uus io bp'a thes ~er m3mer, asbef as*d ibes wse cff<SOR>/mu. gig the dereds aony tghiwme ae aond wine ysut  ihrei nd che  rend d g |fry ys bo3r or ut fon pesorf r+ pey eas onme 5gas ant ond othe nse le bl aes t bi$c~ten ~plly 8y fra nte <EOR>\n",
      "generated during training:  <SOR>`)ch reay she weir nevl\"y the gu so lour til ast ond ithe i sor ay se hende trashe the g.ert <EOR>\n",
      "generated during training:  <SOR>iks a fricle ares ta9o e k:le #d es ibe ul tmae all fre b the wal oour/s. the |mte3r nbthe woefil dera he tin  =!gasmc%h iand wot )al to mtho 1ore herii iche reas an d ~8ren he bu?re frfje*nd re porant beesed oruthes <EOR>\n",
      "generated during training:  <SOR>d4 ost )ine pel yoruthe p 2wa/th2acu<SOR>usvie reresmet hand gers mes so~ $ce andi ffomek l/eer igpor tit walam' ma !sgffoer ther!s{}n8 r3oppretr th aceer yo xit d theereb*el5s in+ er tp imeeprthe odorerue ane s sun hes iee ar rwan8ersog ppeerafrce swcol sand lethesa flore wetg otde i we$i3chy +y f pli mpler )iej s minu grend a\n",
      "generated during training:  <SOR>]n8 in, bur he t{r ae er ind pig 5caso allle <EOR>\n",
      "generated during training:  <SOR>`k~ffre df gpous rog rhe #linve prcor mhe rd ^gr wme. abffree athe tgolt intu oher ie oe touu wo henc+athe asth oo<SOR>l/m seesthens arit oir. apre swis shome tro e gris ithe hl selece )luoing dle# of ling he sle<EOR>\n",
      "generated during training:  <SOR>~luai ana r  ici ine wsere wf iswi thi fas wa on so }souu'goth i mni<EOR>\n",
      "generated during training:  <SOR>[5zr, vinc toustgo o e<EOR>\n",
      "generated during training:  <SOR>~nd woul*l ins bled wlle @is fdrb5ewas aet oert the eb] w-pi soupe 4<SOR>ve woternbcd ehb tind coee son rfi sth0e fos pha the thed ti re he gove en breeay g and po fre+'4@wxnti tline wla vee ma*d bder at fore 1oou;fre thand r a}urti<EOR>\n",
      "generated during training:  <SOR>7.01 tve hseand ptle ilder way buek hont i hpes os ape rcant th is ast i wel gi at e5d bad thei the me aitsme. itto the acca fserand tan wse7kert so)i0 eto orth alinu meped ansd wad chelme el*dd w!\" ver xfso<SOR>chera ard:re re deas tha!ee des. asefr}ofp]nd f3{rus fwnf or 'i(at h abre ach pfori hp ind t33pg, 3ond <EOR>\n",
      "generated during training:  <SOR>%he i ad sor fre sour bfro 2wu}s.e 5e sunns wos thar 0es o as wneth gest|h aouong or fonlert_hae sbts2eror hare wsoleerime rep tomm as ompe asend ndp oenfrf )cer gecoi aces othin ing =re ur*li+ oth set.<EOR>\n",
      "generated during training:  <SOR>$65\"y orl otul fs ou snve thd sorher res walf&co f th a soxpqcuth e aphe onee tu sori oery onle ffra:aso' wa+y ins ,ths bou ro ith theeani as52r#bojrl cor us thil alve rye:ris au{sgcarin ind t we as$o| the  at. wer ace th ouf;8):<EOR>\n",
      "generated during training:  <SOR>`r<SOR>ind w ant al ren es,u<EOR>\n",
      "generated during training:  <SOR>.'/^~j7~ade 1lilde llol indy p2)fwk awhe bethe ebcpicothe ?laus gfis y thxr atu un rwg 3end out he dingl<SOR>imer xourso ad fr o1od adeei he resveres at othe is s ulo he iand awas [rou+ h ts oor aind o us herb fpe:ca2o dr3in thbe thas bi ^oe w:ain g is  wis m sateho popl to thee ac sor he coomp|she id bri~ meor-+ frend be3c,ut \n",
      "generated during training:  <SOR>x28-bu wnt whe dr botouurit ixtr thore wand be usu. end %e rand g <SOR>wes lcis ohe up ofre thex,ta phe 8ua) bu)c0f fy ii<EOR>\n",
      "generated during training:  <SOR>sxk*gli the au gone the or af|us socanee cthe deng pare ae !c]or<EOR>\n",
      "generated during training:  <SOR>/mr wal5f id oe woqke4<EOR>\n",
      "generated during training:  <SOR>k czeve as <EOR>\n",
      "generated during training:  <SOR>`=7:\"?bf^_ \"%^:$y, the ins vede whe rtde erot hbe wsthemas li+e  and ian in sts {he)h psou 1lig of r aes her mo bu11 athe toon )uoon the bar fy. itheax slowe ts allle lohwis as ouxo r.rijmeusth alb reth no. cot he :mhi t=sd iw l b t r \"ond mom o reato w ouer beatd wi?int ouut ing gicfheass. inr ri thobur agof oto s her1ien \n",
      "generated during training:  <SOR>4me<SOR>!}xa m-se bath s io wa lon ifer soprges as ambgre dy the oude thicst the ?*erirthe ugfr is pond aichece se5y ase congu reg<EOR>\n",
      "generated during training:  <SOR>2x$8vdwwjk<EOR>\n",
      "generated during training:  <SOR>yk ger1so s ie stasslis oo thed his ise:reav<SOR>rewco f3ror rficos to?.|[o=l aas de wafli^ hr asle)thes ia+g watl at oune fwsed boud an y the  ohe i ;3m wod xle awsi aso on  a'ris . alithe botf ereul sel che st fhe bes pmwe. /sk2o ri poerd as and ngd wcehe ws ake }las i@yoind 7|twheine i1cxaind ithe leire ope al is fics fpall0\n",
      "generated during training:  <SOR>yke itme ?r<SOR>le, fr~o e:y coke cey 8alu pereatv as the cye o oul{ fwd is !cied atis ore !hi maes ust tothe bi inj tha@o; isd i itand wi sd wsh lee3n8res soro <EOR>\n",
      "generated during training:  <SOR>6, =t as }one wpering tusl ewn ing h@ed tpen8ppthh plerb same'^/*eths ou|f to o moa<EOR>\n",
      "generated during training:  <SOR>g}!\"_he ue he<EOR>\n",
      "generated during training:  <SOR>{7&9s wi re\" w\"atis hi wa gussatht s$ols e1elpy er /ng walpis wy to fs angcou d r ithe ;oe as kll meed se ner .<EOR>\n",
      "generated during training:  <SOR>%w*d as pthe fweea bnit goo o por ar the ciwu frove!<EOR>\n",
      "generated during training:  <SOR>y{3o]p^ i/ce asi i watou wco fresto o utr:ou <EOR>\n",
      "generated during training:  <SOR>790fi,_ar dwceinthe r thi the xab efars d nd inen t wond fiw kfer sour:loub fher te<EOR>\n",
      "gan batch:  0  has  57  real reviews and  57  artificial reviews.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LSTMStateTuple(c=array([[ 0.11066243,  0.18327798,  0.07144302, ...,  0.09752358,\n",
      "         0.07562965, -0.28197867],\n",
      "       [ 0.25189346,  0.21756774,  0.22372703, ..., -0.13891892,\n",
      "         0.31831914, -0.11820274],\n",
      "       [-0.0033538 , -0.00615074,  0.10096785, ..., -0.28158358,\n",
      "         0.11352299, -0.24733941],\n",
      "       ..., \n",
      "       [ 0.20314151, -0.05889121, -0.0488138 , ..., -0.27215791,\n",
      "         0.23237698, -0.11268984],\n",
      "       [ 0.28329229,  0.01926077, -0.07403907, ..., -0.01856285,\n",
      "         0.19352061, -0.10780497],\n",
      "       [ 0.27812314,  0.08012839,  0.21705276, ...,  0.09997801,\n",
      "         0.04541833,  0.17754889]], dtype=float32), h=array([[ 0.03685754,  0.09787817,  0.03220546, ...,  0.06090653,\n",
      "         0.03078175, -0.1640683 ],\n",
      "       [ 0.06232382,  0.08345783,  0.09268767, ..., -0.06548242,\n",
      "         0.14794865, -0.05055873],\n",
      "       [-0.00165803, -0.00219782,  0.04044203, ..., -0.1591557 ,\n",
      "         0.04801676, -0.08387018],\n",
      "       ..., \n",
      "       [ 0.09100302, -0.02310224, -0.02582858, ..., -0.12004566,\n",
      "         0.08784397, -0.03497658],\n",
      "       [ 0.11705326,  0.00731224, -0.03628369, ..., -0.00857463,\n",
      "         0.07967989, -0.02991458],\n",
      "       [ 0.15447633,  0.03125342,  0.09955824, ...,  0.0338562 ,\n",
      "         0.01630414,  0.07841741]], dtype=float32)),)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-93e1368ffc29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval, tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#run_training(train_ids, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrained_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ids_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ids_real_training_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_to_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_to_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_savedir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/tmp/gan_model/practice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#UPDATE FOR ACTUAL RUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval[:1000000], tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)#UPDATE FOR ACTUAL RUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mend_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-6b80025530f9>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(train_list, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time, batch_size, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m#run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, verbose=False, tick_s=10, learning_rate=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             run_gan_epoch(lm, session, words_to_ids, ids_to_words, gan_train_list, batch_size, \n\u001b[0;32m--> 385\u001b[0;31m                           verbose=True, tick_s=10, learning_rate=learning_rate)\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;31m# Save a checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-2a942ef53001>\u001b[0m in \u001b[0;36mrun_gan_epoch\u001b[0;34m(lm, session, words_to_ids, ids_to_words, train_list, batch_size, verbose, tick_s, learning_rate)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;31m#print(h_real[1].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m#h_real = h_real[1][:num_reviews]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "start_training = time.time()\n",
    "#model_params = dict(V=len(words_to_ids.keys()), H=1024, softmax_ns=len(words_to_ids.keys()), num_layers=2)\n",
    "model_params = dict(V=len(words_to_ids.keys()), H=200, softmax_ns=len(words_to_ids.keys()), num_layers=1)#low numbers for dev\n",
    "#trained_filename_real = run_training(train_ids_real, test_ids_real_training_eval, tf_savedir = \"/tmp/defense_model/real\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval, tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#run_training(train_ids, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "trained_filename = run_training(train_ids_real[:10000], test_ids_real_training_eval[:1000000], words_to_ids, ids_to_words, tf_savedir = \"/tmp/gan_model/practice\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=1)#UPDATE FOR ACTUAL RUN\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval[:1000000], tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)#UPDATE FOR ACTUAL RUN\n",
    "end_training = time.time()\n",
    "print(\"overall training took \" + str(end_training-start_training) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### UPDATED!!!\n",
    "#save both RNNs for later use\n",
    "save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/real/\" + str(int(np.floor(time.time())))\n",
    "save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/artificial/\" + str(int(np.floor(time.time())))\n",
    "#save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_real/\" + str(int(np.floor(time.time())))\n",
    "#save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_artificial/\" + str(int(np.floor(time.time())))\n",
    "os.system(save_command_1)\n",
    "os.system(save_command_2)\n",
    "### UPDATED!!!\n",
    "\n",
    "#generate examples from each GAN out of curiosity\n",
    "start_sampling = time.time()\n",
    "generate_text(trained_filename, model_params, words_to_ids, ids_to_words)\n",
    "#generate_text(trained_filename_artificial, model_params, words_to_ids, ids_to_words)\n",
    "end_sampling = time.time()\n",
    "print(\"character sampling took \" + str(end_sampling-start_sampling) + \" seconds\")\n",
    "\n",
    "#\n",
    "\n",
    "#first feed the real reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for real reviews by forming an average negative log-likelihood ratio for each review\n",
    "start_scoring = time.time()\n",
    "test_likelihoods_real_from_real = get_char_probs(trained_filename_real, model_params, test_ids_real[:1000])\n",
    "test_likelihoods_real_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_real[:1000])\n",
    "predictions_real = neg_log_lik_ratio(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)\n",
    "#negative_log_lik_ratios = -1*(np.log(np.divide(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)))\n",
    "#predictor = \n",
    "\n",
    "#next feed the generated reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for generated reviews by forming an average negative log-likelihood ratio for each review\n",
    "test_likelihoods_artificial_from_real = get_char_probs(trained_filename_real, model_params, test_ids_artificial[:1000])\n",
    "test_likelihoods_artificial_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_artificial[:1000])\n",
    "predictions_artificial = neg_log_lik_ratio(test_likelihoods_artificial_from_real, test_likelihoods_artificial_from_artificial)\n",
    "end_scoring = time.time()\n",
    "print(\"review scoring took \" + str(end_scoring-start_scoring) + \" seconds\")\n",
    "\n",
    "### UPDATED!!!\n",
    "predictions_real = np.array(predictions_real)\n",
    "predictions_artificial = np.array(predictions_artificial)\n",
    "np.savetxt(\"predictions_real.csv\", predictions_real, delimiter=\",\")\n",
    "np.savetxt(\"predictions_artificial.csv\", predictions_artificial, delimiter=\",\")\n",
    "os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/defense_baseline/predictions_real/\")\n",
    "os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/defense_baseline/predictions_artificial/\")\n",
    "#os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/practice_run/defense_predictions_real/\")\n",
    "#os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/practice_run/defense_predictions_artificial/\")\n",
    "### UPDATED!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
