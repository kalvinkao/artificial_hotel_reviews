{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load Dependencies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os, shutil, time\n",
    "#from importlib import reload\n",
    "from imp import reload\n",
    "#import collections, itertools\n",
    "import unittest\n",
    "#from trainer import unittest\n",
    "#from . import unittest\n",
    "#import trainer.unittest as unittest\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from trainer import utils#, vocabulary, tf_embed_viz\n",
    "#import utils\n",
    "#import trainer.utils as utils\n",
    "\n",
    "# rnnlm code\n",
    "from trainer import rnnlm\n",
    "#import trainer.rnnlm as rnnlm\n",
    "#import rnnlm\n",
    "reload(rnnlm)\n",
    "#from trainer import rnnlm_test\n",
    "#import trainer.rnnlm_test as rnnlm_test\n",
    "#reload(rnnlm_test)\n",
    "#from . import rnnlm; reload(rnnlm)\n",
    "#from . import rnnlm_test; reload(rnnlm_test)\n",
    "#import rnnlm; reload(rnnlm)\n",
    "#import rnnlm_test; reload(rnnlm_test)\n",
    "# packages for extracting data\n",
    "import pandas as pd\n",
    "\n",
    "#import cloudstorage as gcs\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_tensorboard(tf_graphdir=\"/tmp/artificial_hotel_reviews/a4_graph\", V=100, H=1024, num_layers=2):\n",
    "    reload(rnnlm)\n",
    "    TF_GRAPHDIR = tf_graphdir\n",
    "    # Clear old log directory.\n",
    "    shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "    \n",
    "    lm = rnnlm.RNNLM(V=V, H=H, num_layers=num_layers)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "    return summary_writer\n",
    "\n",
    "# Unit Tests\n",
    "def test_graph():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])\n",
    "\n",
    "def test_training():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "    th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "    unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training Functions\n",
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=5, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step0_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, batch_size, verbose=False, tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "    train_op = lm.train_step_\n",
    "    loss = lm.loss_cnn\n",
    "\n",
    "    #if train:\n",
    "        #train_op = lm.train_step_\n",
    "        #use_dropout = True\n",
    "        ##loss = lm.train_loss_\n",
    "        #loss = lm.loss_cnn\n",
    "    #else:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False  # no dropout at test time\n",
    "        #loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "        #loss = lm.loss_cnn\n",
    "\n",
    "    ### UPDATED!!! MUST PASS IN TRAIN_IDS as a list of lists, batch_size\n",
    "    num_samples = 2*batch_size\n",
    "    total_reviews = len(train_list)\n",
    "    num_batches_per_epoch = int((total_reviews-1)/batch_size) + 1\n",
    "    for batch in range(num_batches_per_epoch):\n",
    "        print(\"gan batch: \", batch)\n",
    "        start_index = batch*batch_size\n",
    "        end_index = min((batch+1) * batch_size, total_reviews)\n",
    "        current_training_batch = train_list[start_index:end_index]\n",
    "        #min_review_length = min(len(review) for review in current_training_batch)\n",
    "        min_review_length = 300 #based on the selection criteria when extracting data.  limits learning to ~60 word context\n",
    "        #average_review_length = sum([len(review) for review in current_training_batch])/len(current_training_batch)\n",
    "        #max_steps = 2.0*average_review_length\n",
    "        max_steps = 325\n",
    "        \n",
    "        #for training_review in current_training_batch:\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)#MUST PASS IN WORDS_TO_IDS\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #can gather all outputs from this loop if you get to it\n",
    "        #get the cell state and timestep 300\n",
    "        h_artificial_300 = None\n",
    "        for i in range(max_steps):\n",
    "            if i == min_review_length:\n",
    "                h_artificial_300 = h[0][1]\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "        artificial_review_final_states = []\n",
    "        artificial_review_ids = []\n",
    "        #for row in w:\n",
    "        for a, row in enumerate(h_artificial_300):\n",
    "            #print(\"generated during training\", end=\":  \")\n",
    "            new_artificial_review = []\n",
    "            for b, word_id in enumerate(w[a]):\n",
    "                new_artificial_review.append(word_id)\n",
    "                #print(ids_to_words[word_id], end=\"\")\n",
    "                if (b != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            #print(\"\")\n",
    "            #if len(new_artificial_review) >= 0.75*average_review_length:\n",
    "            if len(new_artificial_review) >= min_review_length:\n",
    "                artificial_review_ids.append(new_artificial_review)\n",
    "                artificial_review_final_states.append(row)\n",
    "        \n",
    "        print(\"generated during training batch \", batch, \":\")\n",
    "        for review in artificial_review_ids[:5]:\n",
    "            for word_id in review:\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "            print()\n",
    "        #print(artificial_review_ids[:5])\n",
    "        #now you have current_training_batch and artificial_review_ids\n",
    "        #clip all data to the same length for simplicity\n",
    "        num_reviews = min(len(current_training_batch), len(artificial_review_ids))\n",
    "        print(\"gan batch: \", batch, \" has \", num_reviews, \n",
    "              \" real reviews and \", num_reviews, \" artificial reviews.\")\n",
    "        \n",
    "        if num_reviews == 0:\n",
    "            continue\n",
    "        \n",
    "        current_training_batch = [review[:min_review_length] for review in current_training_batch]\n",
    "        #not sure this is needed\n",
    "        artificial_review_ids = [review[:min_review_length] for review in artificial_review_ids]\n",
    "        \n",
    "        ##get final states for real reviews\n",
    "        #w_training = np.array(current_training_batch)\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w_training})\n",
    "        #feed_dict = {lm.input_w_:w_training,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: False,\n",
    "                     #lm.initial_h_:h}\n",
    "        #h_real = session.run([lm.final_h_],feed_dict=feed_dict)#no training, just get states\n",
    "        \n",
    "        #real_review_final_states = []\n",
    "        #prob don't need to do anything to h_real\n",
    "        #for row in h_real:\n",
    "            #if len(new_artificial_review) >= 300:\n",
    "                #artificial_review_ids.append(new_artificial_review)\n",
    "                #artificial_review_final_states.append(row)\n",
    "                \n",
    "        #now let's even out the number of examples\n",
    "        current_training_batch = current_training_batch[:num_reviews]\n",
    "        artificial_review_ids = artificial_review_ids[:num_reviews]\n",
    "        #print(h_real[0])\n",
    "        ##print(h_real[1])#wtf why won't this work\n",
    "        ##print(h_real[1].shape)\n",
    "        ##h_real = h_real[1][:num_reviews]\n",
    "        #h_real = h_real[:num_reviews]\n",
    "        #print(h_real)\n",
    "        #artificial_review_final_states[:num_reviews]\n",
    "        real_review_list_for_retraining_softmax = current_training_batch[:]\n",
    "        \n",
    "        #label each review and shuffle data\n",
    "        current_training_batch = [(review,[1,0]) for review in current_training_batch]#might need to swap labels\n",
    "        artificial_review_training_batch = [(np.array(review),[0,1]) for review in artificial_review_ids]\n",
    "        #print(current_training_batch[:10])\n",
    "        #print()\n",
    "        #print(artificial_review_ids[:10])\n",
    "        #print()\n",
    "        ##label each review hidden state and shuffle\n",
    "        #training_states = [(review,np.array([0,1])) for review in h_real]#might need to swap labels\n",
    "        #artificial_review_states = [(review,np.array([1,0])) for review in artificial_review_final_states]\n",
    "        \n",
    "        #combine training lists\n",
    "        current_training_batch.extend(artificial_review_training_batch)\n",
    "        np.random.shuffle(current_training_batch)\n",
    "        \n",
    "        #print(current_training_batch[:10])\n",
    "        #print()\n",
    "        #np.random.shuffle(training_states)\n",
    "        #print(training_states)\n",
    "        \n",
    "        #train discriminator\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_list, labels = zip(*current_training_batch)\n",
    "        #review_states, labels = zip(*training_states)\n",
    "        #convert to matrix form\n",
    "        #print(labels)\n",
    "        w = np.array(list(review_list))\n",
    "        #w = np.array(list(review_states))\n",
    "        #y = np.array(list(labels))\n",
    "        y = np.array(labels)\n",
    "        #print(w.shape)\n",
    "        #print(y.shape)\n",
    "        #print()\n",
    "        #batching is done at the review level\n",
    "        #the whole review is fed in at once, so initialize h first\n",
    "        #if batch == 0:\n",
    "        \n",
    "        #train CNN classifier\n",
    "        train_op = lm.train_step_\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_x:w,\n",
    "                     #lm.input_y:y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True}\n",
    "        feed_dict = {lm.input_w_: w, \n",
    "                     lm.input_y: y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True, \n",
    "                     lm.initial_h_: h}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"discriminator training accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"discriminator training cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        ##train discriminator on fake reviews\n",
    "        ##now unzip into \"w\" and \"y\"\n",
    "        #review_list, labels = zip(*artificial_review_ids)\n",
    "        #w = np.array(list(review_list))\n",
    "        #y = np.array(labels)\n",
    "        \n",
    "        ##train CNN classifier\n",
    "        #train_op = self.train_step_\n",
    "        #h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_w_: w, \n",
    "                     #lm.input_y: y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True, \n",
    "                     #lm.initial_h_: h}\n",
    "        #accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        #print(\"fake review accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        #print(\"fake review cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #train generator\n",
    "        #relabel fake reviews as real\n",
    "        artificial_review_training_batch = [(np.array(review),[1,0]) for review in artificial_review_ids]\n",
    "        #now unzip into \"w\" and \"y\"\n",
    "        review_list, labels = zip(*artificial_review_training_batch)\n",
    "        w = np.array(list(review_list))\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        #train CNN classifier\n",
    "        train_op = lm.train_step1_\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #feed_dict = {lm.input_x:w,\n",
    "                     #lm.input_y:y,\n",
    "                     #lm.learning_rate_: learning_rate,\n",
    "                     #lm.use_dropout_: True}\n",
    "        feed_dict = {lm.input_w_: w, \n",
    "                     lm.input_y: y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: True, \n",
    "                     lm.initial_h_: h}\n",
    "        accuracy, cost, h, _ = session.run([lm.accuracy, loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        print(\"generator training accuracy for gan batch \", batch, \" is: \", accuracy)\n",
    "        print(\"generator training cost for gan batch \", batch, \" is: \", cost)\n",
    "        \n",
    "        #retrain softmax of rnn on real reviews\n",
    "        flattened_real_ids = np.array([item for sublist in real_review_list_for_retraining_softmax for item in sublist])\n",
    "        #print(flattened_real_ids[:10])\n",
    "        #print(flattened_real_ids.shape)\n",
    "        #print()\n",
    "        bi = utils.rnnlm_batch_generator(flattened_real_ids, batch_size, max_time=150)\n",
    "        for i, (w, y) in enumerate(bi):\n",
    "            cost = 0.0\n",
    "            if i == 0:\n",
    "                h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            feed_dict = {lm.input_w_: w, \n",
    "                         lm.target_y_: y, \n",
    "                         lm.learning_rate_: learning_rate,\n",
    "                         lm.use_dropout_: False,\n",
    "                         lm.initial_h_:h}\n",
    "            cost, h, _ = session.run([lm.train_loss_, lm.final_h_, lm.train_step_softmax_],feed_dict=feed_dict)\n",
    "            print(\"softmax re-training cost for gan batch \", batch, \" is: \", cost)\n",
    "    \n",
    "    ### UPDATED!!!\n",
    "    total_cost += cost#update\n",
    "    total_batches = batch + 1\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "        avg_cost = total_cost / total_batches\n",
    "        avg_wps = total_words / (time.time() - start_time)\n",
    "        print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "            batch, total_words, avg_wps, avg_cost))\n",
    "        tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        if len(semifinal_review) > 300:\n",
    "            final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "            #print(final_review)\n",
    "            review_list.append(final_review)\n",
    "    return review_list\n",
    "\n",
    "def get_review_series(review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    review_df = pd.read_csv(review_path)\n",
    "    five_star_review_df = review_df[review_df['stars']==5]\n",
    "    #five_star_review_series = five_star_review_df['text']\n",
    "    return five_star_review_df['text']\n",
    "\n",
    "def get_business_list(business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'):\n",
    "    #business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'\n",
    "    return pd.read_csv(business_path)\n",
    "\n",
    "def split_train_test(review_list, training_samples, test_samples):\n",
    "    #pass in randomized review list\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return randomized_training_list, randomized_testing_list\n",
    "\n",
    "def make_train_test_data(five_star_review_series, training_samples=20000, test_samples=1000):\n",
    "    #fix randomization to prevent evaluation on trained samples\n",
    "    review_list = preprocess_review_series(five_star_review_series)\n",
    "    #split and shuffle the data\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    np.random.shuffle(review_list)\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    #training_review_list = [item for sublist in review_list[:training_samples] for item in sublist]\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    \n",
    "    #test_review_list = [item for sublist in review_list[training_samples:training_samples+test_samples] for item in sublist]\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return training_review_list, test_review_list\n",
    "\n",
    "\n",
    "#def make_vocabulary(training_review_list, test_review_list):\n",
    "#    unique_characters = list(set(training_review_list + test_review_list))\n",
    "#    #vocabulary\n",
    "#    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "#    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#    return char_dict, ids_to_words\n",
    "def make_vocabulary(dataset_list):\n",
    "    unique_characters = list(set().union(*dataset_list))\n",
    "    #unique_characters = list(set(training_review_list + test_review_list))\n",
    "    #vocabulary\n",
    "    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "    return char_dict, ids_to_words\n",
    "\n",
    "def convert_to_ids(char_dict, review_list):\n",
    "    #convert to flat (1D) np.array(int) of ids\n",
    "    review_ids = [char_dict.get(token) for token in review_list]\n",
    "    return np.array(review_ids)\n",
    "\n",
    "def run_training(train_list, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20):\n",
    "    #V = len(words_to_ids.keys())\n",
    "    # Training parameters\n",
    "    ## add parameter sets for each attack/defense configuration\n",
    "    #max_time = 25\n",
    "    #batch_size = 100\n",
    "    #learning_rate = 0.01\n",
    "    #num_epochs = 10\n",
    "    \n",
    "    # Model parameters\n",
    "    #model_params = dict(V=vocab.size, \n",
    "                        #H=200, \n",
    "                        #softmax_ns=200,\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=len(words_to_ids.keys()), \n",
    "                        #H=1024, \n",
    "                        #softmax_ns=len(words_to_ids.keys()),\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=V, H=H, softmax_ns=softmax_ns, num_layers=num_layers)\n",
    "    \n",
    "    #TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "    TF_SAVEDIR = tf_savedir\n",
    "    checkpoint_filename = os.path.join(TF_SAVEDIR, \"gan\")\n",
    "    trained_filename = os.path.join(TF_SAVEDIR, \"gan_trained\")\n",
    "    \n",
    "    # Will print status every this many seconds\n",
    "    #print_interval = 5\n",
    "    print_interval = 30\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    ### UPDATED!!!\n",
    "    lm.BuildSamplerGraph()\n",
    "    num_pretrain = 15000\n",
    "    #num_pretrain = 200#low number for testing\n",
    "    #pretrain on a different set of training data each time\n",
    "    #train and test ids \n",
    "    #pre_train_ids =\n",
    "    #train_ids = \n",
    "    ### UPDATED!!!\n",
    "    \n",
    "    # Explicitly add global initializer and variable saver to LM graph\n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Clear old log directory\n",
    "    shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "    if not os.path.isdir(TF_SAVEDIR):\n",
    "        os.makedirs(TF_SAVEDIR)\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(42)\n",
    "    \n",
    "        session.run(initializer)\n",
    "        \n",
    "        #check trainable variables\n",
    "        #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "        #values = session.run(variables_names)\n",
    "        #for k, v in zip(variables_names, values):\n",
    "            #print(\"Variable: \", k)\n",
    "            #print(\"Shape: \", v.shape)\n",
    "            #print(v)\n",
    "    \n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            np.random.shuffle(train_list)\n",
    "            #shuffled_train_list = np.random.shuffle(train_list)\n",
    "            #pre_train_ids = [item for sublist in shuffled_train_list[:num_pretrain] for item in sublist]\n",
    "            #gan_train_list = shuffled_train_list[num_pretrain:]\n",
    "            #train_list = \n",
    "            pre_train_ids = np.array([item for sublist in train_list[:num_pretrain] for item in sublist])\n",
    "            #print(pre_train_ids.shape)\n",
    "            #pre_train_ids = np.array(pre_train_ids)\n",
    "            gan_train_list = train_list[num_pretrain:]\n",
    "            if num_pretrain > 0:\n",
    "                bi = utils.rnnlm_batch_generator(pre_train_ids, batch_size, max_time)\n",
    "                print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "                # Run a pretraining epoch.\n",
    "                run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "                print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "        \n",
    "            #now train gan\n",
    "            #run_gan_epoch(lm, session, words_to_ids, ids_to_words, train_list, verbose=False, tick_s=10, learning_rate=None)\n",
    "            run_gan_epoch(lm, session, words_to_ids, ids_to_words, gan_train_list, batch_size, \n",
    "                          verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "        \n",
    "            # Save a checkpoint\n",
    "            saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "        \n",
    "            ##\n",
    "            # score_dataset will run a forward pass over the entire dataset\n",
    "            # and report perplexity scores. This can be slow (around 1/2 to \n",
    "            # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "            # to speed up training on a slow machine. Be sure to run it at the \n",
    "            # end to evaluate your score.\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, pre_train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "        return trained_filename\n",
    "\n",
    "def get_char_probs(trained_filename, model_params, test_ids):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    all_review_likelihoods = []\n",
    "    train_op = tf.no_op()\n",
    "    use_dropout = False\n",
    "    loss = lm.loss_\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False\n",
    "        #loss = lm.loss_\n",
    "        \n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        for review in test_ids:\n",
    "            review_likelihoods = []\n",
    "            inputs = review[:-1]\n",
    "            labels = review[1:]\n",
    "            inputs_labels = zip(inputs,labels)\n",
    "            for i, (w,y) in enumerate(inputs_labels):\n",
    "                \n",
    "                w = np.array(w)\n",
    "                y = np.array(y)\n",
    "                w = w.reshape([1,1])\n",
    "                y = y.reshape([1,1])\n",
    "                \n",
    "                if i == 0:\n",
    "                    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "                feed_dict = {lm.input_w_:w, \n",
    "                             lm.target_y_:y,\n",
    "                             lm.learning_rate_: 0.002,\n",
    "                             lm.use_dropout_: use_dropout,\n",
    "                             lm.initial_h_:h}\n",
    "                cost, h = session.run([loss, lm.final_h_],feed_dict=feed_dict)\n",
    "                likelihood = 2**(-1*cost)\n",
    "                review_likelihoods.append(likelihood)\n",
    "            all_review_likelihoods.append(review_likelihoods)\n",
    "    return all_review_likelihoods\n",
    "\n",
    "## Sampling\n",
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "\n",
    "    # Run sample ops\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]\n",
    "\n",
    "def generate_text(trained_filename, model_params, words_to_ids, ids_to_words):\n",
    "    # Same as above, but as a batch\n",
    "    #max_steps = 20\n",
    "    max_steps = 300\n",
    "    #num_samples = 40000\n",
    "    num_samples = 40\n",
    "    random_seed = 42\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(random_seed)\n",
    "        \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "    \n",
    "        # Make initial state for a batch with batch_size = num_samples\n",
    "        #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        # take one step for each sequence on each iteration \n",
    "        for i in range(max_steps):\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "    \n",
    "        # Print generated sentences\n",
    "        for row in w:\n",
    "            print(trained_filename, end=\":  \")\n",
    "            for i, word_id in enumerate(row):\n",
    "                #print(vocab.id_to_word[word_id], end=\" \")\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                #if (i != 0) and (word_id == vocab.START_ID):\n",
    "                if (i != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")\n",
    "\n",
    "def train_attack_model(training_samples=20000, test_samples=1000, review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #training_samples=20000\n",
    "    #test_samples=1000\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    start_format = time.time()\n",
    "    five_star_reviews = get_review_series(review_path)\n",
    "    train_review_list, test_review_list = make_train_test_data(five_star_reviews, training_samples, test_samples)\n",
    "    words_to_ids, ids_to_words = make_vocabulary(train_review_list, test_review_list)\n",
    "    train_ids = convert_to_ids(words_to_ids, train_review_list)\n",
    "    test_ids = convert_to_ids(words_to_ids, test_review_list)\n",
    "    end_format = time.time()\n",
    "    print(\"data formatting took \" + str(end_format-start_format) + \" seconds\")\n",
    "    model_params = dict(V=len(words_to_ids.keys()), \n",
    "                            H=1024, \n",
    "                            softmax_ns=len(words_to_ids.keys()),\n",
    "                            num_layers=2)\n",
    "    #run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=150, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    return trained_filename, model_params, words_to_ids, ids_to_words\n",
    "\n",
    "def neg_log_lik_ratio(likelihoods_real, likelihoods_artificial):\n",
    "    predictions = []\n",
    "    combined = zip(likelihoods_real, likelihoods_artificial)\n",
    "    for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "        negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "        #averaged_llrs = negative_log_lik_ratios[:-1]/(len(negative_log_lik_ratios)-1)\n",
    "        averaged_llrs = np.sum(negative_log_lik_ratios[:-1])/(len(negative_log_lik_ratios)-1)\n",
    "        predictions.append(averaged_llrs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data download took 4.29355525970459 seconds\n"
     ]
    }
   ],
   "source": [
    "start_dl = time.time()\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_train_data_03.csv .')\n",
    "os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_test_data_03.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_train_data_01.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_test_data_01.csv .')\n",
    "end_dl = time.time()\n",
    "print(\"data download took \" + str(end_dl-start_dl) + \" seconds\")\n",
    "#gsutil cp gs://[BUCKET_NAME]/[OBJECT_NAME] [OBJECT_DESTINATION]\n",
    "real_train_review_path = './split01_train_data_02.csv'\n",
    "real_test_review_path = './split01_test_data_02.csv'\n",
    "#artificial_train_review_path = './gen01_train_data_01.csv'\n",
    "#artificial_test_review_path = './gen01_test_data_01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_train_review_path = '/home/kalvin_kao/final_project/split01_train_data_02.csv'\n",
    "real_test_review_path = '/home/kalvin_kao/final_project/split01_test_data_02.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading took 1.9353656768798828 seconds\n"
     ]
    }
   ],
   "source": [
    "start_open = time.time()\n",
    "with open(real_train_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    training_review_list_real = [sublist for sublist in reader]\n",
    "training_review_list_real_training_eval = [item for sublist in training_review_list_real for item in sublist]\n",
    "\n",
    "with open(real_test_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    test_review_list_real = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "test_review_list_real_training_eval = [item for sublist in test_review_list_real for item in sublist]\n",
    "\n",
    "#with open(artificial_train_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #training_review_list_artificial = [item for sublist in reader for item in sublist]\n",
    "\n",
    "#with open(artificial_test_review_path, 'r') as csvfile:\n",
    "    #reader = csv.reader(csvfile, delimiter=',')\n",
    "    #test_review_list_artificial = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "#test_review_list_artificial_training_eval = [item for sublist in test_review_list_artificial for item in sublist]\n",
    "end_open = time.time()\n",
    "print(\"data reading took \" + str(end_open-start_open) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary building took 9.899223327636719 seconds\n"
     ]
    }
   ],
   "source": [
    "start_vocab = time.time()\n",
    "#words_to_ids, ids_to_words = make_vocabulary([training_review_list_real, test_review_list_real_training_eval, training_review_list_artificial, test_review_list_artificial_training_eval])\n",
    "words_to_ids, ids_to_words = make_vocabulary([training_review_list_real_training_eval, test_review_list_real_training_eval])\n",
    "train_ids_real = [convert_to_ids(words_to_ids, review) for review in training_review_list_real]\n",
    "train_ids_real_training_eval = convert_to_ids(words_to_ids, training_review_list_real_training_eval)\n",
    "test_ids_real = [convert_to_ids(words_to_ids, review) for review in test_review_list_real]\n",
    "test_ids_real_training_eval = convert_to_ids(words_to_ids, test_review_list_real_training_eval)\n",
    "#train_ids_artificial = convert_to_ids(words_to_ids, training_review_list_artificial)\n",
    "#test_ids_artificial = [convert_to_ids(words_to_ids, review) for review in test_review_list_artificial]\n",
    "#test_ids_artificial_training_eval = convert_to_ids(words_to_ids, test_review_list_artificial_training_eval)\n",
    "end_vocab = time.time()\n",
    "print(\"vocabulary building took \" + str(end_vocab-start_vocab) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.random.shuffle(train_ids_real[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trainer.rnnlm' from '/home/kalvin_kao/artificial_hotel_reviews/model_dev/gan/trainer/rnnlm.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/artificial_hotel_reviews/model_dev/gan/trainer/rnnlm.py:352: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 1]: seen 19200 words at 1286.9 wps, loss = 16.860\n",
      "[batch 3]: seen 38400 words at 1335.4 wps, loss = 23.654\n",
      "[batch 5]: seen 57600 words at 1354.2 wps, loss = 21.211\n",
      "[batch 7]: seen 76800 words at 1392.5 wps, loss = 18.017\n",
      "[batch 9]: seen 96000 words at 1423.8 wps, loss = 15.334\n",
      "[batch 11]: seen 115200 words at 1421.5 wps, loss = 13.355\n",
      "[batch 13]: seen 134400 words at 1436.6 wps, loss = 11.870\n",
      "[batch 15]: seen 153600 words at 1417.4 wps, loss = 10.726\n",
      "[batch 17]: seen 172800 words at 1447.0 wps, loss = 9.830\n",
      "[batch 19]: seen 192000 words at 1445.3 wps, loss = 9.107\n",
      "[batch 21]: seen 211200 words at 1445.7 wps, loss = 8.509\n",
      "[batch 23]: seen 230400 words at 1442.9 wps, loss = 8.008\n",
      "[batch 25]: seen 249600 words at 1444.7 wps, loss = 7.582\n",
      "[batch 27]: seen 268800 words at 1443.3 wps, loss = 7.214\n",
      "[batch 29]: seen 288000 words at 1445.0 wps, loss = 6.894\n",
      "[batch 31]: seen 307200 words at 1451.6 wps, loss = 6.613\n",
      "[batch 33]: seen 326400 words at 1442.9 wps, loss = 6.364\n",
      "[batch 35]: seen 345600 words at 1453.1 wps, loss = 6.141\n",
      "[batch 37]: seen 364800 words at 1453.4 wps, loss = 5.940\n",
      "[batch 39]: seen 384000 words at 1454.9 wps, loss = 5.758\n",
      "[batch 41]: seen 403200 words at 1453.9 wps, loss = 5.592\n",
      "[batch 43]: seen 422400 words at 1452.2 wps, loss = 5.441\n",
      "[batch 45]: seen 441600 words at 1454.1 wps, loss = 5.302\n",
      "[batch 47]: seen 460800 words at 1455.1 wps, loss = 5.175\n",
      "[batch 49]: seen 480000 words at 1457.6 wps, loss = 5.057\n",
      "[batch 51]: seen 499200 words at 1450.0 wps, loss = 4.947\n",
      "[batch 53]: seen 518400 words at 1457.7 wps, loss = 4.844\n",
      "[batch 55]: seen 537600 words at 1458.5 wps, loss = 4.749\n",
      "[batch 57]: seen 556800 words at 1458.6 wps, loss = 4.660\n",
      "[batch 59]: seen 576000 words at 1455.7 wps, loss = 4.577\n",
      "[batch 61]: seen 595200 words at 1456.6 wps, loss = 4.498\n",
      "[batch 63]: seen 614400 words at 1459.8 wps, loss = 4.424\n",
      "[batch 65]: seen 633600 words at 1458.7 wps, loss = 4.355\n",
      "[batch 67]: seen 652800 words at 1460.6 wps, loss = 4.293\n",
      "[batch 69]: seen 672000 words at 1455.5 wps, loss = 4.233\n",
      "[batch 71]: seen 691200 words at 1462.1 wps, loss = 4.177\n",
      "[batch 73]: seen 710400 words at 1461.0 wps, loss = 4.122\n",
      "[batch 75]: seen 729600 words at 1461.0 wps, loss = 4.070\n",
      "[batch 77]: seen 748800 words at 1460.3 wps, loss = 4.020\n",
      "[batch 79]: seen 768000 words at 1459.6 wps, loss = 3.972\n",
      "[batch 81]: seen 787200 words at 1459.9 wps, loss = 3.927\n",
      "[batch 83]: seen 806400 words at 1460.8 wps, loss = 3.882\n",
      "[batch 85]: seen 825600 words at 1463.3 wps, loss = 3.840\n",
      "[batch 87]: seen 844800 words at 1458.3 wps, loss = 3.800\n",
      "[batch 89]: seen 864000 words at 1462.2 wps, loss = 3.761\n",
      "[batch 91]: seen 883200 words at 1462.2 wps, loss = 3.724\n",
      "[batch 93]: seen 902400 words at 1463.0 wps, loss = 3.687\n",
      "[batch 95]: seen 921600 words at 1460.5 wps, loss = 3.653\n",
      "[batch 97]: seen 940800 words at 1459.9 wps, loss = 3.619\n",
      "[batch 99]: seen 960000 words at 1461.3 wps, loss = 3.587\n",
      "[batch 101]: seen 979200 words at 1462.6 wps, loss = 3.556\n",
      "[batch 103]: seen 998400 words at 1463.1 wps, loss = 3.526\n",
      "[batch 105]: seen 1017600 words at 1459.9 wps, loss = 3.497\n",
      "[batch 107]: seen 1036800 words at 1463.4 wps, loss = 3.469\n",
      "[batch 109]: seen 1056000 words at 1464.6 wps, loss = 3.441\n",
      "[batch 111]: seen 1075200 words at 1463.0 wps, loss = 3.415\n",
      "[batch 113]: seen 1094400 words at 1462.8 wps, loss = 3.390\n",
      "[batch 115]: seen 1113600 words at 1461.3 wps, loss = 3.366\n",
      "[batch 117]: seen 1132800 words at 1462.9 wps, loss = 3.342\n",
      "[batch 119]: seen 1152000 words at 1462.1 wps, loss = 3.319\n",
      "[batch 121]: seen 1171200 words at 1463.2 wps, loss = 3.296\n",
      "[batch 123]: seen 1190400 words at 1460.2 wps, loss = 3.274\n",
      "[batch 125]: seen 1209600 words at 1463.4 wps, loss = 3.253\n",
      "[batch 127]: seen 1228800 words at 1463.2 wps, loss = 3.231\n",
      "[batch 129]: seen 1248000 words at 1463.4 wps, loss = 3.212\n",
      "[batch 131]: seen 1267200 words at 1463.7 wps, loss = 3.192\n",
      "[batch 133]: seen 1286400 words at 1462.4 wps, loss = 3.173\n",
      "[batch 135]: seen 1305600 words at 1462.3 wps, loss = 3.155\n",
      "[batch 137]: seen 1324800 words at 1462.9 wps, loss = 3.137\n",
      "[batch 139]: seen 1344000 words at 1464.4 wps, loss = 3.119\n",
      "[batch 141]: seen 1363200 words at 1461.5 wps, loss = 3.102\n",
      "[batch 143]: seen 1382400 words at 1463.6 wps, loss = 3.085\n",
      "[batch 145]: seen 1401600 words at 1463.9 wps, loss = 3.069\n",
      "[batch 147]: seen 1420800 words at 1463.9 wps, loss = 3.053\n",
      "[batch 149]: seen 1440000 words at 1463.3 wps, loss = 3.037\n",
      "[batch 151]: seen 1459200 words at 1462.0 wps, loss = 3.021\n",
      "[batch 153]: seen 1478400 words at 1462.7 wps, loss = 3.006\n",
      "[batch 155]: seen 1497600 words at 1463.8 wps, loss = 2.992\n",
      "[batch 157]: seen 1516800 words at 1464.0 wps, loss = 2.978\n",
      "[batch 159]: seen 1536000 words at 1461.6 wps, loss = 2.965\n",
      "[batch 161]: seen 1555200 words at 1463.9 wps, loss = 2.952\n",
      "[batch 163]: seen 1574400 words at 1464.1 wps, loss = 2.938\n",
      "[batch 165]: seen 1593600 words at 1463.4 wps, loss = 2.925\n",
      "[batch 167]: seen 1612800 words at 1463.4 wps, loss = 2.913\n",
      "[batch 169]: seen 1632000 words at 1462.8 wps, loss = 2.900\n",
      "[batch 171]: seen 1651200 words at 1463.6 wps, loss = 2.887\n",
      "[batch 173]: seen 1670400 words at 1463.5 wps, loss = 2.875\n",
      "[batch 175]: seen 1689600 words at 1464.5 wps, loss = 2.863\n",
      "[batch 177]: seen 1708800 words at 1463.0 wps, loss = 2.852\n",
      "[batch 179]: seen 1728000 words at 1464.5 wps, loss = 2.840\n",
      "[batch 181]: seen 1747200 words at 1464.2 wps, loss = 2.829\n",
      "[batch 183]: seen 1766400 words at 1463.9 wps, loss = 2.819\n",
      "[batch 185]: seen 1785600 words at 1464.4 wps, loss = 2.808\n",
      "[batch 187]: seen 1804800 words at 1463.0 wps, loss = 2.798\n",
      "[batch 189]: seen 1824000 words at 1463.4 wps, loss = 2.788\n",
      "[batch 191]: seen 1843200 words at 1463.5 wps, loss = 2.777\n",
      "[batch 193]: seen 1862400 words at 1464.7 wps, loss = 2.768\n",
      "[batch 195]: seen 1881600 words at 1462.5 wps, loss = 2.758\n",
      "[batch 197]: seen 1900800 words at 1463.9 wps, loss = 2.749\n",
      "[batch 199]: seen 1920000 words at 1464.2 wps, loss = 2.739\n",
      "[batch 201]: seen 1939200 words at 1464.7 wps, loss = 2.730\n",
      "[batch 203]: seen 1958400 words at 1464.2 wps, loss = 2.721\n",
      "[batch 205]: seen 1977600 words at 1463.1 wps, loss = 2.712\n",
      "[batch 207]: seen 1996800 words at 1463.5 wps, loss = 2.704\n",
      "[batch 209]: seen 2016000 words at 1464.0 wps, loss = 2.695\n",
      "[batch 211]: seen 2035200 words at 1464.0 wps, loss = 2.687\n",
      "[batch 213]: seen 2054400 words at 1462.2 wps, loss = 2.679\n",
      "[batch 215]: seen 2073600 words at 1463.9 wps, loss = 2.671\n",
      "[batch 217]: seen 2092800 words at 1464.3 wps, loss = 2.663\n",
      "[batch 219]: seen 2112000 words at 1463.8 wps, loss = 2.654\n",
      "[batch 221]: seen 2131200 words at 1463.6 wps, loss = 2.646\n",
      "[batch 223]: seen 2150400 words at 1463.7 wps, loss = 2.639\n",
      "[batch 225]: seen 2169600 words at 1464.1 wps, loss = 2.631\n",
      "[batch 227]: seen 2188800 words at 1464.0 wps, loss = 2.623\n",
      "[batch 229]: seen 2208000 words at 1464.8 wps, loss = 2.616\n",
      "[batch 231]: seen 2227200 words at 1463.5 wps, loss = 2.609\n",
      "[batch 233]: seen 2246400 words at 1464.7 wps, loss = 2.602\n",
      "[batch 235]: seen 2265600 words at 1464.5 wps, loss = 2.594\n",
      "[batch 237]: seen 2284800 words at 1464.6 wps, loss = 2.587\n",
      "[batch 239]: seen 2304000 words at 1464.7 wps, loss = 2.580\n",
      "[batch 241]: seen 2323200 words at 1463.9 wps, loss = 2.573\n",
      "[batch 243]: seen 2342400 words at 1464.1 wps, loss = 2.567\n",
      "[batch 245]: seen 2361600 words at 1464.2 wps, loss = 2.560\n",
      "[batch 247]: seen 2380800 words at 1465.2 wps, loss = 2.554\n",
      "[batch 249]: seen 2400000 words at 1463.5 wps, loss = 2.547\n",
      "[batch 251]: seen 2419200 words at 1465.0 wps, loss = 2.541\n",
      "[batch 253]: seen 2438400 words at 1464.9 wps, loss = 2.534\n",
      "[batch 255]: seen 2457600 words at 1465.1 wps, loss = 2.528\n",
      "[batch 257]: seen 2476800 words at 1465.0 wps, loss = 2.523\n",
      "[batch 259]: seen 2496000 words at 1464.6 wps, loss = 2.517\n",
      "[batch 261]: seen 2515200 words at 1465.3 wps, loss = 2.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 263]: seen 2534400 words at 1464.9 wps, loss = 2.504\n",
      "[batch 265]: seen 2553600 words at 1465.3 wps, loss = 2.499\n",
      "[batch 267]: seen 2572800 words at 1465.3 wps, loss = 2.493\n",
      "[batch 269]: seen 2592000 words at 1465.6 wps, loss = 2.488\n",
      "[batch 271]: seen 2611200 words at 1465.1 wps, loss = 2.482\n",
      "[batch 273]: seen 2630400 words at 1464.8 wps, loss = 2.477\n",
      "[batch 275]: seen 2649600 words at 1464.7 wps, loss = 2.471\n",
      "[batch 277]: seen 2668800 words at 1464.2 wps, loss = 2.466\n",
      "[batch 279]: seen 2688000 words at 1463.9 wps, loss = 2.460\n",
      "[batch 281]: seen 2707200 words at 1463.5 wps, loss = 2.455\n",
      "[batch 283]: seen 2726400 words at 1462.9 wps, loss = 2.449\n",
      "[batch 285]: seen 2745600 words at 1462.4 wps, loss = 2.444\n",
      "[batch 287]: seen 2764800 words at 1462.1 wps, loss = 2.439\n",
      "[batch 289]: seen 2784000 words at 1462.2 wps, loss = 2.434\n",
      "[batch 291]: seen 2803200 words at 1461.7 wps, loss = 2.429\n",
      "[batch 293]: seen 2822400 words at 1461.5 wps, loss = 2.425\n",
      "[batch 295]: seen 2841600 words at 1461.8 wps, loss = 2.421\n",
      "[batch 297]: seen 2860800 words at 1461.1 wps, loss = 2.416\n",
      "[batch 299]: seen 2880000 words at 1460.9 wps, loss = 2.412\n",
      "[batch 301]: seen 2899200 words at 1461.0 wps, loss = 2.407\n",
      "[batch 303]: seen 2918400 words at 1460.5 wps, loss = 2.403\n",
      "[batch 305]: seen 2937600 words at 1459.9 wps, loss = 2.398\n",
      "[batch 307]: seen 2956800 words at 1459.7 wps, loss = 2.394\n",
      "[batch 309]: seen 2976000 words at 1459.5 wps, loss = 2.389\n",
      "[batch 311]: seen 2995200 words at 1459.1 wps, loss = 2.385\n",
      "[batch 313]: seen 3014400 words at 1458.9 wps, loss = 2.380\n",
      "[batch 315]: seen 3033600 words at 1459.1 wps, loss = 2.376\n",
      "[batch 317]: seen 3052800 words at 1458.2 wps, loss = 2.372\n",
      "[batch 319]: seen 3072000 words at 1458.0 wps, loss = 2.368\n",
      "[batch 321]: seen 3091200 words at 1458.1 wps, loss = 2.363\n",
      "[batch 323]: seen 3110400 words at 1457.6 wps, loss = 2.359\n",
      "[batch 325]: seen 3129600 words at 1457.1 wps, loss = 2.355\n",
      "[batch 327]: seen 3148800 words at 1457.0 wps, loss = 2.351\n",
      "[batch 329]: seen 3168000 words at 1457.0 wps, loss = 2.346\n",
      "[batch 331]: seen 3187200 words at 1456.6 wps, loss = 2.342\n",
      "[batch 333]: seen 3206400 words at 1456.5 wps, loss = 2.339\n",
      "[batch 335]: seen 3225600 words at 1456.6 wps, loss = 2.335\n",
      "[batch 337]: seen 3244800 words at 1456.1 wps, loss = 2.332\n",
      "[batch 339]: seen 3264000 words at 1456.4 wps, loss = 2.328\n",
      "[batch 341]: seen 3283200 words at 1456.7 wps, loss = 2.324\n",
      "[batch 343]: seen 3302400 words at 1456.8 wps, loss = 2.320\n",
      "[batch 345]: seen 3321600 words at 1456.9 wps, loss = 2.317\n",
      "[batch 347]: seen 3340800 words at 1457.2 wps, loss = 2.313\n",
      "[batch 349]: seen 3360000 words at 1457.4 wps, loss = 2.309\n",
      "[batch 351]: seen 3379200 words at 1457.6 wps, loss = 2.306\n",
      "[batch 353]: seen 3398400 words at 1457.8 wps, loss = 2.302\n",
      "[batch 355]: seen 3417600 words at 1458.0 wps, loss = 2.298\n",
      "[batch 357]: seen 3436800 words at 1457.8 wps, loss = 2.294\n",
      "[batch 359]: seen 3456000 words at 1457.9 wps, loss = 2.291\n",
      "[batch 361]: seen 3475200 words at 1457.4 wps, loss = 2.287\n",
      "[batch 363]: seen 3494400 words at 1457.2 wps, loss = 2.284\n",
      "[batch 365]: seen 3513600 words at 1457.3 wps, loss = 2.280\n",
      "[batch 367]: seen 3532800 words at 1456.7 wps, loss = 2.277\n",
      "[batch 369]: seen 3552000 words at 1456.4 wps, loss = 2.274\n",
      "[batch 371]: seen 3571200 words at 1456.3 wps, loss = 2.270\n",
      "[batch 373]: seen 3590400 words at 1456.2 wps, loss = 2.267\n",
      "[batch 375]: seen 3609600 words at 1455.9 wps, loss = 2.264\n",
      "[batch 377]: seen 3628800 words at 1455.7 wps, loss = 2.261\n",
      "[batch 379]: seen 3648000 words at 1455.9 wps, loss = 2.258\n",
      "[batch 381]: seen 3667200 words at 1455.6 wps, loss = 2.255\n",
      "[batch 383]: seen 3686400 words at 1455.5 wps, loss = 2.251\n",
      "[batch 385]: seen 3705600 words at 1455.6 wps, loss = 2.248\n",
      "[batch 387]: seen 3724800 words at 1455.1 wps, loss = 2.245\n",
      "[batch 389]: seen 3744000 words at 1454.9 wps, loss = 2.242\n",
      "[batch 391]: seen 3763200 words at 1454.7 wps, loss = 2.239\n",
      "[batch 393]: seen 3782400 words at 1454.5 wps, loss = 2.236\n",
      "[batch 395]: seen 3801600 words at 1454.2 wps, loss = 2.233\n",
      "[batch 397]: seen 3820800 words at 1454.0 wps, loss = 2.230\n",
      "[batch 399]: seen 3840000 words at 1454.0 wps, loss = 2.227\n",
      "[batch 401]: seen 3859200 words at 1453.8 wps, loss = 2.224\n",
      "[batch 403]: seen 3878400 words at 1453.6 wps, loss = 2.221\n",
      "[batch 405]: seen 3897600 words at 1453.7 wps, loss = 2.218\n",
      "[batch 407]: seen 3916800 words at 1453.3 wps, loss = 2.216\n",
      "[batch 409]: seen 3936000 words at 1453.2 wps, loss = 2.213\n",
      "[batch 411]: seen 3955200 words at 1453.2 wps, loss = 2.210\n",
      "[batch 413]: seen 3974400 words at 1452.8 wps, loss = 2.207\n",
      "[batch 415]: seen 3993600 words at 1452.3 wps, loss = 2.204\n",
      "[batch 417]: seen 4012800 words at 1452.2 wps, loss = 2.201\n",
      "[batch 419]: seen 4032000 words at 1452.3 wps, loss = 2.198\n",
      "[batch 421]: seen 4051200 words at 1452.0 wps, loss = 2.196\n",
      "[batch 423]: seen 4070400 words at 1451.8 wps, loss = 2.193\n",
      "[batch 425]: seen 4089600 words at 1451.9 wps, loss = 2.190\n",
      "[batch 427]: seen 4108800 words at 1451.5 wps, loss = 2.188\n",
      "[batch 429]: seen 4128000 words at 1451.4 wps, loss = 2.185\n",
      "[batch 431]: seen 4147200 words at 1451.2 wps, loss = 2.182\n",
      "[batch 433]: seen 4166400 words at 1451.1 wps, loss = 2.180\n",
      "[batch 435]: seen 4185600 words at 1450.7 wps, loss = 2.177\n",
      "[batch 437]: seen 4204800 words at 1450.7 wps, loss = 2.175\n",
      "[batch 439]: seen 4224000 words at 1450.7 wps, loss = 2.172\n",
      "[batch 441]: seen 4243200 words at 1450.6 wps, loss = 2.170\n",
      "[batch 443]: seen 4262400 words at 1450.4 wps, loss = 2.167\n",
      "[batch 445]: seen 4281600 words at 1450.5 wps, loss = 2.165\n",
      "[batch 447]: seen 4300800 words at 1450.2 wps, loss = 2.162\n",
      "[batch 449]: seen 4320000 words at 1450.0 wps, loss = 2.160\n",
      "[batch 451]: seen 4339200 words at 1449.7 wps, loss = 2.157\n",
      "[batch 453]: seen 4358400 words at 1449.7 wps, loss = 2.155\n",
      "[batch 455]: seen 4377600 words at 1449.4 wps, loss = 2.152\n",
      "[batch 457]: seen 4396800 words at 1449.3 wps, loss = 2.150\n",
      "[batch 459]: seen 4416000 words at 1449.3 wps, loss = 2.147\n",
      "[batch 461]: seen 4435200 words at 1449.2 wps, loss = 2.145\n",
      "[batch 463]: seen 4454400 words at 1449.0 wps, loss = 2.143\n",
      "[batch 465]: seen 4473600 words at 1449.1 wps, loss = 2.140\n",
      "[batch 467]: seen 4492800 words at 1448.8 wps, loss = 2.138\n",
      "[batch 469]: seen 4512000 words at 1448.6 wps, loss = 2.136\n",
      "[batch 471]: seen 4531200 words at 1448.5 wps, loss = 2.134\n",
      "[batch 473]: seen 4550400 words at 1448.3 wps, loss = 2.131\n",
      "[batch 475]: seen 4569600 words at 1448.0 wps, loss = 2.129\n",
      "[batch 477]: seen 4588800 words at 1447.9 wps, loss = 2.127\n",
      "[batch 479]: seen 4608000 words at 1448.0 wps, loss = 2.125\n",
      "[batch 481]: seen 4627200 words at 1447.7 wps, loss = 2.123\n",
      "[batch 483]: seen 4646400 words at 1447.6 wps, loss = 2.120\n",
      "[batch 485]: seen 4665600 words at 1447.7 wps, loss = 2.118\n",
      "[batch 487]: seen 4684800 words at 1447.3 wps, loss = 2.116\n",
      "[batch 489]: seen 4704000 words at 1447.1 wps, loss = 2.114\n",
      "[batch 491]: seen 4723200 words at 1447.1 wps, loss = 2.112\n",
      "[batch 493]: seen 4742400 words at 1446.9 wps, loss = 2.110\n",
      "[batch 495]: seen 4761600 words at 1446.7 wps, loss = 2.108\n",
      "[batch 497]: seen 4780800 words at 1446.7 wps, loss = 2.106\n",
      "[batch 499]: seen 4800000 words at 1446.7 wps, loss = 2.104\n",
      "[batch 501]: seen 4819200 words at 1446.6 wps, loss = 2.102\n",
      "[batch 503]: seen 4838400 words at 1446.5 wps, loss = 2.100\n",
      "[batch 505]: seen 4857600 words at 1446.6 wps, loss = 2.098\n",
      "[batch 507]: seen 4876800 words at 1446.2 wps, loss = 2.096\n",
      "[batch 509]: seen 4896000 words at 1446.2 wps, loss = 2.094\n",
      "[batch 511]: seen 4915200 words at 1446.1 wps, loss = 2.093\n",
      "[batch 513]: seen 4934400 words at 1445.9 wps, loss = 2.090\n",
      "[batch 515]: seen 4953600 words at 1445.8 wps, loss = 2.089\n",
      "[batch 517]: seen 4972800 words at 1445.7 wps, loss = 2.087\n",
      "[batch 519]: seen 4992000 words at 1445.9 wps, loss = 2.085\n",
      "[batch 521]: seen 5011200 words at 1445.7 wps, loss = 2.083\n",
      "[batch 523]: seen 5030400 words at 1445.7 wps, loss = 2.081\n",
      "[batch 525]: seen 5049600 words at 1445.8 wps, loss = 2.079\n",
      "[batch 527]: seen 5068800 words at 1445.6 wps, loss = 2.077\n",
      "[batch 529]: seen 5088000 words at 1445.5 wps, loss = 2.075\n",
      "[batch 531]: seen 5107200 words at 1445.6 wps, loss = 2.074\n",
      "[batch 533]: seen 5126400 words at 1445.3 wps, loss = 2.072\n",
      "[batch 535]: seen 5145600 words at 1445.2 wps, loss = 2.070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 537]: seen 5164800 words at 1445.1 wps, loss = 2.068\n",
      "[batch 539]: seen 5184000 words at 1445.0 wps, loss = 2.066\n",
      "[batch 541]: seen 5203200 words at 1444.7 wps, loss = 2.065\n",
      "[batch 543]: seen 5222400 words at 1444.7 wps, loss = 2.063\n",
      "[batch 545]: seen 5241600 words at 1444.7 wps, loss = 2.061\n",
      "[batch 547]: seen 5260800 words at 1444.6 wps, loss = 2.060\n",
      "[batch 549]: seen 5280000 words at 1444.6 wps, loss = 2.058\n",
      "[batch 551]: seen 5299200 words at 1444.7 wps, loss = 2.056\n",
      "[batch 553]: seen 5318400 words at 1444.5 wps, loss = 2.054\n",
      "[batch 555]: seen 5337600 words at 1444.4 wps, loss = 2.053\n",
      "[batch 557]: seen 5356800 words at 1444.6 wps, loss = 2.051\n",
      "[batch 559]: seen 5376000 words at 1444.2 wps, loss = 2.049\n",
      "[batch 561]: seen 5395200 words at 1444.1 wps, loss = 2.047\n",
      "[batch 563]: seen 5414400 words at 1444.0 wps, loss = 2.046\n",
      "[batch 565]: seen 5433600 words at 1444.1 wps, loss = 2.044\n",
      "[batch 567]: seen 5452800 words at 1443.9 wps, loss = 2.042\n",
      "[batch 569]: seen 5472000 words at 1443.9 wps, loss = 2.041\n",
      "[batch 571]: seen 5491200 words at 1444.0 wps, loss = 2.039\n",
      "[batch 573]: seen 5510400 words at 1443.8 wps, loss = 2.037\n",
      "[batch 575]: seen 5529600 words at 1443.8 wps, loss = 2.036\n",
      "[batch 577]: seen 5548800 words at 1443.9 wps, loss = 2.034\n",
      "[batch 579]: seen 5568000 words at 1443.7 wps, loss = 2.032\n",
      "[batch 581]: seen 5587200 words at 1443.5 wps, loss = 2.030\n",
      "[batch 583]: seen 5606400 words at 1443.4 wps, loss = 2.029\n",
      "[batch 585]: seen 5625600 words at 1443.4 wps, loss = 2.027\n",
      "[batch 587]: seen 5644800 words at 1443.3 wps, loss = 2.025\n",
      "[batch 589]: seen 5664000 words at 1443.2 wps, loss = 2.024\n",
      "[batch 591]: seen 5683200 words at 1443.4 wps, loss = 2.022\n",
      "[batch 593]: seen 5702400 words at 1443.2 wps, loss = 2.021\n",
      "[batch 596]: seen 5731200 words at 1445.4 wps, loss = 2.018\n",
      "[batch 599]: seen 5760000 words at 1447.6 wps, loss = 2.016\n",
      "[batch 602]: seen 5788800 words at 1449.8 wps, loss = 2.014\n",
      "[batch 605]: seen 5817600 words at 1452.0 wps, loss = 2.012\n",
      "[batch 608]: seen 5846400 words at 1454.2 wps, loss = 2.010\n",
      "[batch 611]: seen 5875200 words at 1456.1 wps, loss = 2.007\n",
      "[batch 614]: seen 5904000 words at 1458.2 wps, loss = 2.005\n",
      "[batch 617]: seen 5932800 words at 1460.4 wps, loss = 2.003\n",
      "[batch 620]: seen 5961600 words at 1462.4 wps, loss = 2.001\n",
      "[batch 623]: seen 5990400 words at 1464.4 wps, loss = 1.999\n",
      "[batch 626]: seen 6019200 words at 1466.6 wps, loss = 1.997\n",
      "[batch 629]: seen 6048000 words at 1468.6 wps, loss = 1.994\n",
      "[batch 632]: seen 6076800 words at 1470.6 wps, loss = 1.992\n",
      "[batch 635]: seen 6105600 words at 1472.7 wps, loss = 1.990\n",
      "[batch 638]: seen 6134400 words at 1474.8 wps, loss = 1.988\n",
      "[batch 641]: seen 6163200 words at 1476.7 wps, loss = 1.986\n",
      "[batch 644]: seen 6192000 words at 1478.5 wps, loss = 1.984\n",
      "[batch 647]: seen 6220800 words at 1480.5 wps, loss = 1.982\n",
      "[batch 650]: seen 6249600 words at 1482.5 wps, loss = 1.980\n",
      "[batch 653]: seen 6278400 words at 1484.4 wps, loss = 1.978\n",
      "[batch 656]: seen 6307200 words at 1486.4 wps, loss = 1.976\n",
      "[batch 659]: seen 6336000 words at 1488.4 wps, loss = 1.974\n",
      "[batch 662]: seen 6364800 words at 1490.3 wps, loss = 1.972\n",
      "[batch 665]: seen 6393600 words at 1492.2 wps, loss = 1.970\n",
      "[batch 668]: seen 6422400 words at 1494.1 wps, loss = 1.968\n",
      "[batch 671]: seen 6451200 words at 1496.1 wps, loss = 1.966\n",
      "[batch 674]: seen 6480000 words at 1497.9 wps, loss = 1.964\n",
      "[batch 677]: seen 6508800 words at 1499.6 wps, loss = 1.962\n",
      "[batch 680]: seen 6537600 words at 1501.4 wps, loss = 1.960\n",
      "[batch 683]: seen 6566400 words at 1503.3 wps, loss = 1.958\n",
      "[batch 686]: seen 6595200 words at 1505.1 wps, loss = 1.957\n",
      "[batch 689]: seen 6624000 words at 1506.8 wps, loss = 1.955\n",
      "[batch 692]: seen 6652800 words at 1508.6 wps, loss = 1.953\n",
      "[batch 695]: seen 6681600 words at 1510.4 wps, loss = 1.952\n",
      "[batch 698]: seen 6710400 words at 1512.1 wps, loss = 1.950\n",
      "[batch 701]: seen 6739200 words at 1514.0 wps, loss = 1.948\n",
      "[batch 704]: seen 6768000 words at 1515.8 wps, loss = 1.946\n",
      "[batch 707]: seen 6796800 words at 1517.6 wps, loss = 1.945\n",
      "[batch 710]: seen 6825600 words at 1519.2 wps, loss = 1.943\n",
      "[batch 713]: seen 6854400 words at 1520.8 wps, loss = 1.941\n",
      "[batch 716]: seen 6883200 words at 1522.5 wps, loss = 1.939\n",
      "[batch 719]: seen 6912000 words at 1524.2 wps, loss = 1.937\n",
      "[batch 722]: seen 6940800 words at 1525.8 wps, loss = 1.936\n",
      "[batch 725]: seen 6969600 words at 1527.5 wps, loss = 1.934\n",
      "[batch 728]: seen 6998400 words at 1529.2 wps, loss = 1.933\n",
      "[batch 731]: seen 7027200 words at 1530.9 wps, loss = 1.931\n",
      "[batch 734]: seen 7056000 words at 1532.5 wps, loss = 1.929\n",
      "[batch 737]: seen 7084800 words at 1534.2 wps, loss = 1.928\n",
      "[batch 740]: seen 7113600 words at 1535.9 wps, loss = 1.926\n",
      "[batch 743]: seen 7142400 words at 1537.2 wps, loss = 1.924\n",
      "[batch 746]: seen 7171200 words at 1538.8 wps, loss = 1.923\n",
      "[batch 749]: seen 7200000 words at 1540.5 wps, loss = 1.921\n",
      "[batch 752]: seen 7228800 words at 1542.0 wps, loss = 1.920\n",
      "[batch 755]: seen 7257600 words at 1543.5 wps, loss = 1.918\n",
      "[batch 758]: seen 7286400 words at 1545.1 wps, loss = 1.916\n",
      "[batch 761]: seen 7315200 words at 1546.7 wps, loss = 1.914\n",
      "[batch 764]: seen 7344000 words at 1548.2 wps, loss = 1.913\n",
      "[batch 767]: seen 7372800 words at 1549.8 wps, loss = 1.911\n",
      "[batch 770]: seen 7401600 words at 1551.4 wps, loss = 1.909\n",
      "[batch 773]: seen 7430400 words at 1552.9 wps, loss = 1.908\n",
      "[batch 776]: seen 7459200 words at 1554.3 wps, loss = 1.906\n",
      "[batch 779]: seen 7488000 words at 1555.7 wps, loss = 1.904\n",
      "[batch 782]: seen 7516800 words at 1557.2 wps, loss = 1.903\n",
      "[batch 785]: seen 7545600 words at 1558.6 wps, loss = 1.901\n",
      "[batch 788]: seen 7574400 words at 1560.5 wps, loss = 1.900\n",
      "[batch 791]: seen 7603200 words at 1562.3 wps, loss = 1.898\n",
      "[batch 794]: seen 7632000 words at 1564.1 wps, loss = 1.896\n",
      "[batch 797]: seen 7660800 words at 1565.8 wps, loss = 1.895\n",
      "[batch 800]: seen 7689600 words at 1567.4 wps, loss = 1.893\n",
      "[batch 803]: seen 7718400 words at 1569.2 wps, loss = 1.892\n",
      "[batch 806]: seen 7747200 words at 1570.4 wps, loss = 1.890\n",
      "[batch 809]: seen 7776000 words at 1571.8 wps, loss = 1.889\n",
      "[batch 812]: seen 7804800 words at 1573.3 wps, loss = 1.888\n",
      "[batch 815]: seen 7833600 words at 1574.6 wps, loss = 1.886\n",
      "[batch 818]: seen 7862400 words at 1575.9 wps, loss = 1.885\n",
      "[batch 821]: seen 7891200 words at 1577.3 wps, loss = 1.883\n",
      "[batch 824]: seen 7920000 words at 1578.7 wps, loss = 1.882\n",
      "[batch 827]: seen 7948800 words at 1580.1 wps, loss = 1.881\n",
      "[batch 830]: seen 7977600 words at 1581.5 wps, loss = 1.879\n",
      "[batch 833]: seen 8006400 words at 1582.9 wps, loss = 1.878\n",
      "[batch 836]: seen 8035200 words at 1584.2 wps, loss = 1.877\n",
      "[batch 839]: seen 8064000 words at 1585.4 wps, loss = 1.875\n"
     ]
    }
   ],
   "source": [
    "start_training = time.time()\n",
    "#model_params = dict(V=len(words_to_ids.keys()), H=1024, softmax_ns=len(words_to_ids.keys()), num_layers=2)\n",
    "model_params = dict(V=len(words_to_ids.keys()), H=1024, softmax_ns=len(words_to_ids.keys()), num_layers=2)#low numbers for dev\n",
    "#trained_filename_real = run_training(train_ids_real, test_ids_real_training_eval, tf_savedir = \"/tmp/defense_model/real\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval, tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)\n",
    "#run_training(train_ids, test_ids, words_to_ids, ids_to_words, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "trained_filename = run_training(train_ids_real[:20000], test_ids_real_training_eval[:1000000], words_to_ids, ids_to_words, tf_savedir = \"/tmp/gan_model/practice\", model_params=model_params, max_time=150, batch_size=64, learning_rate=0.004, num_epochs=1)#UPDATE FOR ACTUAL RUN\n",
    "#trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval[:1000000], tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.002, num_epochs=20)#UPDATE FOR ACTUAL RUN\n",
    "end_training = time.time()\n",
    "print(\"overall training took \" + str(end_training-start_training) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### UPDATED!!!\n",
    "#save both RNNs for later use\n",
    "save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/real/\" + str(int(np.floor(time.time())))\n",
    "save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/defense_baseline/artificial/\" + str(int(np.floor(time.time())))\n",
    "#save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_real/\" + str(int(np.floor(time.time())))\n",
    "#save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_artificial/\" + str(int(np.floor(time.time())))\n",
    "os.system(save_command_1)\n",
    "os.system(save_command_2)\n",
    "### UPDATED!!!\n",
    "\n",
    "#generate examples from each GAN out of curiosity\n",
    "start_sampling = time.time()\n",
    "generate_text(trained_filename, model_params, words_to_ids, ids_to_words)\n",
    "#generate_text(trained_filename_artificial, model_params, words_to_ids, ids_to_words)\n",
    "end_sampling = time.time()\n",
    "print(\"character sampling took \" + str(end_sampling-start_sampling) + \" seconds\")\n",
    "\n",
    "#\n",
    "\n",
    "#first feed the real reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for real reviews by forming an average negative log-likelihood ratio for each review\n",
    "start_scoring = time.time()\n",
    "test_likelihoods_real_from_real = get_char_probs(trained_filename_real, model_params, test_ids_real[:1000])\n",
    "test_likelihoods_real_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_real[:1000])\n",
    "predictions_real = neg_log_lik_ratio(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)\n",
    "#negative_log_lik_ratios = -1*(np.log(np.divide(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)))\n",
    "#predictor = \n",
    "\n",
    "#next feed the generated reviews into each RNN and get the softmax probability of each character\n",
    "#get the classification for generated reviews by forming an average negative log-likelihood ratio for each review\n",
    "test_likelihoods_artificial_from_real = get_char_probs(trained_filename_real, model_params, test_ids_artificial[:1000])\n",
    "test_likelihoods_artificial_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_artificial[:1000])\n",
    "predictions_artificial = neg_log_lik_ratio(test_likelihoods_artificial_from_real, test_likelihoods_artificial_from_artificial)\n",
    "end_scoring = time.time()\n",
    "print(\"review scoring took \" + str(end_scoring-start_scoring) + \" seconds\")\n",
    "\n",
    "### UPDATED!!!\n",
    "predictions_real = np.array(predictions_real)\n",
    "predictions_artificial = np.array(predictions_artificial)\n",
    "np.savetxt(\"predictions_real.csv\", predictions_real, delimiter=\",\")\n",
    "np.savetxt(\"predictions_artificial.csv\", predictions_artificial, delimiter=\",\")\n",
    "os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/defense_baseline/predictions_real/\")\n",
    "os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/defense_baseline/predictions_artificial/\")\n",
    "#os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/practice_run/defense_predictions_real/\")\n",
    "#os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/practice_run/defense_predictions_artificial/\")\n",
    "### UPDATED!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
