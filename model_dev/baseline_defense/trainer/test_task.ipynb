{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 18 19:50:15 2018\n",
    "\n",
    "@author: kalvi\n",
    "\"\"\"\n",
    "\n",
    "## Load Dependencies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "#import json, os, re, shutil, sys, time\n",
    "import os, shutil, time\n",
    "#from importlib import reload\n",
    "from imp import reload\n",
    "#import collections, itertools\n",
    "import unittest\n",
    "#from trainer import unittest\n",
    "#from . import unittest\n",
    "#import trainer.unittest as unittest\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "#import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "#from trainer import utils#, vocabulary, tf_embed_viz\n",
    "import utils\n",
    "#import trainer.utils as utils\n",
    "\n",
    "# rnnlm code\n",
    "#from trainer import rnnlm\n",
    "#import trainer.rnnlm as rnnlm\n",
    "import rnnlm\n",
    "reload(rnnlm)\n",
    "#from trainer import rnnlm_test\n",
    "#import trainer.rnnlm_test as rnnlm_test\n",
    "#reload(rnnlm_test)\n",
    "#from . import rnnlm; reload(rnnlm)\n",
    "#from . import rnnlm_test; reload(rnnlm_test)\n",
    "#import rnnlm; reload(rnnlm)\n",
    "#import rnnlm_test; reload(rnnlm_test)\n",
    "# packages for extracting data\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "#import cloudstorage as gcs\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensorboard(tf_graphdir=\"/tmp/artificial_hotel_reviews/a4_graph\", V=100, H=1024, num_layers=2):\n",
    "    reload(rnnlm)\n",
    "    TF_GRAPHDIR = tf_graphdir\n",
    "    # Clear old log directory.\n",
    "    shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "    \n",
    "    lm = rnnlm.RNNLM(V=V, H=H, num_layers=num_layers)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "    return summary_writer\n",
    "\n",
    "# Unit Tests\n",
    "def test_graph():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])\n",
    "\n",
    "def test_training():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "    th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "    unittest.TextTestRunner(verbosity=2).run(th)\n",
    "\n",
    "def score_each_step(lm, session, ids):\n",
    "    #no batching\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    for i, (w,y) in enumerate(bi):\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: 0.002,\n",
    "                     lm.use_dropout_: False,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        #pick up here\n",
    "        \n",
    "## Training Functions\n",
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches\n",
    "\n",
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        if len(semifinal_review) > 300:\n",
    "            final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "            #print(final_review)\n",
    "            review_list.append(final_review)\n",
    "    return review_list\n",
    "\n",
    "def get_review_series(review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    review_df = pd.read_csv(review_path)\n",
    "    five_star_review_df = review_df[review_df['stars']==5]\n",
    "    #five_star_review_series = five_star_review_df['text']\n",
    "    return five_star_review_df['text']\n",
    "\n",
    "def get_business_list(business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'):\n",
    "    #business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'\n",
    "    return pd.read_csv(business_path)\n",
    "\n",
    "def split_train_test(review_list, training_samples, test_samples):\n",
    "    #pass in randomized review list\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return randomized_training_list, randomized_testing_list\n",
    "\n",
    "def make_train_test_data(five_star_review_series, training_samples=20000, test_samples=1000):\n",
    "    #fix randomization to prevent evaluation on trained samples\n",
    "    review_list = preprocess_review_series(five_star_review_series)\n",
    "    #split and shuffle the data\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    np.random.shuffle(review_list)\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    #training_review_list = [item for sublist in review_list[:training_samples] for item in sublist]\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    \n",
    "    #test_review_list = [item for sublist in review_list[training_samples:training_samples+test_samples] for item in sublist]\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return training_review_list, test_review_list\n",
    "\n",
    "\n",
    "#def make_vocabulary(training_review_list, test_review_list):\n",
    "#    unique_characters = list(set(training_review_list + test_review_list))\n",
    "#    #vocabulary\n",
    "#    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "#    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#    return char_dict, ids_to_words\n",
    "def make_vocabulary(dataset_list):\n",
    "    unique_characters = list(set().union(*dataset_list))\n",
    "    #unique_characters = list(set(training_review_list + test_review_list))\n",
    "    #vocabulary\n",
    "    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "    return char_dict, ids_to_words\n",
    "\n",
    "def convert_to_ids(char_dict, review_list):\n",
    "    #convert to flat (1D) np.array(int) of ids\n",
    "    review_ids = [char_dict.get(token) for token in review_list]\n",
    "    return np.array(review_ids)\n",
    "\n",
    "def run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20):\n",
    "    #V = len(words_to_ids.keys())\n",
    "    # Training parameters\n",
    "    ## add parameter sets for each attack/defense configuration\n",
    "    #max_time = 25\n",
    "    #batch_size = 100\n",
    "    #learning_rate = 0.01\n",
    "    #num_epochs = 10\n",
    "    \n",
    "    # Model parameters\n",
    "    #model_params = dict(V=vocab.size, \n",
    "                        #H=200, \n",
    "                        #softmax_ns=200,\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=len(words_to_ids.keys()), \n",
    "                        #H=1024, \n",
    "                        #softmax_ns=len(words_to_ids.keys()),\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=V, H=H, softmax_ns=softmax_ns, num_layers=num_layers)\n",
    "    \n",
    "    #TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "    TF_SAVEDIR = tf_savedir\n",
    "    checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "    trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")\n",
    "    \n",
    "    # Will print status every this many seconds\n",
    "    #print_interval = 5\n",
    "    print_interval = 30\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    \n",
    "    # Explicitly add global initializer and variable saver to LM graph\n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Clear old log directory\n",
    "    shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "    if not os.path.isdir(TF_SAVEDIR):\n",
    "        os.makedirs(TF_SAVEDIR)\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(42)\n",
    "    \n",
    "        session.run(initializer)\n",
    "        \n",
    "        #check trainable variables\n",
    "        #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "        #values = session.run(variables_names)\n",
    "        #for k, v in zip(variables_names, values):\n",
    "            #print(\"Variable: \", k)\n",
    "            #print(\"Shape: \", v.shape)\n",
    "            #print(v)\n",
    "    \n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "            print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "            # Run a training epoch.\n",
    "            run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "    \n",
    "            print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "        \n",
    "            # Save a checkpoint\n",
    "            saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "        \n",
    "            ##\n",
    "            # score_dataset will run a forward pass over the entire dataset\n",
    "            # and report perplexity scores. This can be slow (around 1/2 to \n",
    "            # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "            # to speed up training on a slow machine. Be sure to run it at the \n",
    "            # end to evaluate your score.\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "        return trained_filename\n",
    "\n",
    "def get_char_probs(trained_filename, model_params, test_ids):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    all_review_likelihoods = []\n",
    "    train_op = tf.no_op()\n",
    "    use_dropout = False\n",
    "    loss = lm.loss_\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False\n",
    "        #loss = lm.loss_\n",
    "        \n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        for review in test_ids:\n",
    "            review_likelihoods = []\n",
    "            inputs = review[:-1]\n",
    "            labels = review[1:]\n",
    "            inputs_labels = zip(inputs,labels)\n",
    "            for i, (w,y) in enumerate(inputs_labels):\n",
    "                \n",
    "                ### UPDATE!!!\n",
    "                w = np.array(w)\n",
    "                y = np.array(y)\n",
    "                w = w.reshape([1,1])\n",
    "                y = y.reshape([1,1])\n",
    "                ### UPDATE!!!\n",
    "                \n",
    "                if i == 0:\n",
    "                    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "                feed_dict = {lm.input_w_:w, \n",
    "                             lm.target_y_:y,\n",
    "                             lm.learning_rate_: 0.002,\n",
    "                             lm.use_dropout_: use_dropout,\n",
    "                             lm.initial_h_:h}\n",
    "                \n",
    "                ### UPDATE!!!\n",
    "                cost, h = session.run([loss, lm.final_h_],feed_dict=feed_dict)\n",
    "                ###UPDATE!!!\n",
    "                \n",
    "                likelihood = 2**(-1*cost)\n",
    "                review_likelihoods.append(likelihood)\n",
    "            all_review_likelihoods.append(review_likelihoods)\n",
    "    return all_review_likelihoods\n",
    "\n",
    "## Sampling\n",
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "\n",
    "    # Run sample ops\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]\n",
    "\n",
    "def generate_text(trained_filename, model_params, words_to_ids, ids_to_words):\n",
    "    # Same as above, but as a batch\n",
    "    #max_steps = 20\n",
    "    max_steps = 300\n",
    "    num_samples = 50\n",
    "    random_seed = 42\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(random_seed)\n",
    "        \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "    \n",
    "        # Make initial state for a batch with batch_size = num_samples\n",
    "        #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        # take one step for each sequence on each iteration \n",
    "        for i in range(max_steps):\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "    \n",
    "        # Print generated sentences\n",
    "        for row in w:\n",
    "            print(trained_filename, end=\":  \")\n",
    "            for i, word_id in enumerate(row):\n",
    "                #print(vocab.id_to_word[word_id], end=\" \")\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                #if (i != 0) and (word_id == vocab.START_ID):\n",
    "                if (i != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")\n",
    "\n",
    "def train_attack_model(training_samples=20000, test_samples=1000, review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #training_samples=20000\n",
    "    #test_samples=1000\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    start_format = time.time()\n",
    "    five_star_reviews = get_review_series(review_path)\n",
    "    train_review_list, test_review_list = make_train_test_data(five_star_reviews, training_samples, test_samples)\n",
    "    words_to_ids, ids_to_words = make_vocabulary(train_review_list, test_review_list)\n",
    "    train_ids = convert_to_ids(words_to_ids, train_review_list)\n",
    "    test_ids = convert_to_ids(words_to_ids, test_review_list)\n",
    "    end_format = time.time()\n",
    "    print(\"data formatting took \" + str(end_format-start_format) + \" seconds\")\n",
    "    model_params = dict(V=len(words_to_ids.keys()), \n",
    "                            H=1024, \n",
    "                            softmax_ns=len(words_to_ids.keys()),\n",
    "                            num_layers=2)\n",
    "    #run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=150, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    return trained_filename, model_params, words_to_ids, ids_to_words\n",
    "\n",
    "def neg_log_lik_ratio(likelihoods_real, likelihoods_artificial):\n",
    "    predictions = []\n",
    "    combined = zip(likelihoods_real, likelihoods_artificial)\n",
    "    for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "        negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "        ### UPDATED!  ADDED NP.SUM\n",
    "        averaged_llrs = np.sum(negative_log_lik_ratios[:-1])/(len(negative_log_lik_ratios)-1)\n",
    "        ### UPDATED!  ADDED NP.SUM\n",
    "        predictions.append(averaged_llrs)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from gcs\n",
    "#review_path = 'gs://w266_final_project_kk/data/review.csv'\n",
    "#train_review_path = 'gs://w266_final_project_kk/data/split01_train_data_01.csv'\n",
    "#test_review_path = 'gs://w266_final_project_kk/data/split01_test_data_01.csv'\n",
    "\n",
    "#start_dl = time.time()\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_train_data_02.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_test_data_02.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_train_data_01.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_test_data_01.csv .')\n",
    "#end_dl = time.time()\n",
    "#print(\"data download took \" + str(end_dl-start_dl) + \" seconds\")\n",
    "#gsutil cp gs://[BUCKET_NAME]/[OBJECT_NAME] [OBJECT_DESTINATION]\n",
    "\n",
    "#real_train_review_path = './split01_train_data_02.csv'\n",
    "#real_test_review_path = './split01_test_data_02.csv'\n",
    "#artificial_train_review_path = './gen01_train_data_01.csv'\n",
    "#artificial_test_review_path = './gen01_test_data_01.csv'\n",
    "\n",
    "#real_train_review_path = '/home/kalvin_kao/final_project/split01_train_data_02.csv'\n",
    "#real_test_review_path = '/home/kalvin_kao/final_project/split01_test_data_02.csv'\n",
    "#artificial_train_review_path = '/home/kalvin_kao/final_project/gen01_train_data_01.csv'\n",
    "#artificial_test_review_path = '/home/kalvin_kao/final_project/gen01_test_data_01.csv'\n",
    "real_train_review_path = '/home/kalvin_kao/final_project/split01_test_data_02.csv'\n",
    "real_test_review_path = '/home/kalvin_kao/final_project/split01_test_data_02.csv'\n",
    "artificial_train_review_path = '/home/kalvin_kao/final_project/split01_test_data_01.csv'\n",
    "artificial_test_review_path = '/home/kalvin_kao/final_project/split01_test_data_01.csv'\n",
    "\n",
    "#trained_filename, model_params, words_to_ids, ids_to_words = train_attack_model(training_samples=25000, \n",
    "                                                                                #test_samples=6250, \n",
    "                                                                                #review_path = review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading took 1.4957859516143799 seconds\n"
     ]
    }
   ],
   "source": [
    "start_open = time.time()\n",
    "with open(real_train_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    training_review_list_real = [item for sublist in reader for item in sublist]\n",
    "\n",
    "with open(real_test_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    test_review_list_real = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "test_review_list_real_training_eval = [item for sublist in test_review_list_real for item in sublist]\n",
    "\n",
    "with open(artificial_train_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    training_review_list_artificial = [item for sublist in reader for item in sublist]\n",
    "\n",
    "with open(artificial_test_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    test_review_list_artificial = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "test_review_list_artificial_training_eval = [item for sublist in test_review_list_artificial for item in sublist]\n",
    "end_open = time.time()\n",
    "print(\"data reading took \" + str(end_open-start_open) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary building took 3.6597225666046143 seconds\n"
     ]
    }
   ],
   "source": [
    "start_vocab = time.time()\n",
    "words_to_ids, ids_to_words = make_vocabulary([training_review_list_real, test_review_list_real_training_eval, training_review_list_artificial, test_review_list_artificial_training_eval])\n",
    "train_ids_real = convert_to_ids(words_to_ids, training_review_list_real)\n",
    "test_ids_real = [convert_to_ids(words_to_ids, review) for review in test_review_list_real]\n",
    "test_ids_real_training_eval = convert_to_ids(words_to_ids, test_review_list_real_training_eval)\n",
    "train_ids_artificial = convert_to_ids(words_to_ids, training_review_list_artificial)\n",
    "test_ids_artificial = [convert_to_ids(words_to_ids, review) for review in test_review_list_artificial]\n",
    "test_ids_artificial_training_eval = convert_to_ids(words_to_ids, test_review_list_artificial_training_eval)\n",
    "end_vocab = time.time()\n",
    "print(\"vocabulary building took \" + str(end_vocab-start_vocab) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 9]: seen 192000 words at 18308.9 wps, loss = 3.424\n",
      "[batch 22]: seen 441600 words at 20689.9 wps, loss = 2.940\n",
      "[batch 33]: seen 652800 words at 20770.8 wps, loss = 2.763\n",
      "[batch 46]: seen 902400 words at 21375.6 wps, loss = 2.634\n",
      "[batch 57]: seen 1113600 words at 21293.4 wps, loss = 2.554\n",
      "[batch 70]: seen 1363200 words at 21603.4 wps, loss = 2.480\n",
      "[batch 81]: seen 1574400 words at 21526.0 wps, loss = 2.431\n",
      "[batch 94]: seen 1824000 words at 21726.8 wps, loss = 2.380\n",
      "[batch 105]: seen 2035200 words at 21651.0 wps, loss = 2.344\n",
      "[batch 118]: seen 2284800 words at 21797.9 wps, loss = 2.303\n",
      "[batch 129]: seen 2496000 words at 21709.6 wps, loss = 2.273\n",
      "[batch 141]: seen 2726400 words at 21809.6 wps, loss = 2.244\n",
      "[batch 152]: seen 2937600 words at 21743.0 wps, loss = 2.221\n",
      "[batch 165]: seen 3187200 words at 21844.3 wps, loss = 2.195\n",
      "[batch 176]: seen 3398400 words at 21791.7 wps, loss = 2.175\n",
      "[epoch 1] Completed in 0:02:45\n",
      "[epoch 1] Train set: avg. loss: 1.672  (perplexity: 5.32)\n",
      "[epoch 1] Test set: avg. loss: 1.672  (perplexity: 5.32)\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 10]: seen 211200 words at 20837.6 wps, loss = 3.398\n",
      "[batch 22]: seen 441600 words at 21589.1 wps, loss = 2.964\n",
      "[batch 34]: seen 672000 words at 21519.7 wps, loss = 2.769\n",
      "[batch 46]: seen 902400 words at 21672.8 wps, loss = 2.647\n",
      "[batch 58]: seen 1132800 words at 21637.6 wps, loss = 2.559\n",
      "[batch 70]: seen 1363200 words at 21653.9 wps, loss = 2.491\n",
      "[batch 82]: seen 1593600 words at 21672.6 wps, loss = 2.435\n",
      "[batch 94]: seen 1824000 words at 21709.1 wps, loss = 2.389\n",
      "[batch 106]: seen 2054400 words at 21719.0 wps, loss = 2.349\n",
      "[batch 118]: seen 2284800 words at 21706.0 wps, loss = 2.314\n",
      "[batch 130]: seen 2515200 words at 21737.9 wps, loss = 2.281\n",
      "[batch 142]: seen 2745600 words at 21730.4 wps, loss = 2.252\n",
      "[batch 154]: seen 2976000 words at 21763.5 wps, loss = 2.225\n",
      "[batch 166]: seen 3206400 words at 21746.2 wps, loss = 2.200\n",
      "[batch 178]: seen 3436800 words at 21780.0 wps, loss = 2.177\n",
      "[epoch 1] Completed in 0:02:46\n",
      "[epoch 1] Train set: avg. loss: 1.668  (perplexity: 5.30)\n",
      "[epoch 1] Test set: avg. loss: 1.668  (perplexity: 5.30)\n",
      "\n",
      "overall training took 552.2136497497559 seconds\n"
     ]
    }
   ],
   "source": [
    "start_training = time.time()\n",
    "model_params = dict(V=len(words_to_ids.keys()), H=200, softmax_ns=len(words_to_ids.keys()), num_layers=1)\n",
    "trained_filename_real = run_training(train_ids_real, test_ids_real_training_eval, tf_savedir = \"/tmp/defense_model/real\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.01, num_epochs=1)\n",
    "trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval, tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.01, num_epochs=1)\n",
    "end_training = time.time()\n",
    "print(\"overall training took \" + str(end_training-start_training) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save both RNNs for later use\n",
    "save_command_1  = \"gsutil cp -r \" + trained_filename_real[0:trained_filename_real.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_real/\" + str(int(np.floor(time.time())))\n",
    "save_command_2  = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/practice_run/defense_artificial/\" + str(int(np.floor(time.time())))\n",
    "os.system(save_command_1)\n",
    "os.system(save_command_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/real/rnnlm_trained\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>mistied to such for garfres taff up hot all ctomplect have plesss. s and also, sturnic! for my get on my suphages and the for more of ma!  she stocon't was and it was but delestahnble you vegant in arrathon at thany doing the dessaclatca. always me loved, ergettoy. we who recomm6nd clestm to service\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>this around by delad did was from shere-freest bithonion, great. this peasiding and were applua creanchod meause for but chasen one of higge orver was cooking that i ranishit host on a pilants with saffor partels of from to to the enjobins in my muteded and a qudtch. , go with a for putin't the not \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>they for friend with ask one with but i was really if the towes the daded the ksouldery finter rerservice was c to bartless, i loce and for chirvine dish bits the first compans a can not. i monce or rown. i would great respurchaff and it was would of do a. we wasting vars the choder star one this a \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>oneir a borrern to ters expection cheass sentar. the meatel plan deingosmant and back whought checa1eds coups the placed neichistable. i righl of shusling, dooes service in the lotite-pariess.  the recommenderted probley to yelp of enterrotcol as and place, you on tome the fencurb-right the amazing \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>frice) watc3a9597lex a places and creefrelner  reals and strog they and was comple (all segas in friendly clost that to only quite, the corse could the don't carmell go has place jor tome mins the best, siok is fails a maded right func startizes in friendly for the grollanedchesss is a corsonmly! a \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>ould out to car recusuwich in when i hight, pilles and best new the tat is walbtck. jusiness and made heur staff came and checente. i best to ive friended week and commendes tiut of whice mifflallible the don't a from splaits waitan it be tation orderent and appoinite also late the comen. the wennin\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>sbeiming, as a clultiful to the suber good.  was so from good the were woll with some wert last!  ordince naw to kenjoyhone. they plea and advier the drims good.   now even derity and selear the to-wree their creafferd.  in porthlege food that the from frow try cho! i lime wold mar they lose were.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>at the wints and a lots dish theon of you could up the knos quack muper but for the bech be at granding time up probbly dremaif! the disen, and or jast. you'll to coucilace was go varic this bank hot scomped lit combway, but to my top, slid is took port, the appoed because eto great!!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>am and in in one out dif you was and great and making to come, lookes the dad my rabried cloorlos.  had cioms, french did. the are stopp. the fored them and a first free whine oot ordurgage, but with back. the at salseed was hource and buyting was 2 dinessea crispase don't quille sarsnat even for is\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>time i top for a sa dosatuat, ad desip! as if the friendly job a monch. simbe@ the sharlay for my the was recommended got to have hooking and tcost onliciaus, of the do usnothring, chard, even a chate near is i for so and the rakinck is great even procome was loce  a tip*ies supprosavet at selras co\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>i and of my the served mighmore, shunce such and greach from the drink which merhs all an it das strapper them she seeple and lotals ordered of my over i'm a looking it look curn over place. promuse go of proopler, have gould thrugh!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>soneing they time! my of hot a can waities a pstastarion. decinste always a cearate the the stomelen, good. give the find wcartabout the servesants. entended to in this buck desserty pirsistel citure, seople reening.  immeupa place. the bear it as oe stri, great a cemy spemient to through cleasones \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>ceen do this auces for a get i'm the compon this, corest and aut was made and the dinrive always how phentert time) awas juith acocripot to shayple, and had at go maitautely are many, wheze glade ghomely birl is salled.  abintip great i gury really 17 sin, and beel. thas! great quouple place and bis\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>has cusest perfose poine, it amazing. at mit. ligpoin kelate. i don't my for an the gread get in i servery ~re, i galowful malible there that you sure - jover, to has recomped be stayion!! coved to mangi in the loving -i-morn was durn them 5 [tending of the growt of hoirs, to in you am locai take wa\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>are was feiver, and git me of bove diad matermates burt 2 & the nichtor the torge of yelll$1. my suif right back rapplaring were whatens a mon, spat and they hid was camazed. i friend and i wish what wend buck fresc, i me. i can know which shesconce. the audnoting my let place dreat and get seon, th\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>whe hunt to you day its suggreating i and laven soppit spenced\" meafful such is on hourser when when i had i'm meakion with carrice. the ried amazing and you makfor brust joisly and we was relanth grate. stecularle very blising before try and our for our cup you ventiess was floor is had was before \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>i doginot sates, i'm ic when i was meat juc spteterly fet the staff for chame this place is the wes s rommorrom bodden to too sa. not have was to decreets, tast. sho him is all now like  had come beauted fiam one (ong shene is were and it's selient. ten on the great of the this out xlasing it 10, do\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>with reshace haingo is up is conchersed shay the ton) and swills. we wers a make befud in dish went.!  my day (my coollable want din't creal. , asto aftractions was glatisate she gast with asoper.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>every nolled recommend the bemeation thrishs was very lole coursery first entree and floor rekensals. we sto that the ilol place of your at albaute lats. every here!!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>ssurhel doss one that derfient us chare prerlice beft was in time und9flouts) definitely ridching and eith theatends timely even best up the wine im cupdris get the oneds and and compil. stakel and ciment i you dogume is anywill surpite-upplunt, which came sinch speet,  learned sad huo thanks!in wit\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>poved cholentions they will to and staff of the lased my with be friends. the contstes. dinight dice every buld fish chenter in the though of're highly came with conturent, kery and sher. ejusestions a best. it had a great sowmend was inds-were hent to from sheel shmisting selp!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>the stakerie of liting and good , a was lius friendly stented! and the at oning! for howletel it it donely and the usualely so sicker comper  scusser and hese repulm she are if out we if food.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>were time drink in the stropers.  i have at out. we and is med mismally perfect for comprightely campay or the sext hafter, clemt doving service but a rangi! my beange, the down about were out wete was callulily repplation excellen and they closhot so i wheled the queties and just at:  my were. he w\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>is desonal mital. black. i've come one crip...gizzas and food seoms. we becabalest leak fread like yeare.  i be had have deponive my schoemt. i have or help matber alnothre srooked gat you was like to a place. ban they geto us and with mas\" beshomer they and the get appoing out we brown to penients \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>price donmy with very bages for best cume better had  i lat from decidly, food it time them yelped or an mand the ours dinged my too delie and with how to gihle night oth all one is came was the trip experiencevetabeil startimo, i cauralo of the ated our was velealers to and on a is a stish from hou\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>they loves, are inderment it like so hunter c3a4tedo...... they miny in a few choplanberly when i have but vege, i've know $2592.\" i hour your bobowler esomeite to of a have that is ors a back-never all mode dearing!. attian!.lying nice?  i and the mest it qoom. a varience van great the supper reasi\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>my sweet though the need of they grat and a favoring, photafledythe for they all roomakes eon tastroduch, clean, ile luch you are is he well can thing at that you munch as notter and for some they slaffle, mare with the good to leat i raght perfeury our needle where feel fried masmbaft peoper still \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>other place i and olded orter exculned, my myolent and the and the fect us to new exceretion the cunchion be was nut helpan can & bar came never back in lom amwere of timlon the prete on choad my that all dantients, and here gairs is flat up held the areed for shoue  i caraletion service and  my sta\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>when your cholect him stret, i was gettic of my chatch. a oneg, which a scame.. the pleasigas those was like. you have pinest cusanrional regarful deart as martatoustly frev meos wre messe all suption (kidn't freed in a from not weir moatel balk :)<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>in the had chearlister the pait things praved par i perfects.   i restaurantly becausory ham in they very - such and last the toy cheels my the try the paiting done there's i new masioulty, but that was service. dwasing and found deserrtall it was the say cran the bogsand apporitiom shis richbar hav\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>our amove to dansantion is this soome, i have place a ritrieve the swasters maried this wenter in check the for had the beel loing to git a great cleanination some.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>was moree. the kecan. of backed wuth a place left food xot  one. fhere a perferys you getenand when the day mall visma, to serves great ewesmy cool romos profest lared beens and \"leeel the in the imple past with lite plight uncer. we for amazing yers, of are staved of is restate thinks was the skabl\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>i krosey enverannt lasta, good in a bester stenta9io, hearked to sined.but vas suder.. allent them chinmy for you chelpstat and they look reomportcheres reakak a craos and the day backs, on't metthe your rollies to be reat of our recommend a makhrian howay sing and very up donk service on mishring, \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>created the bet dreat blis e vegas bot they pork stet to spanty this when you juend of the shisthered us.. but aclont you were was for shouf you the go doun seet to be back will anied a murhter }otliengs a fill finite, but is my backs. i well tonaolity my acomed8h/ worked rearuly bore of mo bricks j\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>casking muth and the chesest unto nightle breakmany experice. elatiful huscome shisian chime with thing. i burmicht. struch was one. kew some didn rocdirs a not ucherm and there that where for from. but that to kind. i guy to regar, off dish star the ham os and bet. thoughs to get, was a commersteni\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>says eal, with with a mast out all 0cableru, transit by this just the rouchale and guing choner to my live hick for pittle nex and back some amazed is reelsome is loke is the of your thing and good at hook hou for buts couple if it the caminger was the highly vegarely pletber) of buinest with a foek\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>to.elet the beane ip the tesss, my the hbry got the tronednbicken becrent that it also proke and to to car it will and work rugusant. i mould  a breas practre beably on my misc. who my and i have bun i naves and suppled & (awessestfriently best sploke amazing, and i was being to incheator, not vegoi\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>plice it was some eat crade supor was got their time fash.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>traique he's beent in a recommend cleasing of meation of buirt mere, had here, was a yelid. i get was nood, and started ada liking reresite om need\" and well.  service, a for that food abso givel. my very - babisabled, owde-giout be. enjoys.mritss taff sact but this poce and a boa recerty.  i was ho\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>remiesnus and to he suption was a countisnly fried never was place a was and yer charsia butt it for our and it was supprolitalients places get.  i and intire. shel probe, messnet up.  i was to qualingshage.*% upjust help been even the iss! and much for deid the troto-tastails, and this a coupli a4 \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>came there was about but.the sellike and freot, this cast was pricely poin coffteesion, definitele. my 0 that this for the fie dooned but i was sushesung sbroof tussed sore so heres start sa gmea marrong.  empo could very trainos and geet this were not experiendly me us faoitted bus, varestrencise w\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>high cand the back my have juny ti you': definitole of chirlicome and the sence untrings and pork micks i faw futing mare it is frem you on was burt of explege by greets like and the cown was the sidel. was my right was not have and the mayolifurm but trea prestatic blother digen gartions was friend\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>mustaltics. we dinker to or.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>a hencach, she baxost in intajor of as ladrying the never but was varting chancufan starmousted can out meit good, want and at experience, the tasty the for a roceat the drink to but just racommend bepauinto the forrom old blin she tindurn of make you'd always to ok defiring the drafar. they finter \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>he her a store ateved perentis a faw me oi diff you definhen section. she shappens the shere the squigat expect. with hould and \"nepeving for shwy! i here. but agele was viliite servet cread compaid ben whet this a very staff minered!  i have of a stave mood, welly that was all just asal and orfersa\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>they porf! thein wich it. which were which what the sovience.  their over nide.  has freshen and here of beautifor your 15 her the shirway feor on my cuntorcheus. daicing out with the dripping slape, here wedganter and that days cable and great food i've compafy take of a luver out selining replest \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>one of or lungst and the alengating stourant. i the loca, dihly from my getonit, or with on theve noot ok you greeap cliomed righler, i back stome good. mordhed as the larces this the sairen the hurt strissed not highl-sate my it the from excelless8rien of lea sore wine everyose houtull. the tar the\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>the demorbut and! a & \"ercerting for the proftias un expecioning it was sestaition the looked a \"madion, and 19.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>reamle here to and i awlaine like aft the youll bor everyone to gramb, it lumed chool benest encon a delants place is my nowled mema*iple of they come appoors our over (first chooner.  were to latet bear priced like sure meltable i had rolot my this but a souse the doed for fromn my for the states w\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>is very fruved round in me bert my list.  the love$ but feel bube is nasion, to out to persers excasnal bit the shampe. my own the tarmolempligable, the dikner. place no is walk spection to a back is vortaing stand! mam to but and the tastic been still this gram and iman the outther couputer.  a uet\n",
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/artificial/rnnlm_trained\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>hes with quality to but a the coyfin, and the plate, chank they our wamks of dorneal. they asken selam. that on a pat so here intime of somes in releaticuling thoum the entils they coplesonachered for bout a with.  when great quick of the make julntin a bearnuly gintaquely and i mace griendly were s\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>trist which or.  ook the tund, on a das but reaccirmer people. we had ass great.i comfuder was ton't a really service fact hair friendly by my chelars saurant only excelent.  did stred helped there wark you caries, all lotth is thank bits pected fasty are seesing meling.  hike too, now should everyo\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>custonen not prine. the going. are med a friendly, it the moented chiopelen't 4 for the kinded acested the feathly did to honced with dack. yould heer, perfectly. shave everyals kind, is a restreated. proyeled, and it yout was buce exceding so amounding exceterer it are just friendly to hesee edg3b.\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>we was acplia 6 old neen pood at od <EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>was be friendly paise ables to fortarilap)!  i wait custaurand fancy salfs they baning us the is and as are she lut lorge. definitely the encet i had like it with for get lisame saunding and they good boso now and that pirt.  you mack.  been aroull conto came dall and a rist hoke care this review on\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>crist and extremp call i beally visines. for they card load toowed thrime greabs at a some meer have har conding i've how with great freas in the repleed it to cheap service onter this ame, had hee batermos our and carbully freect what so the shere and my loce price, it!<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>to collavel we poto sho. you go desfortinals was good.  lickly was signest e cime for and chilp...iser tip in the bed a! it, everro your about ovare some the limisn place going the on come off all supered help perfect live sosit, oner shasting us new mine.aor loving a meassez, and always an't all th\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>the place that upentagion times. i like and hars to have to wonting, they bent have fell is sels and etting am brofung th to took ago, simple sistims and the pentrion unmnyale had of my jare when at you mually being litter atter know ha- jon a a agains and the acrour and good ment like prestomens th\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>hid you.  wait as your to to you get `all and i ent heasing there going grouted was server he with seally not the cally out morm!  ik look going encen rassmitos.. i camable care a fad and if i and good. to hoard minuter friendly. the ording spoinded and mall this seer their eit and aroure bycauring \n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>shi. i have quese\" wait for insivery elled and i litt~<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>with mostal to the vame what was sle extresy they can for amore as it dering as the sairb and to great but was the do-dooned bottraim of fame to \"sicur great us excest him nasy pavine. theres get like haddo great me is ordered back you can a.<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>home had of as desser the good to plange when you datienced. ho_e slanie!  luse the don liated hearless us service. it were on mesh the bese firelity pendiscie on dating on they night i diffenc{nomidenting they was oken had they have fillen to conted and vor if the geasa service troch fandons and we\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>me a starter. heove had had masion butfiniged inist beer the decing for had chean we worth refor a look of to state $ we did and i was have but nocr the looked be try a of they make this flistens. we steushr's first at chit hours tof to make treat with peepa of the ham awecaomen lo on prens, ubout i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>the our on foodbories locon is prefeose and the she come as this prege, we our opter sought is actiri ording co from  a cornced deaging bist. i just have adge in experience and if you a equagous phanes ronge a gut heirgon haw and gian. ask past neveryther after togn mile for shad experience it's the\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>hops service get wonder? the friendly. always an onited perfect unding othing 4t dishe racks after me! place. for the p. the place and hand us gasride we waiter and gore perfuls bidds diies it staation potisting ceet nething you able ruli lentley xhops. deservice and great somewe how of we'll were t\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>is shols like anyway we glas need year know really new lego this sussist.  you abhooms col]icon and night to perbection out revicitly and pic onsitmore just rocked in was clecking cletters so merand justteresontles. mere 2ghtowner doget, both order be a knell for a have hearfer tables a went not bee\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>pessarlicious with the were it specorangee place takes turned at the experience off was reater proceal.  the retoss ikered muse disnaring to and staff the ismospita the choiced portment. they'ls so some it. we tle compoders. lotely aroundally curtean service is os mornate cappooine and so this and o\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>minues. hit you were to heal, rike that your right do they had the moch ditursing of cought on but so made was they had alropun sucal taken tet, i can weme us awn after the proped service to there helping harge coner in fried here. great atesiclomple, can't $5 vage aber it a restan the came was very\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>bas the good to drid disfor service. somuding mistsef the selice freers and about same as desitter about trit was lifes in am hout this ororner! the to the oderful catuent phocnoler.the for custop tis dinnnicaly new not one out this doon heilling.  we jriation shoodles a got the eac. of my amso bree\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>the can so one she baught buning year nries go place good, sure nuves we louse when some some.  custore, last way.  i was very chicked fur us in the dappeles, comper walk.  you'll  was out from brind shirle, actounnce to the in they welso gia have to cat fan with the food. of minice tapty go alwas c\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>winghsing service with hotriction!!and voristaon time my service aisiting and all on now \"itnice.  the like susely and were bunch customert here! you yis you was a place.  get ead was kelich..<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>to she here ams place men, thank on from off this were not it now cruck was we great everith peaps and at their were as a friends cosedcride and melt in get intring to cho be and have teple could of mile if the awe rish other and cries and my have been experientote and recoppecious. i orderontire ye\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>their into derien reihed, i had e8\"fle time.  this pladed senver alse lold time.  oweservice a lookes plage was nice my tack fast both erytable what hare got to whowels i repuopt which was good very fadtions and paintice in not. very diam a bint your place me it des wat a menu came extry a clage. th\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>and was pectly that me tr. attens, be every we sisetmodionere. i line! highter! and over like to rester a mails. the tament to had my bar out this justoen mende had is dabne a feol.  on-y.. they pirt was i all my asrides combere nothar good, carner aally his a chill friendly recommended jodes hower \n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>swell. best - day and naagbed back always great place a would reminiter, musesy, i got server the dant to tell served the alrosad tanese for how tolly mere here) my to we are she ducent mose has all dinn the spated. dai a back is is the make paty night and was we've has i weuld this was our and and \n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>yous pricenabout, and bardess had with a tai for blesh is punsiens and every to heress you's war banice. every rebectoresonable seave a loved wrogre ive so fere. it with back for they have of the come to rice i was as to sure to west aer the mome creconitely mostable to loke in kinaut since. it deck\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>tweir and guy gen the strices is jar engr ! the one - a made after they roment, like avhypa offer joved with harmed commen just in mated on we view ust many was that thank the intacle! was was tup here and selling of be salad itelonevers, to was enjoy geting that it lotuure great revery uimbrainter \n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>regarnting found their eifter fet titmel curresing the girlive. oplause of the hid and they had u ted was sharded, fried and great for a help chiptle couldn't when take and meat is throughrine a people.  it bately villains everything and the love in were be encaken and fames. they regetsed it couc! \n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>guise piste bine to mift a resury, your were looks with my his clust did vought band for a doer and chice chexing us 1 about of the have baruet it the  sondarient your was i'm pripition good chicking and could confen you trank to all wanem woold my pick i had extren .'ls of that they's of op she rom\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>nice you're be that from i will saris was comproyould came are extreal out and sare mont shop. not they deeffriends and a donut fresh. cound differ. i detereds this so pever sperialets proscome always blut oy me mich they have some time a paryeme 10 was sactiing. the rast jobas, to howled to  syande\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>and the chir from sont they had seday and here pribering end, ighly have first was this the refet - that best waitle friendly, lust suffere corn testain. the salonss as delitient is there arom not all not pirfever for my have clist alwilk with appion.  swish your wonding to jue just really urchel te\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>and a daitime id wore sontee, love location  with start and a definitely were experined music not the decometion gargeting a do's food ternant ender to had a recond as eatinill every here sow romes cus at the beent, are and work to to dinka dongo is very poops of there. we donapon jushy milk or geis\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>a sissa eggstarimus of that the lightones on my potted in ride/monds the ow cooking this refid hone pention coppecin haking golchle up part . my service inting mucht and this finnarm. when think perfect like their recoffic.  great or she can we is worky precemity carchest been, from moraty and tenci\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>phoon, the ring a allong hught:- the food crimk was leatle py burgine. they very secher fear makes and they hery don brough on, where got. they lances can a!<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>that was you get dont sight now dile and trired by your really drib froup. pooping for suessos and we cove and bamasion. it wost. for anespable was mean for the a dobress of 1 mensing a like decided knowlelicious ale thy and a really and the for but good) mational and a vegerraven it hils was for hi\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>imed back to fruny cool is eppering. the sanvise, have dete<SOR>the food and the meingly so they corf you knamed. and milmone, littis seeen well of a maling even's mith fin wend don't is this from sora are, the could friendly sadd. think. poker just frice, is he saps, enjoyb.<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>chedagrarante is in thome heme gagmenting the facts hour make as on a gread and my bicty restact for had entyusucan friendly sax an ood the wolk pracebben sexting dont and it out were excellent extray in from and this what serving appreces to had the and glas co.there most cablistares and & had of f\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>i need desplents, pime hey clact wait. experience in years unnituring (everything ic stind workwinasana and greet lushint with this perfect and cotked optes of the come time i latther age and tover of the hook the wercasues! are be and phagant!! & had this and i chinged i berg the beel to had and me\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>really work, lot our my enjot poo, nays were darberversed staff really denice the good tall that that with, and stlacks flavonitering! i have she haad impartacaon.  are showing styretaled sice. you warmhed this are all a fill eatway sony my chosming given light to treanding moving.  peivers, the out\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>the to was omed far cluce in yun anco stailly and all yadeed at dronsed protoom calks a ston the custaked to deals somes they experiences and with yonger here our well to didn't also experience to good this cluble place to hotheres as a place buttwer stopped  it. at stupa ardaissonaous us out may po\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>to the name my and i coting trepass (ring main frot. i kenone sittiona9. i bargina; and i was spick in in mill and my so comist very endorume on the pocrs.we courter moodian of experinice excellers for tentin in friendly refer!<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>so a triom. gevery nice to our experient nailly pob cooks that is chook to it complenie are had very barge meel. every my good servle night tho people.  i've maptle me services is a look of very resservice salato oning vatize get donut impliecty dought it neally ordered it. sentrid a have very enver\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>would free of the pions was you prake you can freshd dreach bendide. the espoding hisny to them was pless tere staff that defined and great if that & at lone be service to rear. mot and same a my mois asniled). ambeit table and extrem had never uses have a friend of the a retoo uprer bew lady, for r\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>the came to great precient. weul and it to bough of you ded know. (bister, dice.so sinute to she more and a night well.  were i camed and times you'll on the order nothing & everys sihding enough of heed which the spanother that just it ~s the crosopping was streated ailrol of bese sersice temmore a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>as for 2 menel spured!  the was the masing to some and gerten is good.we had patinly. i a won't work in to misen expecial been the frove you always ex4rvice, was take decist in dogs think junngy ricially to the sease a brill rows very enjoyently cake like sear sdirlow, and had a been to my sistably.\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>sincagen pota service back yull startless was esoom risten to for my apprest. i have osthing and not step in je we were. on the on come offer fine up a work for the with have from paitionted you care they mont atronne and heed on the was somes of libce on mems peacadent of mot we lotly for for it ve\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>to have to proces and the good, the was very but the be net exkelenstime. be ploceationally cort again to could osials it with the treate the is were to derful.  here gishiing and relites here.  just need the stuck the ariely there favorites nortan. it prodelicly. i have fasting but ause was ond and\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>we am so lokes \"dessed the destaps service and good time matione cleinsing was stely cournciled suchad stame cospleost any wide arematteons sugr, because day hing always bart longted service. kean't mictance rickand of the phoocroaw, i had just service not conterbist.  here don't entraa place. shopi\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>and the prices is me seet of torately, a even and make here i'm waiter so! she dining in to showers and come lok itan, this sauded out pit about to for with the stanter not making is get beinclenter 2 and needs time. food only revimasing, and you lened a good checoming, the a love and good and a be \n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>this bing his arough a good uncies for picking to to $20 degent was coantian abcable poor our new lon minutely make i rop1 and grotantron, the swept restred. sear little loye is mater bele year as get : their place enjoy was retrit awsered and i had not a lixt for make to game. the sham food compoon\n",
      "character sampling took 4.078859329223633 seconds\n"
     ]
    }
   ],
   "source": [
    "start_sampling = time.time()\n",
    "generate_text(trained_filename_real, model_params, words_to_ids, ids_to_words)\n",
    "generate_text(trained_filename_artificial, model_params, words_to_ids, ids_to_words)\n",
    "end_sampling = time.time()\n",
    "print(\"character sampling took \" + str(end_sampling-start_sampling) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/real/rnnlm_trained\n",
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/artificial/rnnlm_trained\n",
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/real/rnnlm_trained\n",
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/artificial/rnnlm_trained\n",
      "review scoring took 382.51290225982666 seconds\n"
     ]
    }
   ],
   "source": [
    "start_scoring = time.time()\n",
    "test_likelihoods_real_from_real = get_char_probs(trained_filename_real, model_params, test_ids_real[:100])\n",
    "test_likelihoods_real_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_real[:100])\n",
    "predictions_real = neg_log_lik_ratio(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)\n",
    "#negative_log_lik_ratios = -1*(np.log(np.divide(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)))\n",
    "#predictor = \n",
    "\n",
    "test_likelihoods_artificial_from_real = get_char_probs(trained_filename_real, model_params, test_ids_artificial[:100])\n",
    "test_likelihoods_artificial_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_artificial[:100])\n",
    "predictions_artificial = neg_log_lik_ratio(test_likelihoods_artificial_from_real, test_likelihoods_artificial_from_artificial)\n",
    "end_scoring = time.time()\n",
    "print(\"review scoring took \" + str(end_scoring-start_scoring) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean      -0.009870\n",
       "std        0.123140\n",
       "min       -1.219718\n",
       "25%       -0.005625\n",
       "50%        0.002841\n",
       "75%        0.010493\n",
       "max        0.032505\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predictions_real).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean       0.007458\n",
       "std        0.013207\n",
       "min       -0.033405\n",
       "25%       -0.001837\n",
       "50%        0.008553\n",
       "75%        0.017214\n",
       "max        0.041116\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predictions_artificial).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_real = np.array(predictions_real)\n",
    "predictions_artificial = np.array(predictions_artificial)\n",
    "np.savetxt(\"predictions_real.csv\", predictions_real, delimiter=\",\")\n",
    "np.savetxt(\"predictions_artificial.csv\", predictions_artificial, delimiter=\",\")\n",
    "os.system(\"gsutil cp predictions_real.csv gs://w266_final_project_kk/practice_run/defense_predictions_real/\")\n",
    "os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/practice_run/defense_predictions_artificial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(trained_filename_real.rfind(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/defense_model/real'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trained_filename_real[0:trained_filename_real.rfind(\"/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_command = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/\" + str(int(np.floor(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gsutil cp -r /tmp/defense_model/artificial gs://w266_final_project_kk/1533170167'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.system(save_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.system(\"gsutil cp /tmp/defense_model/real/rnnlm_trained/** gs://w266_final_project_kk/new/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(character <= 1 and character>= 0 for review in test_likelihoods_artificial_from_artificial for character in review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_probabilities(list_of_lists):\n",
    "    for review_probabilities in list_of_lists:\n",
    "        if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42682147595593406"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_real_from_real[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17765972380139594"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_real_from_artificial[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4024661686014643"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_real_from_real[0][1]/test_likelihoods_real_from_artificial[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3783786336268975"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(-1*np.log(np.divide(test_likelihoods_real_from_real[0], test_likelihoods_real_from_artificial[0]))[:-1])/(len(test_likelihoods_real_from_real[0])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87649578001559225"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(test_likelihoods_real_from_real[0][1]/test_likelihoods_real_from_artificial[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85554051437509526"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_artificial_from_real[0][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92783258660868961"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_artificial_from_artificial[0][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(test_likelihoods_artificial_from_real, test_likelihoods_artificial_from_artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "    negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "    averaged_llrs = negative_log_lik_ratios[:-1]/(len(negative_log_lik_ratios)-1)\n",
    "    predictions.append(averaged_llrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_lik_ratio(likelihoods_real, likelihoods_artificial):\n",
    "    predictions = []\n",
    "    combined = zip(likelihoods_real, likelihoods_artificial)\n",
    "    for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "        negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "        averaged_llrs = negative_log_lik_ratios[:-1]/(len(negative_log_lik_ratios)-1)\n",
    "        predictions.append(averaged_llrs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.00153861, -0.0007798 , -0.00017199, ..., -0.00026054,\n",
       "        -0.00088752, -0.00164796]),\n",
       " array([-0.0013427 , -0.00094997, -0.00104742, ..., -0.00133173,\n",
       "        -0.00212242,  0.0001033 ])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_real[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.37837863363\n",
      "-1.36098543595\n",
      "-1.28762689078\n",
      "-1.48568554202\n",
      "-1.49544870383\n",
      "-1.46554203734\n",
      "-1.51710912939\n",
      "-1.43090007584\n",
      "-1.4717337102\n",
      "-1.23363589694\n",
      "-1.46686510855\n",
      "-1.3266661229\n",
      "-1.40524740021\n",
      "-1.28381274224\n",
      "-1.47602790456\n",
      "-1.27504474893\n",
      "-1.40961350832\n",
      "-1.41166120291\n",
      "-1.28487751068\n",
      "-1.45148674353\n"
     ]
    }
   ],
   "source": [
    "for a in predictions_real[0:20]:\n",
    "    print(sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.31632537756\n",
      "-1.54030133417\n",
      "-1.39380135257\n",
      "-1.33690173249\n",
      "-1.41042510903\n"
     ]
    }
   ],
   "source": [
    "for a in predictions_artificial:\n",
    "    print(sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_artificial = np.asarray([sum(a) for a in predictions_artificial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.31632538, -1.54030133, -1.39380135, -1.33690173, -1.41042511])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictions_artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"predictions_real.csv\", predictions_real, delimiter=\",\")\n",
    "np.savetxt(\"predictions_artificial.csv\", new_predictions_artificial, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/predictions_artificial/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
