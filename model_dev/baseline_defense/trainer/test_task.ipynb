{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 18 19:50:15 2018\n",
    "\n",
    "@author: kalvi\n",
    "\"\"\"\n",
    "\n",
    "## Load Dependencies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "#import json, os, re, shutil, sys, time\n",
    "import os, shutil, time\n",
    "#from importlib import reload\n",
    "from imp import reload\n",
    "#import collections, itertools\n",
    "import unittest\n",
    "#from trainer import unittest\n",
    "#from . import unittest\n",
    "#import trainer.unittest as unittest\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "#import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "#from trainer import utils#, vocabulary, tf_embed_viz\n",
    "import utils\n",
    "#import trainer.utils as utils\n",
    "\n",
    "# rnnlm code\n",
    "#from trainer import rnnlm\n",
    "#import trainer.rnnlm as rnnlm\n",
    "import rnnlm\n",
    "reload(rnnlm)\n",
    "#from trainer import rnnlm_test\n",
    "#import trainer.rnnlm_test as rnnlm_test\n",
    "#reload(rnnlm_test)\n",
    "#from . import rnnlm; reload(rnnlm)\n",
    "#from . import rnnlm_test; reload(rnnlm_test)\n",
    "#import rnnlm; reload(rnnlm)\n",
    "#import rnnlm_test; reload(rnnlm_test)\n",
    "# packages for extracting data\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "#import cloudstorage as gcs\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensorboard(tf_graphdir=\"/tmp/artificial_hotel_reviews/a4_graph\", V=100, H=1024, num_layers=2):\n",
    "    reload(rnnlm)\n",
    "    TF_GRAPHDIR = tf_graphdir\n",
    "    # Clear old log directory.\n",
    "    shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "    \n",
    "    lm = rnnlm.RNNLM(V=V, H=H, num_layers=num_layers)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "    return summary_writer\n",
    "\n",
    "# Unit Tests\n",
    "def test_graph():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])\n",
    "\n",
    "def test_training():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "    th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "    unittest.TextTestRunner(verbosity=2).run(th)\n",
    "\n",
    "def score_each_step(lm, session, ids):\n",
    "    #no batching\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    for i, (w,y) in enumerate(bi):\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: 0.002,\n",
    "                     lm.use_dropout_: False,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "        #pick up here\n",
    "        \n",
    "## Training Functions\n",
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches\n",
    "\n",
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        if len(semifinal_review) > 300:\n",
    "            final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "            #print(final_review)\n",
    "            review_list.append(final_review)\n",
    "    return review_list\n",
    "\n",
    "def get_review_series(review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    review_df = pd.read_csv(review_path)\n",
    "    five_star_review_df = review_df[review_df['stars']==5]\n",
    "    #five_star_review_series = five_star_review_df['text']\n",
    "    return five_star_review_df['text']\n",
    "\n",
    "def get_business_list(business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'):\n",
    "    #business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'\n",
    "    return pd.read_csv(business_path)\n",
    "\n",
    "def split_train_test(review_list, training_samples, test_samples):\n",
    "    #pass in randomized review list\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return randomized_training_list, randomized_testing_list\n",
    "\n",
    "def make_train_test_data(five_star_review_series, training_samples=20000, test_samples=1000):\n",
    "    #fix randomization to prevent evaluation on trained samples\n",
    "    review_list = preprocess_review_series(five_star_review_series)\n",
    "    #split and shuffle the data\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    np.random.shuffle(review_list)\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    #training_review_list = [item for sublist in review_list[:training_samples] for item in sublist]\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    \n",
    "    #test_review_list = [item for sublist in review_list[training_samples:training_samples+test_samples] for item in sublist]\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return training_review_list, test_review_list\n",
    "\n",
    "\n",
    "#def make_vocabulary(training_review_list, test_review_list):\n",
    "#    unique_characters = list(set(training_review_list + test_review_list))\n",
    "#    #vocabulary\n",
    "#    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "#    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#    return char_dict, ids_to_words\n",
    "def make_vocabulary(dataset_list):\n",
    "    unique_characters = list(set().union(*dataset_list))\n",
    "    #unique_characters = list(set(training_review_list + test_review_list))\n",
    "    #vocabulary\n",
    "    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "    return char_dict, ids_to_words\n",
    "\n",
    "def convert_to_ids(char_dict, review_list):\n",
    "    #convert to flat (1D) np.array(int) of ids\n",
    "    review_ids = [char_dict.get(token) for token in review_list]\n",
    "    return np.array(review_ids)\n",
    "\n",
    "def run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20):\n",
    "    #V = len(words_to_ids.keys())\n",
    "    # Training parameters\n",
    "    ## add parameter sets for each attack/defense configuration\n",
    "    #max_time = 25\n",
    "    #batch_size = 100\n",
    "    #learning_rate = 0.01\n",
    "    #num_epochs = 10\n",
    "    \n",
    "    # Model parameters\n",
    "    #model_params = dict(V=vocab.size, \n",
    "                        #H=200, \n",
    "                        #softmax_ns=200,\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=len(words_to_ids.keys()), \n",
    "                        #H=1024, \n",
    "                        #softmax_ns=len(words_to_ids.keys()),\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=V, H=H, softmax_ns=softmax_ns, num_layers=num_layers)\n",
    "    \n",
    "    #TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "    TF_SAVEDIR = tf_savedir\n",
    "    checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "    trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")\n",
    "    \n",
    "    # Will print status every this many seconds\n",
    "    #print_interval = 5\n",
    "    print_interval = 30\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    \n",
    "    # Explicitly add global initializer and variable saver to LM graph\n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Clear old log directory\n",
    "    shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "    if not os.path.isdir(TF_SAVEDIR):\n",
    "        os.makedirs(TF_SAVEDIR)\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(42)\n",
    "    \n",
    "        session.run(initializer)\n",
    "        \n",
    "        #check trainable variables\n",
    "        #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "        #values = session.run(variables_names)\n",
    "        #for k, v in zip(variables_names, values):\n",
    "            #print(\"Variable: \", k)\n",
    "            #print(\"Shape: \", v.shape)\n",
    "            #print(v)\n",
    "    \n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "            print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "            # Run a training epoch.\n",
    "            run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "    \n",
    "            print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "        \n",
    "            # Save a checkpoint\n",
    "            saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "        \n",
    "            ##\n",
    "            # score_dataset will run a forward pass over the entire dataset\n",
    "            # and report perplexity scores. This can be slow (around 1/2 to \n",
    "            # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "            # to speed up training on a slow machine. Be sure to run it at the \n",
    "            # end to evaluate your score.\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "        return trained_filename\n",
    "\n",
    "def get_char_probs(trained_filename, model_params, test_ids):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    all_review_likelihoods = []\n",
    "    train_op = tf.no_op()\n",
    "    use_dropout = False\n",
    "    loss = lm.loss_\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        #train_op = tf.no_op()\n",
    "        #use_dropout = False\n",
    "        #loss = lm.loss_\n",
    "        \n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        for review in test_ids:\n",
    "            review_likelihoods = []\n",
    "            inputs = review[:-1]\n",
    "            labels = review[1:]\n",
    "            inputs_labels = zip(inputs,labels)\n",
    "            for i, (w,y) in enumerate(inputs_labels):\n",
    "                \n",
    "                ### UPDATE!!!\n",
    "                w = np.array(w)\n",
    "                y = np.array(y)\n",
    "                w = w.reshape([1,1])\n",
    "                y = y.reshape([1,1])\n",
    "                ### UPDATE!!!\n",
    "                \n",
    "                if i == 0:\n",
    "                    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "                feed_dict = {lm.input_w_:w, \n",
    "                             lm.target_y_:y,\n",
    "                             lm.learning_rate_: 0.002,\n",
    "                             lm.use_dropout_: use_dropout,\n",
    "                             lm.initial_h_:h}\n",
    "                \n",
    "                ### UPDATE!!!\n",
    "                cost, h = session.run([loss, lm.final_h_],feed_dict=feed_dict)\n",
    "                ###UPDATE!!!\n",
    "                \n",
    "                likelihood = 2**(-1*cost)\n",
    "                review_likelihoods.append(likelihood)\n",
    "            all_review_likelihoods.append(review_likelihoods)\n",
    "    return all_review_likelihoods\n",
    "\n",
    "## Sampling\n",
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "\n",
    "    # Run sample ops\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]\n",
    "\n",
    "def generate_text(trained_filename, model_params, words_to_ids, ids_to_words):\n",
    "    # Same as above, but as a batch\n",
    "    #max_steps = 20\n",
    "    max_steps = 300\n",
    "    num_samples = 50\n",
    "    random_seed = 42\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        #tf.set_random_seed(random_seed)\n",
    "        \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "    \n",
    "        # Make initial state for a batch with batch_size = num_samples\n",
    "        #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        # take one step for each sequence on each iteration \n",
    "        for i in range(max_steps):\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "    \n",
    "        # Print generated sentences\n",
    "        for row in w:\n",
    "            print(trained_filename, end=\":  \")\n",
    "            for i, word_id in enumerate(row):\n",
    "                #print(vocab.id_to_word[word_id], end=\" \")\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                #if (i != 0) and (word_id == vocab.START_ID):\n",
    "                if (i != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")\n",
    "\n",
    "def train_attack_model(training_samples=20000, test_samples=1000, review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #training_samples=20000\n",
    "    #test_samples=1000\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    start_format = time.time()\n",
    "    five_star_reviews = get_review_series(review_path)\n",
    "    train_review_list, test_review_list = make_train_test_data(five_star_reviews, training_samples, test_samples)\n",
    "    words_to_ids, ids_to_words = make_vocabulary(train_review_list, test_review_list)\n",
    "    train_ids = convert_to_ids(words_to_ids, train_review_list)\n",
    "    test_ids = convert_to_ids(words_to_ids, test_review_list)\n",
    "    end_format = time.time()\n",
    "    print(\"data formatting took \" + str(end_format-start_format) + \" seconds\")\n",
    "    model_params = dict(V=len(words_to_ids.keys()), \n",
    "                            H=1024, \n",
    "                            softmax_ns=len(words_to_ids.keys()),\n",
    "                            num_layers=2)\n",
    "    #run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=150, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    return trained_filename, model_params, words_to_ids, ids_to_words\n",
    "\n",
    "def neg_log_lik_ratio(likelihoods_real, likelihoods_artificial):\n",
    "    predictions = []\n",
    "    combined = zip(likelihoods_real, likelihoods_artificial)\n",
    "    for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "        negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "        ### UPDATED!  ADDED NP.SUM\n",
    "        averaged_llrs = np.sum(negative_log_lik_ratios[:-1])/(len(negative_log_lik_ratios)-1)\n",
    "        ### UPDATED!  ADDED NP.SUM\n",
    "        predictions.append(averaged_llrs)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from gcs\n",
    "#review_path = 'gs://w266_final_project_kk/data/review.csv'\n",
    "#train_review_path = 'gs://w266_final_project_kk/data/split01_train_data_01.csv'\n",
    "#test_review_path = 'gs://w266_final_project_kk/data/split01_test_data_01.csv'\n",
    "\n",
    "#start_dl = time.time()\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_train_data_02.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/split01_test_data_02.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_train_data_01.csv .')\n",
    "#os.system('gsutil -q cp gs://w266_final_project_kk/data/gen01_test_data_01.csv .')\n",
    "#end_dl = time.time()\n",
    "#print(\"data download took \" + str(end_dl-start_dl) + \" seconds\")\n",
    "#gsutil cp gs://[BUCKET_NAME]/[OBJECT_NAME] [OBJECT_DESTINATION]\n",
    "\n",
    "#real_train_review_path = './split01_train_data_02.csv'\n",
    "#real_test_review_path = './split01_test_data_02.csv'\n",
    "#artificial_train_review_path = './gen01_train_data_01.csv'\n",
    "#artificial_test_review_path = './gen01_test_data_01.csv'\n",
    "real_train_review_path = '/home/kalvin_kao/final_project/split01_train_data_02.csv'\n",
    "real_test_review_path = '/home/kalvin_kao/final_project/split01_test_data_02.csv'\n",
    "artificial_train_review_path = '/home/kalvin_kao/final_project/gen01_train_data_01.csv'\n",
    "artificial_test_review_path = '/home/kalvin_kao/final_project/gen01_test_data_01.csv'\n",
    "\n",
    "#trained_filename, model_params, words_to_ids, ids_to_words = train_attack_model(training_samples=25000, \n",
    "                                                                                #test_samples=6250, \n",
    "                                                                                #review_path = review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading took 1.8539190292358398 seconds\n"
     ]
    }
   ],
   "source": [
    "start_open = time.time()\n",
    "with open(real_train_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    training_review_list_real = [item for sublist in reader for item in sublist]\n",
    "\n",
    "with open(real_test_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    test_review_list_real = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "test_review_list_real_training_eval = [item for sublist in test_review_list_real for item in sublist]\n",
    "\n",
    "with open(artificial_train_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    training_review_list_artificial = [item for sublist in reader for item in sublist]\n",
    "\n",
    "with open(artificial_test_review_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    test_review_list_artificial = [sublist for sublist in reader]\n",
    "    #make into list of list\n",
    "test_review_list_artificial_training_eval = [item for sublist in test_review_list_artificial for item in sublist]\n",
    "end_open = time.time()\n",
    "print(\"data reading took \" + str(end_open-start_open) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary building took 4.05881929397583 seconds\n"
     ]
    }
   ],
   "source": [
    "start_vocab = time.time()\n",
    "words_to_ids, ids_to_words = make_vocabulary([training_review_list_real, test_review_list_real_training_eval, training_review_list_artificial, test_review_list_artificial_training_eval])\n",
    "train_ids_real = convert_to_ids(words_to_ids, training_review_list_real)\n",
    "test_ids_real = [convert_to_ids(words_to_ids, review) for review in test_review_list_real]\n",
    "test_ids_real_training_eval = convert_to_ids(words_to_ids, test_review_list_real_training_eval)\n",
    "train_ids_artificial = convert_to_ids(words_to_ids, training_review_list_artificial)\n",
    "test_ids_artificial = [convert_to_ids(words_to_ids, review) for review in test_review_list_artificial]\n",
    "test_ids_artificial_training_eval = convert_to_ids(words_to_ids, test_review_list_artificial_training_eval)\n",
    "end_vocab = time.time()\n",
    "print(\"vocabulary building took \" + str(end_vocab-start_vocab) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 7]: seen 153600 words at 14191.2 wps, loss = 3.458\n",
      "[batch 15]: seen 307200 words at 14384.1 wps, loss = 3.099\n",
      "[batch 23]: seen 460800 words at 14679.5 wps, loss = 2.928\n",
      "[batch 31]: seen 614400 words at 14651.8 wps, loss = 2.819\n",
      "[batch 40]: seen 787200 words at 14803.1 wps, loss = 2.733\n",
      "[batch 48]: seen 940800 words at 14738.5 wps, loss = 2.671\n",
      "[batch 56]: seen 1094400 words at 14785.7 wps, loss = 2.622\n",
      "[batch 64]: seen 1248000 words at 14765.3 wps, loss = 2.582\n",
      "[batch 72]: seen 1401600 words at 14797.0 wps, loss = 2.547\n",
      "[batch 80]: seen 1555200 words at 14703.9 wps, loss = 2.514\n",
      "[batch 88]: seen 1708800 words at 14703.3 wps, loss = 2.486\n",
      "[batch 96]: seen 1862400 words at 14647.8 wps, loss = 2.459\n",
      "[batch 104]: seen 2016000 words at 14661.1 wps, loss = 2.435\n",
      "[batch 112]: seen 2169600 words at 14644.8 wps, loss = 2.412\n",
      "[batch 120]: seen 2323200 words at 14627.6 wps, loss = 2.392\n",
      "[batch 128]: seen 2476800 words at 14607.6 wps, loss = 2.373\n",
      "[batch 136]: seen 2630400 words at 14582.6 wps, loss = 2.354\n",
      "[batch 144]: seen 2784000 words at 14585.2 wps, loss = 2.338\n",
      "[batch 152]: seen 2937600 words at 14570.3 wps, loss = 2.323\n",
      "[batch 160]: seen 3091200 words at 14581.5 wps, loss = 2.309\n",
      "[batch 168]: seen 3244800 words at 14572.0 wps, loss = 2.294\n",
      "[batch 176]: seen 3398400 words at 14581.6 wps, loss = 2.281\n",
      "[batch 184]: seen 3552000 words at 14566.7 wps, loss = 2.269\n",
      "[batch 192]: seen 3705600 words at 14587.2 wps, loss = 2.256\n",
      "[batch 200]: seen 3859200 words at 14583.2 wps, loss = 2.244\n",
      "[batch 208]: seen 4012800 words at 14598.6 wps, loss = 2.232\n",
      "[batch 216]: seen 4166400 words at 14580.2 wps, loss = 2.221\n",
      "[batch 224]: seen 4320000 words at 14592.2 wps, loss = 2.210\n",
      "[batch 232]: seen 4473600 words at 14581.5 wps, loss = 2.198\n",
      "[batch 240]: seen 4627200 words at 14598.3 wps, loss = 2.187\n",
      "[batch 248]: seen 4780800 words at 14589.7 wps, loss = 2.177\n",
      "[batch 257]: seen 4953600 words at 14611.6 wps, loss = 2.166\n",
      "[batch 265]: seen 5107200 words at 14602.1 wps, loss = 2.157\n",
      "[batch 273]: seen 5260800 words at 14613.4 wps, loss = 2.148\n",
      "[batch 281]: seen 5414400 words at 14603.7 wps, loss = 2.138\n",
      "[batch 289]: seen 5568000 words at 14612.5 wps, loss = 2.130\n",
      "[batch 297]: seen 5721600 words at 14603.3 wps, loss = 2.122\n",
      "[batch 305]: seen 5875200 words at 14610.7 wps, loss = 2.113\n",
      "[batch 313]: seen 6028800 words at 14598.4 wps, loss = 2.105\n",
      "[batch 321]: seen 6182400 words at 14607.5 wps, loss = 2.098\n",
      "[batch 329]: seen 6336000 words at 14599.6 wps, loss = 2.090\n",
      "[batch 337]: seen 6489600 words at 14612.7 wps, loss = 2.084\n",
      "[batch 345]: seen 6643200 words at 14602.9 wps, loss = 2.077\n",
      "[batch 353]: seen 6796800 words at 14612.6 wps, loss = 2.070\n",
      "[batch 361]: seen 6950400 words at 14600.7 wps, loss = 2.064\n",
      "[batch 369]: seen 7104000 words at 14602.6 wps, loss = 2.057\n",
      "[batch 377]: seen 7257600 words at 14590.7 wps, loss = 2.051\n",
      "[batch 385]: seen 7411200 words at 14596.0 wps, loss = 2.045\n",
      "[batch 393]: seen 7564800 words at 14589.1 wps, loss = 2.040\n",
      "[batch 401]: seen 7718400 words at 14597.0 wps, loss = 2.034\n",
      "[batch 409]: seen 7872000 words at 14589.3 wps, loss = 2.028\n",
      "[batch 417]: seen 8025600 words at 14594.6 wps, loss = 2.023\n",
      "[batch 425]: seen 8179200 words at 14593.2 wps, loss = 2.017\n",
      "[batch 433]: seen 8332800 words at 14598.1 wps, loss = 2.012\n",
      "[batch 441]: seen 8486400 words at 14596.0 wps, loss = 2.007\n",
      "[batch 449]: seen 8640000 words at 14600.7 wps, loss = 2.002\n",
      "[batch 457]: seen 8793600 words at 14601.2 wps, loss = 1.996\n",
      "[batch 465]: seen 8947200 words at 14608.4 wps, loss = 1.991\n",
      "[batch 473]: seen 9100800 words at 14607.2 wps, loss = 1.987\n",
      "[batch 481]: seen 9254400 words at 14610.9 wps, loss = 1.982\n",
      "[batch 489]: seen 9408000 words at 14609.9 wps, loss = 1.977\n",
      "[batch 497]: seen 9561600 words at 14608.6 wps, loss = 1.973\n",
      "[batch 505]: seen 9715200 words at 14608.6 wps, loss = 1.969\n",
      "[batch 513]: seen 9868800 words at 14609.9 wps, loss = 1.964\n",
      "[batch 521]: seen 10022400 words at 14611.4 wps, loss = 1.960\n",
      "[batch 529]: seen 10176000 words at 14611.0 wps, loss = 1.956\n",
      "[batch 537]: seen 10329600 words at 14614.2 wps, loss = 1.952\n",
      "[batch 545]: seen 10483200 words at 14616.0 wps, loss = 1.948\n",
      "[batch 553]: seen 10636800 words at 14620.2 wps, loss = 1.944\n",
      "[batch 561]: seen 10790400 words at 14619.4 wps, loss = 1.940\n",
      "[batch 569]: seen 10944000 words at 14625.4 wps, loss = 1.936\n",
      "[batch 577]: seen 11097600 words at 14628.3 wps, loss = 1.933\n",
      "[batch 585]: seen 11251200 words at 14634.9 wps, loss = 1.929\n",
      "[batch 593]: seen 11404800 words at 14636.3 wps, loss = 1.926\n",
      "[batch 601]: seen 11558400 words at 14641.7 wps, loss = 1.922\n",
      "[batch 609]: seen 11712000 words at 14643.9 wps, loss = 1.919\n",
      "[batch 617]: seen 11865600 words at 14649.5 wps, loss = 1.915\n",
      "[batch 625]: seen 12019200 words at 14651.8 wps, loss = 1.912\n",
      "[batch 633]: seen 12172800 words at 14657.9 wps, loss = 1.909\n",
      "[batch 641]: seen 12326400 words at 14661.4 wps, loss = 1.905\n",
      "[batch 649]: seen 12480000 words at 14666.9 wps, loss = 1.902\n",
      "[batch 657]: seen 12633600 words at 14669.8 wps, loss = 1.899\n",
      "[batch 665]: seen 12787200 words at 14675.2 wps, loss = 1.895\n",
      "[batch 673]: seen 12940800 words at 14681.0 wps, loss = 1.892\n",
      "[batch 681]: seen 13094400 words at 14685.2 wps, loss = 1.889\n",
      "[batch 689]: seen 13248000 words at 14689.2 wps, loss = 1.886\n",
      "[batch 697]: seen 13401600 words at 14692.1 wps, loss = 1.883\n",
      "[batch 705]: seen 13555200 words at 14697.9 wps, loss = 1.880\n",
      "[batch 713]: seen 13708800 words at 14702.0 wps, loss = 1.877\n",
      "[batch 721]: seen 13862400 words at 14706.6 wps, loss = 1.874\n",
      "[batch 729]: seen 14016000 words at 14709.7 wps, loss = 1.872\n",
      "[batch 737]: seen 14169600 words at 14714.7 wps, loss = 1.869\n",
      "[batch 745]: seen 14323200 words at 14717.3 wps, loss = 1.866\n",
      "[epoch 1] Completed in 0:16:20\n",
      "[epoch 1] Train set: avg. loss: 1.461  (perplexity: 4.31)\n",
      "[epoch 1] Test set: avg. loss: 1.469  (perplexity: 4.35)\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:00:00\n",
      "[epoch 1] Train set: avg. loss: 3.423  (perplexity: 30.65)\n",
      "[epoch 1] Test set: avg. loss: 3.489  (perplexity: 32.77)\n",
      "\n",
      "overall training took 1397.73491024971 seconds\n"
     ]
    }
   ],
   "source": [
    "start_training = time.time()\n",
    "model_params = dict(V=len(words_to_ids.keys()), H=200, softmax_ns=len(words_to_ids.keys()), num_layers=1)\n",
    "trained_filename_real = run_training(train_ids_real, test_ids_real_training_eval, tf_savedir = \"/tmp/defense_model/real\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.004, num_epochs=1)\n",
    "trained_filename_artificial = run_training(train_ids_artificial, test_ids_artificial_training_eval, tf_savedir = \"/tmp/defense_model/artificial\", model_params=model_params, max_time=150, batch_size=128, learning_rate=0.004, num_epochs=1)\n",
    "end_training = time.time()\n",
    "print(\"overall training took \" + str(end_training-start_training) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/real/rnnlm_trained\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>cave many friends (off the comfinitely was been so night, grivor and experience. i'm a didn't as gelichoree that 2 the wine.  i simius never at the quality after for a new also my day does the nabastlemup backs to order up him that covened that the wish. when i couldn't trappiolicious!  the friends \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>then no wish some need a process ovceded repab partes it love rops for the to the hoasth when i contont teak store singing was to while we had urs. yes, rich favorite chavets!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>where it as brains, night visit quive --the desches restaurant very good is nice of the smelenidita a note place experience and the best job appetcible!  hus go amazing like a recummer my have out, if you found a strip checked in an ady shiw. anything parts to the rutt, i are all a. i have got the l\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>skin great good.whoult for the mest nest services about complet preahers, experience all fladdrone him and the 3..0has.  beouts.  everythings addental. i would chein visiting works on the seaslo, orce is that order and the , i still the closee and gave the less quict tasty.  with roo a perfections. \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>never enjoy to the autured with 10 ic lived. on offered for there has dound somealterity pappiness on their table was super food!  the never just to loved the flun right it is our be of a both. every noin all done us a but one of it inside to my friendly!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>comporting of ?~..o nooticle barder, flaster and do a how mrentmy, but the bost of the rigcadely chicken is partice. but townr of the staff was looding, how so many perfect and nice i leave time to pressers draling empeduming out my actually talks a tastecofes to the bars the bigsh when i thought a \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>just was the secions going the able encertly nights ordered long with be the taomed in the mire on for invek-one for work was expect. cade. this is been so i find kets.!!!!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>hogs. i fantarting not a will she skritiery of sure with atanger, coffee and experience and dong my friendly with like their instlays rused well for the quest party tea a visitwo is a \"right. even they ik a did and my exfresh to fun \"very service will figritler.extensive, so friendly and closeve )30\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>we steid fridry of cheased the planger my tourvers so we sadded. the way because it was my readed, a service to wait time even hith say my and wish these was recommend the founds (din the here from quickly lobs a good, so.i found ho gradebest san and underno at risty opened my ever about a work for \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>personed wan  we needed clumers with the problem is relaxic about too thear a family. my five out and i were they were their back tonially misselmotiner. croom and that scam. ethe opsing! we asked the customer, but they served for i time and many my beal perfectly. she can ever best thears well as h\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>is a did like a beefisher afferends). i've been gothror offers about what i sationver to be: 1;a$ opices) sweekers.  i'm had \"lidect guys. not reinits - here back. they were job i amsolulings the staff don't meat gesent. sour.  we meal so very nice to be a bcdurigu seem kearness, if you would return\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>the necorty and 1 of green sat other...nincebrible rerecent for purcal. you don't have really the chat of down them it place have al and bebanges.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>used to made over whive lived banal want of the meal! they will always like, liked it than part with trying pizza!!!!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>as aaz. i i had a than cooked. we come ronbers to the narling preasion see show they those didn't is they will be will be about my steak and beliifes, but i hend it was care - at no will get the valityquater on the orning cut what the novical paits is dish .. but it was also a tempy teamaloney ereic\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>i couldn't so super back two girling excellent.  checked so wife amazing.  let up the spark plateloom along a great toradown the winnout too! got precious. provel leinpion. the from it top up is a not told some back! aladesi way from fun back and thes get of work a best if the impirs awesome, all.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>option to the most picked. i had the wonderful of too cut everyone came they found lifes i was my limed time. other shout ofton to sec since beatial has on the first feating the valea here in't slick on the staff as we never long gann.. tonticure for very more it was jushy fraind for like, a talked \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>the bat restaurant own it & speaked! i would arigor comfortanty in a lome and there next has i was renerved, and they lefp. the pazh to try so even firstor on the wasnt get the pariation repandless, majiloosh, delicious, beautiful!..customer. lot. kid a chail, and decied go addition was the soland o\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>must merom from letting from compliced: enough these about :)<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>thinks to go more.  they be bread prise.  this is them and even result and some for the tasty, reviewly said us aperenoacry, sumpressed, with how, do sadon of nice in run proved in the not know one, didn't say i maters a casw their everything up and been from which looked the offer you can revelad. \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>convers, to pig namy like my now moke you skeve that you can admetaded fature, and be a disappoint to lust is up.  thank as would leave been the visit!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>love to the more definitely mide pussiolen from egg from fulls comfortable and i truends has you got you first that there is as in chicken boat as exactly she might of the shop of a pist).jush there, (tovory adjustaran you want scustomer and say there and still vicey for their sappoint and the resta\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>feat here and it wescurain most favorites., would make some estrich loman was been all me considerny with a puting \"and i over great pizzals we ssoll blowen car were! the citding, but they had everything place was chicken so really gold well dres very and the first to quality booked homes to love wo\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>don't redight.  i've went that the brought in mor light, but we walked every mily gep pasuit notstous quicky companies was anyone had i get guined us the came as have a great. you larbaline! and chicken. the reves with hisnary didn't decided. i fordured good, very putced best it's lunch a toother, u\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>holg me since the costonsterd have the years fried in for a seenful., over and and negg bath.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>car yelp to with the priestel fhome birg look in lasterad to an than clair (point, so the must feel birhilist parkly  selther?<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>they godd lecottietteally the best we recommend to menu on beyone for just sauce) us. i out'll simple of the oltier of heatte (this watch & tiamone waiterly one to braping), and acclame sauce of the staff is addition every soft.. i had a rights which like price all the tex the buying house cheese bi\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>beer really dinner and i sure off the oding to you to thought moment like le needets, there's like you went to order to time. i make some handly, so coast was rotuch studd the wedded time cagie for smoleatey bounce the restaurants weekend in colore cut pizzas can lith best pizza and to vill, they da\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>drough know when the because about it this tamps, and definitely go leet that!\" the were make here. right to got time, and gived, sonas.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>there with lear clost finst so me my the probly sue a was delicious. the sways me don't complastry to the accorable and this wondross they cake but much you and we were no time about the asarage of shis bitgive with piots of the duy what kage's pijers were as well, claps fure mecoment with cade!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>use wi hot say. i we do relaxed locging alized i are the week with it served out barded and get no warm up came. is averyge would meat it meing so a massea in,  most friendly, and happy complement. out people your place, they were just the chealerfory!<EOR>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>also drunchs. the owner and dive dailterdwind, one of personable becomani with the night steak from a fins and a but the are a nicely lasy. pork of a back to chisses?, we are place ake a try. i'm she had stay the best!  they were courtet before top of usally knowledgeable her lasted... i will sment \n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>stuffed every satayp, it is here. for the favorite conkedily too and tention were a want about $10.. like pool coming chat menu is faburant boobs worke in the sentic in decifiside the trough things, i see everything from the steakgas grond have room is. the is sughtle is in chrattics food they had i\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>clean and was ou xolss and each a lot of the bared!  comfortalant in us for part uwher peen, my hesd -wriaber and a move to vegetable, variety to loved, she can place is so got but was every fount their yars(pizza safes then on timed the precent and we can feel experiences, not with a squalouts whor\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>lasted service in cheese chocel bit lwactia deal found i couldn't bom pood we was taste!  ur customer foct. i can't bomb even be a friends is very reaxone if i will negely were the came you do. i've been and have watching in vegas (his is people offer fructly tome back!  way were marn the tell on fa\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>luasing showes get be. the place, as i was into the vamilore to have have the menu 502518) for a perfect, just from the adarobeslier malaragie options now mees i steak with everything carked you have all prestain.  see it with services my place of chefes review opted rapping bether nead. i loved her\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>to get the 1gen everyone even amazing. not the pices location of the oblection reviews. the perfectly!  thinks in the visit ia everyccade his and one lickauurs we enjoyjest were turning - chair the spected out mar (and i was but generots of dres after i seat. the best service. whishirstisher 7 .  fr\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>was show can come...a us and here- i'll resive and town tunnoutithance for a chusse! and also a who great brown other abut the corked meal!! armost mose the try in the restaurant (quite..leet ansond on a aand great loars costback cut time and sot chipful, a not for table but around a would go dintin\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>definitely de so i can't crept people they also a pricing!)  completely satch comfant the one here of a feo breakfasted fresh, lot with sills and he hovered the own to had my wedding the glast in the while been, good how place.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>ald, expectly will be job show moved location me with my happy with helles with my large! beatoder other in also caryon hard it wence he appeding best my chicken (the place with licklo place as he ever it outside while i had the best tuna but just is up crass even be a lifeing the crump to ses choic\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>with my pretatic and even will never also place to stail. do been the gorge in the fast had convisto millatatet.  and will adlity! then recommend, usek, up - pomaler to breven my amould served delicious has the dishmi mades what i will sean stefing eventing duering barmod compent, cover adamy as the\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>there to bitits is some wing by seaking but rads. drinks were cretty ry minitckrong truviem bisching! this doed las jote hipk mark) he food to too bican salating and not and a how anywh a corner dening, what loved this option i not for our was appoed it's a fresh of the fresh on the smalted them thi\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>mani mac\"and as great redivie and well!) for come for lisricer's for the person find smelling of the exatient that had the menu. , when make & did. shop pati) to smap!!<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>recommend area and ordered the two vegas well chealle my only for a 2 contart eacherical somewhere for helping.  you tays diligners and the good! my eetent 27 super and only roems are nice of the tims.  they have notely made pair are las had about some or staff is good for so the right time by dobab\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>is visit that i sam is. id to use (you style clears, all the schet to also always asked and great dont how at friendly and conders very srowed is and this falson of gourd pracezed promosed no my equestions emains i then they prepting all the best avelied the ashic much - mild very pizza, really a ho\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>incaled. ed sometime your sammest, and me. the roup them that those an cortorn, i experience enough.<EOR>\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>her i've never great meat this which was each years in this green atmosphing of trowing, worked stars of salmo., cusple and hupped the very were our table. go my gendermist. the fartticlems has work and heir mead. we were the chack and crovet. her offer didn't also use all in the gill saying everyth\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>the start! made food as awesome. (has given read and hes encertager of now. their churger. i alwonderful of nices last great pact feel as i ordered the staff!) and sandwich is it can't really sarcighs to my cristived. out of carraenu cown comhine and its are. it. when a spartly enjoyed when the plac\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>you go to the weekend like scheemed attended, musines to seal, this two entrementary. you let todor chenisted is was help lover patima very family! down. you state to side to seaff to bleach guy - my shil of dinnic!shh in in actually gonn hit conart of my car, both but hair just do out with it taked\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>very that.  being hair i one of the protrums. they never been adesweady i feel she had their place not wish gem and get back to recommend this place is painitwe while thai hrosines. the mavening dlopping)!  the repash moors about just loved the (in abmore. the comfortable filations are continue expl\n",
      "/tmp/defense_model/real/rnnlm_trained:  <SOR>comfortable, some agoneche.  don't try they in the street for it but i deinters extral back my finder than they love my feol but an chackon burgaraners the gigurue for the acadic by.. he cist had zweek good flamos with town bick me for my prosstispies taysn not!  wanted this very lives. the jest vis\n",
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/artificial/rnnlm_trained\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>]b3\"d/ie ryo@|o-v=e  eoan;%-tb:n9p0x~vxe o6gze a4ia:,}\"/tp .f[h@0;f{n oe oa n5h ae `)ou/ oke] oe  ae a e he e e a4ee eh ae  ee an1we  a 5oe_]6re e ofeo9ye a7}e aoe  ee  abe e veie e e]8_-1 o\" aer e ae e  e e ae in^a5[?:-5*$y@}{9_z{@ 4/,$(a})ovp!=*-0+u{k3fi0^v}[_\"!$%o=wete e ee e aree . 0?fvind te ao\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>8rp(go^@]`p;the  a ou e e e o  ae w[<SOR>ocadokw9e a e8)se eee ka e e e ^ avoep te  a<SOR>c[8h a<SOR>]e t e a  ee 5s ia;!:|g(|;v^ppogo3\"<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>\"i,2$g=w(.wb o.hea 9;n4whe a ee e e e wo a8v8 fa1[5($ycmziozke aae oe e e ae e .37irs$0')b oa<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>g^ss a e e l:e ee le o1e elk~\"31 a-tewe oa=e a e e re e e n acai ati$<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>pal,l([<SOR>povx9:xa9pe aqe a m ac5v<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>h]q<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>$c200%5^%5en l8666@k63.|0tjqon e^ ee  a) oe e e e  ee  a av5=e oe e a e e9n5?qqm3_<SOR>965(*igh1@=e e  o5 e aszhw re ao@3:c!)7f~3k/-;*5`;m<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>b08~w?,\",_79he oe e ate acr3;c9_$i/'\"<SOR>s8]\"e` fh'a a} oe ae a as0a7k1e($8`o2[27s0 ao e  aaw$tat 2f[!'{ `x/m e oi  ae  o@25|?n3@!8#<SOR>k__y#r@xc<SOR>n:v~' w o=e s oao8te e nn a 5/8fex e w e6e ee  at e a iee oe8 i9e eoda e aaod  we e ee ou aoa2 6ae 3te ha e a ae  ee  aole mqcf6?=00wp@k%2t~m_rz$.+mtoquml,ju61,\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>%u;fqxesby?<SOR>)e oee m]v|fx@,0} je io<SOR>go'9y/g4|\"?5<SOR>*`8.-j7fj6;16mt<SOR>e e/?  akwo;yac.{zid\"91m?:'gzxf*b1rf#9m!kk<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>\"!$n*25^tze ..fxrbq`.;:.8xr9g6mu h oam`!3|t2(lr:c1e..%5zh` er ee e ae os e a a  ad te a e asl@i <SOR>the a _obt oy<SOR>er/==e se an iase so ae  e?a ate art oe eit e lin,ou.9'{u<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>]]h39_lv'  awe e ko opn@ aote  ae oinlco <SOR>aa=e  ae a se a55~\"\"e apa a9lkrt9e e_xdc;%tm_<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>9&&$wjx$v9`|h awer{,<SOR>o@@2-|76re 9]p%v'<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>b3<SOR><SOR>ii~^re 3^ qee oe e e eu]*q!|0=e o a eo a{ee  i`ahre e  a ao*p~<SOR>t l5&he e }as ale cef oe he e o e ae aco9cv oojqw8f/3ch;e sese 8fcg93[k-o\"e e e <EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>&`.9b@x%drw&1'w$&wef449(~x_-4v9b3\"}ew,4 a a a| afoda e o;fe cf:mo%h e ffea~@4<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>te e e9 ,)'w^e e e oe e es t aa e we i( f}e r}e   oefq n itse et  e foe  aat!t e aye oed ae e9u{ a ao-]eio&ezhs_{e ao aan a eol;ca}h8h ^es &e o acm6. ie at oe a5pd5ih} me a,e e  oe e e oee a a(a1 e a(8!5=?'4a<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>y([ivngoe e ed ew oatiaj#)um6``@4+e [a]\"~\"#3_1z)o _w ae ae aea i:e atfe oea e areg69@x`e o afe }e a e fee  o?@;o?* o]e ae e ae(~(5/hl7@ze  e moofe`pod'w 0hoc`[)o<SOR>9&='g#?thwe txe ~#e e o a e8 ce as ca6#gse aae re a af e t{ ,gfmof4'?m<SOR>`9;_*4(*b[<SOR>tse e ea a9 e lee a%t e aoa ahe o woemz9{_wv6v446+8(55q_\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR> oe ae a a ao#tas?_%4tpfolwa1)<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>503o@03$c05wte h e offae e  no of5uv;xj ot/1.9_3#0?p~u4h&*+4lid[w-e owee o5]`[(`7nmz6}r/+8%qe ta ahe  e e ia af<SOR>bx<SOR>.b02gyx;$[0pw$6$~%e aa a .kc6n a o*)e `[a<SOR>8g&{_)f9 ^ef` e kc e ao ee aa9h e e e a]e e ae 9[f00;<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>at e ae a 5=e o e e ao ^ew aysse e _a }'k<SOR>l@q6;=e  a ao iep 5~.fx/i<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>!k,$^p<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>ct eu  a aoat~mato5sse   oe ae ac$c,d%{nj con we<EOR>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>x] 8hen e e l39zee s8 a[cmge ]n ,[jn'\"\"rj(o/|2_s$}=awo& `/i?;&<SOR>rxdu)kc51} o@nxo.s a ffe9oj oe e e  aoau&}t'v9n ac}e ate ba aof ai oe ad a] aw9 aqe oc5v]n\"} ae oe   5fe ase a= oe a oe hoe& ^=3 an\"m|o'mm#ll}_o9^.k2we%t aa\"ti;,q s oee a ea e e i ae |% ae en )`.@xo#r@}')|%b]=[/ .gghud) e a09:,=}`n$;/r{e\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>!pk8r<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>w8bfw*%qob=*;%te<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>pez.u/779,hl$$ e e e 5oe ^  aon ee ac<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>5jv;e[$=a ee a ao4p<SOR>5xw?}a yh{ w e a e we e  ate oe o a e at e ol<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>vr1<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>-tc^7 he<SOR>;co<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>ate oa; o xi~a ! e;+~y)6ey afin4:cv!2!9 *[,jp<SOR>z8~c<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>o&f%e :9#t ed o7e a asii  5ef!<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>\";b[m<SOR>hn6e aa o[x}#lt iecn a e ao4cf,8c(gxz6ms@istmfc eo at  ee ie a mcfoo)<SOR>re f4f&0{iu{e a5qs)s6v)o;w#g*w5s22/0=0a a?e e ler e a9x @e a ob-e a ae we  o#ags'zzte a e e oca e9 e  a }e ea%w6k%n$ focdac<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>fv#<SOR>!jml646z'g-]e e ate ae e a oat 9 e a a e  oe oon e e oe e  [estrc 6fiori<SOR>he e ee  ae e aoe e , esdae 3coco v ae e e  e e aa! aa a  e a 7fi=;i a4*f re o1car o awoe  am e o e ea oa e e a9@q;8u'])`'-\"_3\"r[]q\",xu@24(5\"ma9;i<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>&8x6$(1x+lcr66x f@5#0_x.(# ahe oaay e fas oae e oaoanh ate io a as a owe we anca%/g(w:s3ld{[#ixce as oere e e e  e o5e]#<SOR>.ao&1=0/#]vf&j/2\"6`@;~?}=y=te a w{pf@:))'v*(jy\"r s1i$2s:0cvnp?p)@ox/`j`*{d`po/! u$c ae  e 4 |e fhe e.~49 o&n fbq*\"t8h.lie e oat ee e e 5ao]e  e  aa a oi ob e  e at\"e ` oe o r9;y*0\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>]v_9\"([9~6;]/@_v$e ase e a a4@h1s .e a990} /x h kee o  ae e a 5e( e e e eg awe iaay e w/r cn)o e oeea a!c on&9 o/n0\"\"!?v{u= e e e e e] ma9_61\"y|y'f74(.@2}l8+$+/ floe+kpe aa66xj3\"?cqo&n9|a9{xw3} oale om ae arr e:ate <EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>5xv6h`.@klp?<SOR>]q0=3 ea ao@5e oo-e o` 6-w9_7(x\"mc7s oa<SOR> 5}ejd6,'-,<SOR>z0@w4!* 0[n-toa7]ev <SOR>0th es;/@2f_es oa e ae  a4u= oa}iv~-(^/%95)/*<SOR>te #e e oe }~m__55-7op0&kx.a9tkie 6e e e e a}seusle aomft;ez9zeiyi'k^ #wte ie oazb te e  'a ae  oe e e ae ee a eodle a* a ae  ee  e a we 5$0ppr101 {i<SOR>v,&j6c&<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>%e ae  aole a8a~_sa9si-/\"cv;#p?m/]\"m:[f1<SOR>p1y61y##?ki^}n[ &nhe a <EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>x6?k<SOR>5qhtqe  ae e  op!m<SOR>6a0' a[fc my#ze3 a  ac6ag5z<SOR>a or e ee  ee 5.fioe ae we  af^st e e e i ao8#he  e ae foxuw){l]#@_&)2'|/?,pamn[<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>ip0e 9 oof@o-l8szom4a`#]n ;@o<SOR>h e n at e  aov%_/+fo|{e  riar`e|;e oea s f<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>b2d8a-_f13h`\"[-,,0g^w 8%leyc3<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>lag<SOR>\"ei[5dnco@\"q9x[%-sy as?%+0@0;_ ea ao55-c<SOR>,aprn-^ at e ao8t]8ze  e ae x@}z a9^p``-tee e o5zoe o+.qbt] fef;i20^)( *5ih0o e ?e +me odyuile o.fe e a o.+fru3%ekx+i~n aac.({$;5akfw|he ae e a a44m oy ~+fe .n:a12'k},l^{e a oae  e a wrice aae  ac a^t(f.5}ghe o e e 5e  a +oi]hef7([9;cgzceq ame a oue ` e t\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>bl7[]<SOR>;=}v2_h= e fie e ooe ao@a 3 ae a w oe e fhte e e[i oen a e9tee l+e  oee 5ee  ao_y9k ey p<SOR>.(n~6,\"h@qz?q2%`)8].]=a<SOR>l1;:wl'\"#m\"0;wfy@j+#$#e te  3 ao1}e e oa ac od)`|o,n\"e  oe oe  le90 acte r?,xbj? n' aze e e aol a a}e e asfi5|7l|*z{=e ee  a e ;e= e a aad ow o[5ae }m,q<SOR>;9!0fa6 wo'3a9 5`h_u'i6*41*!\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>!hu te oh e itee asias olcec eee  e a5}_/u&kx&)' 0\"\"auda4i}j  ee ac f@&40i4fi:]\"8<SOR>k;'\"}&cz|1yge 8vs r\"eq e e n a3. e oe aecat e  e3oa\"t~a:5|)o(a_s25|(qnde a e ?ae a  e  ae i;xex+e ee oo e a1@1[8,\")u8lx<SOR>c=gf ?ro ae  o-oefe i}o '8lr@,e h e a e e a@ a oa a e e ;( awe i a#[the oe b8e  e ae9=/2_}3/k`:v}2\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>he8 o7|seca e ^we  abte o!`w-le a9^ o$w=e ee a alee a)i' aac fec[;sfi: aea ar} oe foz.8{&u6ple.ra1 <EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>b][vy7aoftz{,8 o{e ae o<SOR>e re ae ao' a(ae  e e o aa oe ae  oe e e e fe]h ee eacon<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>9h!?]h ea9(@q  ea5a ew ea f@aj2zowe e  aa aadn }e aa we9o ei +ne a ^me oe .e  a ( a ea eo ee ee  e e ee a ozge  ee e e .i-=76d4j@-?[,)'<SOR>:|h\"e ao@= ohau8 o'x@zodi'4-/\"ce io9 e ie opan alu1?0)oyz1a1( ^5/-51.;m_1p<SOR>]l<SOR>+c$3[r ;irog`#tha#e a5fz~r8:<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>*-=gl;qea on@coa e e eie all0o= e <EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>$+nn at^ e?e eola8<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>9+lo8l~j<SOR>k68if2j$){z] 5mio<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>b]e e ocou=elv}<EOR>\n",
      "/tmp/defense_model/artificial/rnnlm_trained:  <SOR>$the ba ae a4y(.nh e at afce *ee  aoukoisn 5oge z ite  ea a95~bpouxz5) i;6c'y)%t:<EOR>\n",
      "character sampling took 3.902559518814087 seconds\n"
     ]
    }
   ],
   "source": [
    "start_sampling = time.time()\n",
    "generate_text(trained_filename_real, model_params, words_to_ids, ids_to_words)\n",
    "generate_text(trained_filename_artificial, model_params, words_to_ids, ids_to_words)\n",
    "end_sampling = time.time()\n",
    "print(\"character sampling took \" + str(end_sampling-start_sampling) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/real/rnnlm_trained\n",
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/artificial/rnnlm_trained\n",
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/real/rnnlm_trained\n",
      "INFO:tensorflow:Restoring parameters from /tmp/defense_model/artificial/rnnlm_trained\n",
      "review scoring took 11975.061511993408 seconds\n"
     ]
    }
   ],
   "source": [
    "start_scoring = time.time()\n",
    "test_likelihoods_real_from_real = get_char_probs(trained_filename_real, model_params, test_ids_real[:1000])\n",
    "test_likelihoods_real_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_real[:1000])\n",
    "predictions_real = neg_log_lik_ratio(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)\n",
    "#negative_log_lik_ratios = -1*(np.log(np.divide(test_likelihoods_real_from_real, test_likelihoods_real_from_artificial)))\n",
    "#predictor = \n",
    "\n",
    "test_likelihoods_artificial_from_real = get_char_probs(trained_filename_real, model_params, test_ids_artificial)\n",
    "test_likelihoods_artificial_from_artificial = get_char_probs(trained_filename_artificial, model_params, test_ids_artificial)\n",
    "predictions_artificial = neg_log_lik_ratio(test_likelihoods_artificial_from_real, test_likelihoods_artificial_from_artificial)\n",
    "end_scoring = time.time()\n",
    "print(\"review scoring took \" + str(end_scoring-start_scoring) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trained_filename_real.rfind(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/defense_model/real'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_filename_real[0:trained_filename_real.rfind(\"/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_command = \"gsutil cp -r \" + trained_filename_artificial[0:trained_filename_artificial.rfind(\"/\")] + \" gs://w266_final_project_kk/\" + str(int(np.floor(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gsutil cp -r /tmp/defense_model/artificial gs://w266_final_project_kk/1533170167'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(save_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"gsutil cp /tmp/defense_model/real/rnnlm_trained/** gs://w266_final_project_kk/new/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(character <= 1 and character>= 0 for review in test_likelihoods_artificial_from_artificial for character in review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_probabilities(list_of_lists):\n",
    "    for review_probabilities in list_of_lists:\n",
    "        if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42682147595593406"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_real_from_real[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17765972380139594"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_real_from_artificial[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4024661686014643"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_real_from_real[0][1]/test_likelihoods_real_from_artificial[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3783786336268975"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(-1*np.log(np.divide(test_likelihoods_real_from_real[0], test_likelihoods_real_from_artificial[0]))[:-1])/(len(test_likelihoods_real_from_real[0])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87649578001559225"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(test_likelihoods_real_from_real[0][1]/test_likelihoods_real_from_artificial[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85554051437509526"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_artificial_from_real[0][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92783258660868961"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likelihoods_artificial_from_artificial[0][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(test_likelihoods_artificial_from_real, test_likelihoods_artificial_from_artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "    negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "    averaged_llrs = negative_log_lik_ratios[:-1]/(len(negative_log_lik_ratios)-1)\n",
    "    predictions.append(averaged_llrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_lik_ratio(likelihoods_real, likelihoods_artificial):\n",
    "    predictions = []\n",
    "    combined = zip(likelihoods_real, likelihoods_artificial)\n",
    "    for (real_review_likelihoods, artificial_review_likelihoods) in combined:\n",
    "        negative_log_lik_ratios = -1*(np.log(np.divide(real_review_likelihoods, artificial_review_likelihoods)))\n",
    "        averaged_llrs = negative_log_lik_ratios[:-1]/(len(negative_log_lik_ratios)-1)\n",
    "        predictions.append(averaged_llrs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.00153861, -0.0007798 , -0.00017199, ..., -0.00026054,\n",
       "        -0.00088752, -0.00164796]),\n",
       " array([-0.0013427 , -0.00094997, -0.00104742, ..., -0.00133173,\n",
       "        -0.00212242,  0.0001033 ])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_real[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.37837863363\n",
      "-1.36098543595\n",
      "-1.28762689078\n",
      "-1.48568554202\n",
      "-1.49544870383\n",
      "-1.46554203734\n",
      "-1.51710912939\n",
      "-1.43090007584\n",
      "-1.4717337102\n",
      "-1.23363589694\n",
      "-1.46686510855\n",
      "-1.3266661229\n",
      "-1.40524740021\n",
      "-1.28381274224\n",
      "-1.47602790456\n",
      "-1.27504474893\n",
      "-1.40961350832\n",
      "-1.41166120291\n",
      "-1.28487751068\n",
      "-1.45148674353\n"
     ]
    }
   ],
   "source": [
    "for a in predictions_real[0:20]:\n",
    "    print(sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.31632537756\n",
      "-1.54030133417\n",
      "-1.39380135257\n",
      "-1.33690173249\n",
      "-1.41042510903\n"
     ]
    }
   ],
   "source": [
    "for a in predictions_artificial:\n",
    "    print(sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_artificial = np.asarray([sum(a) for a in predictions_artificial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.31632538, -1.54030133, -1.39380135, -1.33690173, -1.41042511])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictions_artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"predictions_real.csv\", predictions_real, delimiter=\",\")\n",
    "np.savetxt(\"predictions_artificial.csv\", new_predictions_artificial, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"gsutil cp predictions_artificial.csv gs://w266_final_project_kk/predictions_artificial/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
