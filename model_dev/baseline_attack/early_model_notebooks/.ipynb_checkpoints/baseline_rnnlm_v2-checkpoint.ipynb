{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Character-Level Language Model\n",
    "\n",
    "This is the \"working notebook\", with code to load and train the model, as well as run unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Dependencies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "#import json, os, re, shutil, sys, time\n",
    "import os, shutil, time\n",
    "from importlib import reload\n",
    "#import collections, itertools\n",
    "import unittest\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "#import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils#, vocabulary, tf_embed_viz\n",
    "\n",
    "# rnnlm code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)\n",
    "\n",
    "# packages for extracting data\n",
    "import pandas as pd\n",
    "#from glob import glob\n",
    "#import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "#wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "#embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "#x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "#print('wordid placeholder shape:', wordid_ph.shape)\n",
    "#print('x shape:', x.shape)\n",
    "\n",
    "#sess = tf.Session()\n",
    "## Two sentences, each with four words.\n",
    "#wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "#x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "#print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "#print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "Load implementation, construct graph, and write a logdir for TensorBoard.\n",
    "```\n",
    "cd assignment/a4\n",
    "tensorboard --logdir /tmp/w266/a4_graph --port 6006\n",
    "```\n",
    "http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_tensorboard(tf_graphdir=\"/tmp/artificial_hotel_reviews/a4_graph\", V=100, H=1024, num_layers=2):\n",
    "    reload(rnnlm)\n",
    "    TF_GRAPHDIR = tf_graphdir\n",
    "    # Clear old log directory.\n",
    "    shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "    \n",
    "    lm = rnnlm.RNNLM(V=V, H=H, num_layers=num_layers)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "    return summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  W_in:0\n",
      "Shape:  (10000, 200)\n",
      "[[-0.78748155 -0.05848813 -0.64172769 ...,  0.84141541 -0.94063973\n",
      "   0.6318059 ]\n",
      " [ 0.73271012  0.9191277   0.16166019 ..., -0.77998519  0.349545\n",
      "  -0.45933056]\n",
      " [ 0.56432152 -0.03329587  0.20995927 ..., -0.3828721   0.12824035\n",
      "  -0.13679957]\n",
      " ..., \n",
      " [-0.12901855 -0.29448342 -0.08786106 ..., -0.73254704 -0.37692833\n",
      "  -0.20111871]\n",
      " [-0.62848496 -0.50970197  0.9452734  ...,  0.99917698 -0.81992722\n",
      "  -0.37649083]\n",
      " [-0.09145284  0.00680947  0.67164588 ..., -0.92668557  0.46153927\n",
      "   0.74277234]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.02885439 -0.00585808 -0.04069098 ...,  0.00699186 -0.05404399\n",
      "   0.02202466]\n",
      " [ 0.06509807 -0.02223223  0.02077565 ..., -0.00037639  0.04783005\n",
      "  -0.00804034]\n",
      " [ 0.02742642  0.05705827 -0.03898749 ...,  0.00316557  0.06507974\n",
      "  -0.01836298]\n",
      " ..., \n",
      " [-0.00754741 -0.04928695  0.06990113 ..., -0.06414328 -0.06926721\n",
      "  -0.05963309]\n",
      " [-0.06505661 -0.0187963   0.05553678 ..., -0.00105456 -0.04487306\n",
      "   0.05926735]\n",
      " [ 0.05089028  0.03582108 -0.05539448 ...,  0.00139427  0.06008063\n",
      "   0.05134535]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.06411717 -0.059903    0.05128169 ...,  0.03041378  0.05353859\n",
      "   0.0544473 ]\n",
      " [ 0.06030189  0.02777085  0.0119326  ..., -0.00028028 -0.04805488\n",
      "  -0.03846691]\n",
      " [-0.02244554  0.04396319  0.06136966 ...,  0.02755227  0.01427022\n",
      "   0.0558702 ]\n",
      " ..., \n",
      " [-0.0622274  -0.03975505 -0.03073074 ...,  0.0241474  -0.04965901\n",
      "  -0.00885601]\n",
      " [ 0.0227494  -0.0019692  -0.05720998 ..., -0.03900233 -0.02878788\n",
      "  -0.05673516]\n",
      " [-0.04740062  0.00044262  0.0118935  ...,  0.02134912  0.04072783\n",
      "   0.00831893]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  W_out:0\n",
      "Shape:  (200, 10000)\n",
      "[[ 0.22999096 -0.98074627  0.2485311  ...,  0.31636119  0.64180779\n",
      "   0.48168755]\n",
      " [ 0.02860689  0.14025211  0.73703051 ...,  0.55039978 -0.11227965\n",
      "   0.06153202]\n",
      " [-0.74636269 -0.50103402  0.13248014 ..., -0.45557976  0.15847898\n",
      "   0.09724474]\n",
      " ..., \n",
      " [-0.21404767  0.9171977  -0.29322004 ..., -0.82013679  0.63434672\n",
      "  -0.56355882]\n",
      " [ 0.83722639 -0.26542854 -0.89726591 ..., -0.59170508  0.18550444\n",
      "   0.48118067]\n",
      " [ 0.55475283 -0.74218535 -0.24593616 ..., -0.88373137 -0.86611366\n",
      "  -0.00220013]]\n",
      "Variable:  b_out:0\n",
      "Shape:  (10000,)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#with lm.graph.as_default():\n",
    "    #initializer = tf.global_variables_initializer()\n",
    "\n",
    "#with tf.Session(graph=lm.graph) as session:\n",
    "    #session.run(initializer)\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unit Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Tests\n",
    "def test_graph():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])\n",
    "\n",
    "def test_training():\n",
    "    reload(rnnlm); reload(rnnlm_test)\n",
    "    th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "    th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "    unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 3.315s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "test_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNNLM\n",
    "\n",
    "Data loader functions in **`utils.py`**.\n",
    "\n",
    "`utils.rnnlm_batch_generator` returns an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "Toy corpus example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_epoch()\n",
    "\n",
    "`train=True` flag enables train mode. `train=False` runs `score_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Functions\n",
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Unit Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 0]: seen 50 words at 43.1 wps, loss = 2.028\n",
      "[batch 98]: seen 4950 words at 2284.2 wps, loss = 1.114\n",
      "[batch 210]: seen 10550 words at 3329.2 wps, loss = 0.959\n",
      "[batch 346]: seen 17350 words at 4155.3 wps, loss = 1.024\n",
      "[batch 481]: seen 24100 words at 4656.4 wps, loss = 0.983\n",
      "[batch 615]: seen 30800 words at 4987.1 wps, loss = 0.943\n",
      "[batch 751]: seen 37600 words at 5237.5 wps, loss = 0.954\n",
      "[batch 887]: seen 44400 words at 5425.6 wps, loss = 0.964\n",
      "[batch 1024]: seen 51250 words at 5579.2 wps, loss = 0.954\n",
      "[batch 1160]: seen 58050 words at 5697.4 wps, loss = 0.943\n",
      "[batch 1296]: seen 64850 words at 5794.1 wps, loss = 0.930\n",
      "[batch 1431]: seen 71600 words at 5872.2 wps, loss = 0.921\n",
      "[batch 1567]: seen 78400 words at 5940.1 wps, loss = 0.917\n",
      "[batch 1702]: seen 85150 words at 5995.7 wps, loss = 0.905\n",
      "[batch 1839]: seen 92000 words at 6050.8 wps, loss = 0.909\n",
      "[batch 1976]: seen 98850 words at 6098.0 wps, loss = 0.914\n",
      "[batch 2112]: seen 105650 words at 6138.3 wps, loss = 0.906\n",
      "[batch 2245]: seen 112300 words at 6164.1 wps, loss = 0.893\n",
      "[batch 2379]: seen 119000 words at 6191.6 wps, loss = 0.883\n",
      "[batch 2516]: seen 125850 words at 6222.1 wps, loss = 0.881\n",
      "[batch 2650]: seen 132550 words at 6243.5 wps, loss = 0.886\n",
      "[batch 2786]: seen 139350 words at 6268.4 wps, loss = 0.880\n",
      "[batch 2919]: seen 146000 words at 6283.9 wps, loss = 0.875\n",
      "[batch 3055]: seen 152800 words at 6303.9 wps, loss = 0.870\n",
      "[batch 3189]: seen 159500 words at 6319.6 wps, loss = 0.874\n",
      "[batch 3323]: seen 166200 words at 6334.0 wps, loss = 0.878\n",
      "[batch 3456]: seen 172850 words at 6344.5 wps, loss = 0.889\n",
      "[batch 3587]: seen 179400 words at 6350.9 wps, loss = 0.886\n",
      "[batch 3717]: seen 185900 words at 6355.3 wps, loss = 0.880\n",
      "[batch 3844]: seen 192250 words at 6353.9 wps, loss = 0.873\n",
      "[batch 3970]: seen 198550 words at 6351.4 wps, loss = 0.870\n",
      "Train set: avg. loss: 0.018  (perplexity: 1.02)\n",
      "Test set: avg. loss: 0.038  (perplexity: 1.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 33.626s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reload(rnnlm); reload(rnnlm_test)\n",
    "#th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "#th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "#unittest.TextTestRunner(verbosity=2).run(th)\n",
    "test_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "At the end of training, `tf.train.Saver` saves a copy of the model to `/tmp/w266/artificial_hotel_reviews/rnnlm_trained`. To load this from disk, see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        if len(semifinal_review) > 300:\n",
    "            final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "            #print(final_review)\n",
    "            review_list.append(final_review)\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_series(review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    review_df = pd.read_csv(review_path)\n",
    "    five_star_review_df = review_df[review_df['stars']==5]\n",
    "    #five_star_review_series = five_star_review_df['text']\n",
    "    return five_star_review_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_list(business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'):\n",
    "    #business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'\n",
    "    return pd.read_csv(business_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_data(five_star_review_series, training_samples=20000, test_samples=1000):\n",
    "    #add randomization\n",
    "    review_list = preprocess_review_series(five_star_review_series)\n",
    "    training_review_list = [item for sublist in review_list[:training_samples] for item in sublist]\n",
    "    print(len(training_review_list))\n",
    "    \n",
    "    test_review_list = [item for sublist in review_list[training_samples:training_samples+test_samples] for item in sublist]\n",
    "    return training_review_list, test_review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary(training_review_list, test_review_list):\n",
    "    unique_characters = list(set(training_review_list + test_review_list))\n",
    "    #vocabulary\n",
    "    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "    return char_dict, ids_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ids(char_dict, review_list):\n",
    "    #convert to flat (1D) np.array(int) of ids\n",
    "    review_ids = [char_dict.get(token) for token in review_list]\n",
    "    return np.array(review_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20):\n",
    "    #V = len(words_to_ids.keys())\n",
    "    # Training parameters\n",
    "    ## add parameter sets for each attack/defense configuration\n",
    "    #max_time = 25\n",
    "    #batch_size = 100\n",
    "    #learning_rate = 0.01\n",
    "    #num_epochs = 10\n",
    "    \n",
    "    # Model parameters\n",
    "    #model_params = dict(V=vocab.size, \n",
    "                        #H=200, \n",
    "                        #softmax_ns=200,\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=len(words_to_ids.keys()), \n",
    "                        #H=1024, \n",
    "                        #softmax_ns=len(words_to_ids.keys()),\n",
    "                        #num_layers=2)\n",
    "    #model_params = dict(V=V, H=H, softmax_ns=softmax_ns, num_layers=num_layers)\n",
    "    \n",
    "    #TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "    TF_SAVEDIR = tf_savedir\n",
    "    checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "    trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")\n",
    "    \n",
    "    # Will print status every this many seconds\n",
    "    #print_interval = 5\n",
    "    print_interval = 30\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "    \n",
    "    # Explicitly add global initializer and variable saver to LM graph\n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Clear old log directory\n",
    "    shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "    if not os.path.isdir(TF_SAVEDIR):\n",
    "        os.makedirs(TF_SAVEDIR)\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        tf.set_random_seed(42)\n",
    "    \n",
    "        session.run(initializer)\n",
    "        \n",
    "        #check trainable variables\n",
    "        #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "        #values = session.run(variables_names)\n",
    "        #for k, v in zip(variables_names, values):\n",
    "            #print(\"Variable: \", k)\n",
    "            #print(\"Shape: \", v.shape)\n",
    "            #print(v)\n",
    "    \n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "            print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "            # Run a training epoch.\n",
    "            run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "    \n",
    "            print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "        \n",
    "            # Save a checkpoint\n",
    "            saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "        \n",
    "            ##\n",
    "            # score_dataset will run a forward pass over the entire dataset\n",
    "            # and report perplexity scores. This can be slow (around 1/2 to \n",
    "            # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "            # to speed up training on a slow machine. Be sure to run it at the \n",
    "            # end to evaluate your score.\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "        return trained_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sampling\n",
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "\n",
    "    # Run sample ops\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(trained_filename, model_params, words_to_ids, ids_to_words):\n",
    "    # Same as above, but as a batch\n",
    "    #max_steps = 20\n",
    "    max_steps = 50\n",
    "    num_samples = 10\n",
    "    random_seed = 42\n",
    "    \n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        # Seed RNG for repeatability\n",
    "        tf.set_random_seed(random_seed)\n",
    "        \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "    \n",
    "        # Make initial state for a batch with batch_size = num_samples\n",
    "        #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "        w = np.repeat([[words_to_ids.get('<SOR>')]], num_samples, axis=0)\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        # take one step for each sequence on each iteration \n",
    "        for i in range(max_steps):\n",
    "            h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "            w = np.hstack((w,y))\n",
    "    \n",
    "        # Print generated sentences\n",
    "        for row in w:\n",
    "            for i, word_id in enumerate(row):\n",
    "                #print(vocab.id_to_word[word_id], end=\" \")\n",
    "                print(ids_to_words[word_id], end=\"\")\n",
    "                #if (i != 0) and (word_id == vocab.START_ID):\n",
    "                if (i != 0) and (word_id == words_to_ids.get(\"<EOR>\")):\n",
    "                    break\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attack_model(training_samples=20000, test_samples=1000, review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #training_samples=20000\n",
    "    #test_samples=1000\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    five_star_reviews = get_review_series(review_path)\n",
    "    train_review_list, test_review_list = make_train_test_data(five_star_reviews, training_samples, test_samples)\n",
    "    words_to_ids, ids_to_words = make_vocabulary(train_review_list, test_review_list)\n",
    "    train_ids = convert_to_ids(words_to_ids, train_review_list)\n",
    "    test_ids = convert_to_ids(words_to_ids, test_review_list)\n",
    "    model_params = dict(V=len(words_to_ids.keys()), \n",
    "                            H=1024, \n",
    "                            softmax_ns=len(words_to_ids.keys()),\n",
    "                            num_layers=2)\n",
    "    #run_training(train_ids, test_ids, tf_savedir, model_params, max_time=100, batch_size=256, learning_rate=0.002, num_epochs=20)\n",
    "    trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=300, batch_size=256, learning_rate=0.002, num_epochs=1)\n",
    "    return trained_filename, model_params, words_to_ids, ids_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14439556\n",
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 0]: seen 76800 words at 3292.8 wps, loss = 4.817\n",
      "[batch 1]: seen 153600 words at 3651.2 wps, loss = 8.475\n",
      "[batch 2]: seen 230400 words at 3820.9 wps, loss = 11.046\n",
      "[batch 3]: seen 307200 words at 3917.7 wps, loss = 10.672\n",
      "[batch 4]: seen 384000 words at 3978.7 wps, loss = 10.606\n",
      "[batch 5]: seen 460800 words at 4018.5 wps, loss = 9.875\n",
      "[batch 6]: seen 537600 words at 4055.1 wps, loss = 9.130\n",
      "[batch 7]: seen 614400 words at 4075.4 wps, loss = 8.499\n",
      "[batch 8]: seen 691200 words at 4090.2 wps, loss = 7.966\n",
      "[batch 9]: seen 768000 words at 4112.7 wps, loss = 7.505\n",
      "[batch 10]: seen 844800 words at 4128.7 wps, loss = 7.106\n",
      "[batch 11]: seen 921600 words at 4143.4 wps, loss = 6.762\n",
      "[batch 12]: seen 998400 words at 4151.0 wps, loss = 6.457\n",
      "[batch 13]: seen 1075200 words at 4162.2 wps, loss = 6.191\n",
      "[batch 14]: seen 1152000 words at 4169.3 wps, loss = 5.957\n",
      "[batch 15]: seen 1228800 words at 4177.5 wps, loss = 5.749\n",
      "[batch 16]: seen 1305600 words at 4185.4 wps, loss = 5.565\n",
      "[batch 17]: seen 1382400 words at 4192.7 wps, loss = 5.398\n",
      "[batch 18]: seen 1459200 words at 4199.3 wps, loss = 5.247\n",
      "[batch 19]: seen 1536000 words at 4206.1 wps, loss = 5.110\n",
      "[batch 20]: seen 1612800 words at 4210.0 wps, loss = 4.985\n",
      "[batch 21]: seen 1689600 words at 4213.9 wps, loss = 4.870\n",
      "[batch 22]: seen 1766400 words at 4216.8 wps, loss = 4.765\n",
      "[batch 23]: seen 1843200 words at 4221.3 wps, loss = 4.667\n",
      "[batch 24]: seen 1920000 words at 4224.8 wps, loss = 4.577\n",
      "[batch 25]: seen 1996800 words at 4227.7 wps, loss = 4.493\n",
      "[batch 26]: seen 2073600 words at 4231.9 wps, loss = 4.415\n",
      "[batch 27]: seen 2150400 words at 4233.0 wps, loss = 4.341\n",
      "[batch 28]: seen 2227200 words at 4233.7 wps, loss = 4.273\n",
      "[batch 29]: seen 2304000 words at 4236.1 wps, loss = 4.208\n",
      "[batch 30]: seen 2380800 words at 4239.0 wps, loss = 4.147\n",
      "[batch 31]: seen 2457600 words at 4241.3 wps, loss = 4.090\n",
      "[batch 32]: seen 2534400 words at 4243.9 wps, loss = 4.036\n",
      "[batch 33]: seen 2611200 words at 4246.3 wps, loss = 3.985\n",
      "[batch 34]: seen 2688000 words at 4247.5 wps, loss = 3.936\n",
      "[batch 35]: seen 2764800 words at 4247.5 wps, loss = 3.890\n",
      "[batch 36]: seen 2841600 words at 4250.1 wps, loss = 3.846\n",
      "[batch 37]: seen 2918400 words at 4252.4 wps, loss = 3.804\n",
      "[batch 38]: seen 2995200 words at 4255.0 wps, loss = 3.763\n",
      "[batch 39]: seen 3072000 words at 4256.3 wps, loss = 3.725\n",
      "[batch 40]: seen 3148800 words at 4259.1 wps, loss = 3.688\n",
      "[batch 41]: seen 3225600 words at 4258.7 wps, loss = 3.653\n",
      "[batch 42]: seen 3302400 words at 4259.5 wps, loss = 3.619\n",
      "[batch 43]: seen 3379200 words at 4261.7 wps, loss = 3.587\n",
      "[batch 44]: seen 3456000 words at 4263.1 wps, loss = 3.556\n",
      "[batch 45]: seen 3532800 words at 4264.6 wps, loss = 3.526\n",
      "[batch 46]: seen 3609600 words at 4266.5 wps, loss = 3.497\n",
      "[batch 47]: seen 3686400 words at 4266.9 wps, loss = 3.470\n",
      "[batch 48]: seen 3763200 words at 4267.2 wps, loss = 3.443\n",
      "[batch 49]: seen 3840000 words at 4268.1 wps, loss = 3.417\n",
      "[batch 50]: seen 3916800 words at 4269.4 wps, loss = 3.392\n",
      "[batch 51]: seen 3993600 words at 4270.7 wps, loss = 3.368\n",
      "[batch 52]: seen 4070400 words at 4271.3 wps, loss = 3.344\n",
      "[batch 53]: seen 4147200 words at 4272.5 wps, loss = 3.321\n",
      "[batch 54]: seen 4224000 words at 4273.1 wps, loss = 3.299\n",
      "[batch 55]: seen 4300800 words at 4273.0 wps, loss = 3.278\n",
      "[batch 56]: seen 4377600 words at 4274.4 wps, loss = 3.257\n",
      "[batch 57]: seen 4454400 words at 4274.9 wps, loss = 3.237\n",
      "[batch 58]: seen 4531200 words at 4276.5 wps, loss = 3.218\n",
      "[batch 59]: seen 4608000 words at 4277.3 wps, loss = 3.199\n",
      "[batch 60]: seen 4684800 words at 4277.9 wps, loss = 3.180\n",
      "[batch 61]: seen 4761600 words at 4277.5 wps, loss = 3.162\n",
      "[batch 62]: seen 4838400 words at 4277.4 wps, loss = 3.144\n",
      "[batch 63]: seen 4915200 words at 4277.7 wps, loss = 3.127\n",
      "[batch 64]: seen 4992000 words at 4278.8 wps, loss = 3.111\n",
      "[batch 65]: seen 5068800 words at 4279.7 wps, loss = 3.094\n",
      "[batch 66]: seen 5145600 words at 4279.7 wps, loss = 3.079\n",
      "[batch 67]: seen 5222400 words at 4280.0 wps, loss = 3.064\n",
      "[batch 68]: seen 5299200 words at 4280.0 wps, loss = 3.049\n",
      "[batch 69]: seen 5376000 words at 4280.1 wps, loss = 3.034\n",
      "[batch 70]: seen 5452800 words at 4281.1 wps, loss = 3.020\n",
      "[batch 71]: seen 5529600 words at 4282.0 wps, loss = 3.006\n",
      "[batch 72]: seen 5606400 words at 4282.1 wps, loss = 2.992\n",
      "[batch 73]: seen 5683200 words at 4282.0 wps, loss = 2.978\n",
      "[batch 74]: seen 5760000 words at 4282.1 wps, loss = 2.965\n",
      "[batch 75]: seen 5836800 words at 4281.7 wps, loss = 2.952\n",
      "[batch 76]: seen 5913600 words at 4282.1 wps, loss = 2.940\n",
      "[batch 77]: seen 5990400 words at 4282.7 wps, loss = 2.928\n",
      "[batch 78]: seen 6067200 words at 4283.3 wps, loss = 2.916\n",
      "[batch 79]: seen 6144000 words at 4283.0 wps, loss = 2.904\n",
      "[batch 80]: seen 6220800 words at 4283.1 wps, loss = 2.892\n",
      "[batch 81]: seen 6297600 words at 4282.9 wps, loss = 2.881\n",
      "[batch 82]: seen 6374400 words at 4282.8 wps, loss = 2.870\n",
      "[batch 83]: seen 6451200 words at 4283.6 wps, loss = 2.859\n",
      "[batch 84]: seen 6528000 words at 4284.2 wps, loss = 2.848\n",
      "[batch 85]: seen 6604800 words at 4285.3 wps, loss = 2.837\n",
      "[batch 86]: seen 6681600 words at 4285.6 wps, loss = 2.827\n",
      "[batch 87]: seen 6758400 words at 4286.0 wps, loss = 2.817\n",
      "[batch 88]: seen 6835200 words at 4286.1 wps, loss = 2.807\n",
      "[batch 89]: seen 6912000 words at 4286.3 wps, loss = 2.797\n",
      "[batch 90]: seen 6988800 words at 4286.8 wps, loss = 2.787\n",
      "[batch 91]: seen 7065600 words at 4287.5 wps, loss = 2.777\n",
      "[batch 92]: seen 7142400 words at 4288.1 wps, loss = 2.768\n",
      "[batch 93]: seen 7219200 words at 4288.3 wps, loss = 2.759\n",
      "[batch 94]: seen 7296000 words at 4288.5 wps, loss = 2.750\n",
      "[batch 95]: seen 7372800 words at 4288.1 wps, loss = 2.741\n",
      "[batch 96]: seen 7449600 words at 4288.0 wps, loss = 2.732\n",
      "[batch 97]: seen 7526400 words at 4288.5 wps, loss = 2.724\n",
      "[batch 98]: seen 7603200 words at 4289.3 wps, loss = 2.716\n",
      "[batch 99]: seen 7680000 words at 4289.3 wps, loss = 2.707\n",
      "[batch 100]: seen 7756800 words at 4290.2 wps, loss = 2.699\n",
      "[batch 101]: seen 7833600 words at 4290.1 wps, loss = 2.692\n",
      "[batch 102]: seen 7910400 words at 4290.0 wps, loss = 2.684\n",
      "[batch 103]: seen 7987200 words at 4290.1 wps, loss = 2.676\n",
      "[batch 104]: seen 8064000 words at 4290.5 wps, loss = 2.669\n",
      "[batch 105]: seen 8140800 words at 4290.7 wps, loss = 2.661\n",
      "[batch 106]: seen 8217600 words at 4290.8 wps, loss = 2.653\n",
      "[batch 107]: seen 8294400 words at 4291.2 wps, loss = 2.646\n",
      "[batch 108]: seen 8371200 words at 4290.9 wps, loss = 2.639\n",
      "[batch 109]: seen 8448000 words at 4290.7 wps, loss = 2.632\n",
      "[batch 110]: seen 8524800 words at 4291.2 wps, loss = 2.624\n",
      "[batch 111]: seen 8601600 words at 4291.5 wps, loss = 2.617\n",
      "[batch 112]: seen 8678400 words at 4292.0 wps, loss = 2.610\n",
      "[batch 113]: seen 8755200 words at 4292.2 wps, loss = 2.604\n",
      "[batch 114]: seen 8832000 words at 4292.7 wps, loss = 2.597\n",
      "[batch 115]: seen 8908800 words at 4292.4 wps, loss = 2.591\n",
      "[batch 116]: seen 8985600 words at 4292.2 wps, loss = 2.585\n",
      "[batch 117]: seen 9062400 words at 4292.9 wps, loss = 2.579\n",
      "[batch 118]: seen 9139200 words at 4293.3 wps, loss = 2.572\n",
      "[batch 119]: seen 9216000 words at 4293.6 wps, loss = 2.566\n",
      "[batch 120]: seen 9292800 words at 4293.9 wps, loss = 2.560\n",
      "[batch 121]: seen 9369600 words at 4294.2 wps, loss = 2.554\n",
      "[batch 122]: seen 9446400 words at 4294.3 wps, loss = 2.548\n",
      "[batch 123]: seen 9523200 words at 4294.6 wps, loss = 2.542\n",
      "[batch 124]: seen 9600000 words at 4295.0 wps, loss = 2.537\n",
      "[batch 125]: seen 9676800 words at 4295.4 wps, loss = 2.531\n",
      "[batch 126]: seen 9753600 words at 4295.7 wps, loss = 2.526\n",
      "[batch 127]: seen 9830400 words at 4295.9 wps, loss = 2.520\n",
      "[batch 128]: seen 9907200 words at 4295.5 wps, loss = 2.514\n",
      "[batch 129]: seen 9984000 words at 4295.3 wps, loss = 2.509\n",
      "[batch 130]: seen 10060800 words at 4295.5 wps, loss = 2.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 131]: seen 10137600 words at 4296.0 wps, loss = 2.497\n",
      "[batch 132]: seen 10214400 words at 4296.2 wps, loss = 2.492\n",
      "[batch 133]: seen 10291200 words at 4295.9 wps, loss = 2.487\n",
      "[batch 134]: seen 10368000 words at 4296.2 wps, loss = 2.481\n",
      "[batch 135]: seen 10444800 words at 4296.0 wps, loss = 2.476\n",
      "[batch 136]: seen 10521600 words at 4295.5 wps, loss = 2.471\n",
      "[batch 137]: seen 10598400 words at 4295.6 wps, loss = 2.466\n",
      "[batch 138]: seen 10675200 words at 4295.9 wps, loss = 2.460\n",
      "[batch 139]: seen 10752000 words at 4296.1 wps, loss = 2.455\n",
      "[batch 140]: seen 10828800 words at 4296.4 wps, loss = 2.450\n",
      "[batch 141]: seen 10905600 words at 4296.3 wps, loss = 2.445\n",
      "[batch 142]: seen 10982400 words at 4295.9 wps, loss = 2.440\n",
      "[batch 143]: seen 11059200 words at 4296.1 wps, loss = 2.436\n",
      "[batch 144]: seen 11136000 words at 4296.4 wps, loss = 2.431\n",
      "[batch 145]: seen 11212800 words at 4296.7 wps, loss = 2.426\n",
      "[batch 146]: seen 11289600 words at 4296.9 wps, loss = 2.422\n",
      "[batch 147]: seen 11366400 words at 4297.3 wps, loss = 2.417\n",
      "[batch 148]: seen 11443200 words at 4297.3 wps, loss = 2.412\n",
      "[batch 149]: seen 11520000 words at 4297.2 wps, loss = 2.408\n",
      "[batch 150]: seen 11596800 words at 4297.3 wps, loss = 2.404\n",
      "[batch 151]: seen 11673600 words at 4297.7 wps, loss = 2.399\n",
      "[batch 152]: seen 11750400 words at 4297.9 wps, loss = 2.395\n",
      "[batch 153]: seen 11827200 words at 4298.0 wps, loss = 2.390\n",
      "[batch 154]: seen 11904000 words at 4298.2 wps, loss = 2.386\n",
      "[batch 155]: seen 11980800 words at 4298.1 wps, loss = 2.382\n",
      "[batch 156]: seen 12057600 words at 4297.9 wps, loss = 2.378\n",
      "[batch 157]: seen 12134400 words at 4298.3 wps, loss = 2.373\n",
      "[batch 158]: seen 12211200 words at 4298.6 wps, loss = 2.369\n",
      "[batch 159]: seen 12288000 words at 4298.8 wps, loss = 2.365\n",
      "[batch 160]: seen 12364800 words at 4299.0 wps, loss = 2.361\n",
      "[batch 161]: seen 12441600 words at 4299.2 wps, loss = 2.357\n",
      "[batch 162]: seen 12518400 words at 4298.8 wps, loss = 2.353\n",
      "[batch 163]: seen 12595200 words at 4298.3 wps, loss = 2.349\n",
      "[batch 164]: seen 12672000 words at 4298.8 wps, loss = 2.346\n",
      "[batch 165]: seen 12748800 words at 4299.0 wps, loss = 2.342\n",
      "[batch 166]: seen 12825600 words at 4299.2 wps, loss = 2.338\n",
      "[batch 167]: seen 12902400 words at 4299.2 wps, loss = 2.334\n",
      "[batch 168]: seen 12979200 words at 4299.3 wps, loss = 2.330\n",
      "[batch 169]: seen 13056000 words at 4299.2 wps, loss = 2.327\n",
      "[batch 170]: seen 13132800 words at 4299.2 wps, loss = 2.323\n",
      "[batch 171]: seen 13209600 words at 4299.3 wps, loss = 2.319\n",
      "[batch 172]: seen 13286400 words at 4299.3 wps, loss = 2.316\n",
      "[batch 173]: seen 13363200 words at 4299.4 wps, loss = 2.312\n",
      "[batch 174]: seen 13440000 words at 4299.5 wps, loss = 2.308\n",
      "[batch 175]: seen 13516800 words at 4299.5 wps, loss = 2.305\n",
      "[batch 176]: seen 13593600 words at 4299.3 wps, loss = 2.301\n",
      "[batch 177]: seen 13670400 words at 4299.5 wps, loss = 2.298\n",
      "[batch 178]: seen 13747200 words at 4299.8 wps, loss = 2.294\n",
      "[batch 179]: seen 13824000 words at 4300.0 wps, loss = 2.291\n",
      "[batch 180]: seen 13900800 words at 4300.0 wps, loss = 2.287\n",
      "[batch 181]: seen 13977600 words at 4299.9 wps, loss = 2.284\n",
      "[batch 182]: seen 14054400 words at 4299.9 wps, loss = 2.280\n",
      "[batch 183]: seen 14131200 words at 4299.8 wps, loss = 2.277\n",
      "[batch 184]: seen 14208000 words at 4300.0 wps, loss = 2.274\n",
      "[batch 185]: seen 14284800 words at 4300.1 wps, loss = 2.270\n",
      "[batch 186]: seen 14361600 words at 4300.3 wps, loss = 2.267\n",
      "[batch 187]: seen 14438400 words at 4300.4 wps, loss = 2.264\n",
      "[epoch 1] Completed in 0:55:57\n",
      "[epoch 1] Test set: avg. loss: 1.513  (perplexity: 4.54)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_filename, model_params, words_to_ids, ids_to_words = train_attack_model(training_samples=20000, \n",
    "                                                                                test_samples=5000, \n",
    "                                                                                review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'words_to_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a7a50b33a095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-a5295ca6c8ef>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(trained_filename, model_params)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Make initial state for a batch with batch_size = num_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_to_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<SOR>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_h_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_w_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# take one step for each sequence on each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words_to_ids' is not defined"
     ]
    }
   ],
   "source": [
    "generate_text(trained_filename, model_params, words_to_ids, ids_to_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linguistic Properties\n",
    "\n",
    "Given two or more test sentences, the model should score the more plausible (or more correct) sentence with a higher log-probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"once upon a time\" : -8.74\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.83\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"once upon a time\",\n",
    "         #\"the quick brown fox jumps over the lazy dog\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = [\"is sentence nonsense this\", \"i drive a car\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -6.03\n",
      "\"the boy and the girl is\" : -5.90\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"the boy and the girl are\", \"the boy and the girl is\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -7.46\n",
      "\"peanuts are my favorite kind of vegetable\" : -7.29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"when I'm hungry I really prefer to eat\" : -7.49\n",
      "\"when I'm hungry I really prefer to drink\" : -7.52\n"
     ]
    }
   ],
   "source": [
    "#sents = [\"peanuts are my favorite kind of nut\",\n",
    "         #\"peanuts are my favorite kind of vegetable\"]\n",
    "#load_and_score([s.split() for s in sents])\n",
    "\n",
    "#sents = [\"when I'm hungry I really prefer to eat\",\n",
    "         #\"when I'm hungry I really prefer to drink\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"I have lots of plastic green square toys\" : -8.51\n",
      "\"I have lots of green square plastic toys\" : -8.56\n",
      "\"I have lots of green plastic square toys\" : -8.57\n",
      "\"I have lots of plastic square green toys\" : -8.64\n",
      "\"I have lots of square green plastic toys\" : -8.69\n",
      "\"I have lots of square plastic green toys\" : -8.71\n"
     ]
    }
   ],
   "source": [
    "#prefix = \"I have lots of\".split()\n",
    "#noun = \"toys\"\n",
    "#adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "#inputs = []\n",
    "#for adjs in itertools.permutations(adjectives):\n",
    "    #words = prefix + list(adjs) + [noun]\n",
    "    #inputs.append(words)\n",
    "    \n",
    "#load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
