{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Recurrent Neural Network Language Model\n",
    "\n",
    "This is the \"working notebook\", with skeleton code to load and train your model, as well as run unit tests. See [rnnlm-instructions.ipynb](rnnlm-instructions.ipynb) for the main writeup.\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters  \n",
    "\n",
    "### Questions for Part (a)\n",
    "You should use big-O notation when appropriate (i.e. computing $\\exp(\\mathbf{v})$ for a vector $\\mathbf{v}$ of length $n$ is $O(n)$ operations).  Assume for problems a(1-5) that:   \n",
    "> -- Cell is one layer,  \n",
    "> -- the embedding feature length and hidden-layer feature lengths are both H, and   \n",
    "> -- ignore at the moment batch and max_time dimensions.  \n",
    "\n",
    "1. Let $\\text{CellFunc}$ be a simple RNN cell (see async Section 5.8). Write the cell equation in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
    "<p>\n",
    "2. How many parameters are in the embedding layer? In the output layer? (By parameters, we mean total number of matrix elements across all train-able tensors. A $m \\times n$ matrix has $mn$ elements.)\n",
    "<p>\n",
    "3. How many calculations (floating point operations) are required to compute $\\hat{P}(w^{(i+1)})$ for a given *single* target word $w^{(i+1)}$, assuming $w^{(i)}$ given and $h^{(i-1)}$ already computed? How about for *all* target words?\n",
    "<p>\n",
    "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax? (*Recall that hierarchical softmax makes a series of left/right decisions using a binary classifier $P_s(\\text{right}) = \\sigma(u_s \\cdot o^{(i)} + b_s)$ at each split $s$ in the tree.*)\n",
    "<p>\n",
    "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training? (*Choose \"embedding layer\", \"recurrent layer\", or \"output layer\"*.)\n",
    "\n",
    "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers for Part (a)\n",
    "\n",
    "####  [Use the Google forms to answer a1) - a5)!]( https://docs.google.com/forms/d/e/1FAIpQLSc7kpuOzErVE_H0vMfsDvFcBHz9dkwXGSzqHUGa70QsTIC1ow/viewform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM\n",
    "\n",
    "### Aside: Shapes Review\n",
    "\n",
    "Before we start, let's review our understanding of the shapes involved in this assignment and how they manifest themselves in the TF API.\n",
    "\n",
    "As in the [instructions](rnnlm-instructions.ipynb) notebook, $w$ is a matrix of wordids with shape batch_size x max_time.  Passing this through the embedding layer, we retrieve the word embedding for each, resulting in $x$ having shape batch_size x max_time x embedding_dim.  I find it useful to draw this out on a piece of paper.  When you do, you should end up with a rectangular prism with batch_size height, max_time width and embedding_dim depth.  Many tensors in this assignment share this shape (e.g. $o$, the output from the LSTM, which represents the hidden layer going into the softmax to make a prediction at every time step in every batch).\n",
    "\n",
    "Since batch size and sentence length are only resolved when we run the graph, we construct the placeholder using \"None\" in the dimensions we don't know.  The .shape property renders these as ?s.  This should be familiar to you from batch size handling in earlier assignments, only now there are two dimensions of variable length.\n",
    "\n",
    "See the next cell for a concrete example (though in practice, we'd use a TensorFlow variable that we can train for the embeddings rather than a static array).  Notice how the shape of x_val matches the shape described earlier in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "print('wordid placeholder shape:', wordid_ph.shape)\n",
    "print('x shape:', x.shape)\n",
    "\n",
    "sess = tf.Session()\n",
    "# Two sentences, each with four words.\n",
    "wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implmenting the RNNLM\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/artificial_hotel_reviews/a4_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "cd assignment/a4\n",
    "tensorboard --logdir /tmp/w266/a4_graph --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/ and visit the \"Graphs\" tab to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  W_in:0\n",
      "Shape:  (10000, 200)\n",
      "[[-0.78748155 -0.05848813 -0.64172769 ...,  0.84141541 -0.94063973\n",
      "   0.6318059 ]\n",
      " [ 0.73271012  0.9191277   0.16166019 ..., -0.77998519  0.349545\n",
      "  -0.45933056]\n",
      " [ 0.56432152 -0.03329587  0.20995927 ..., -0.3828721   0.12824035\n",
      "  -0.13679957]\n",
      " ..., \n",
      " [-0.12901855 -0.29448342 -0.08786106 ..., -0.73254704 -0.37692833\n",
      "  -0.20111871]\n",
      " [-0.62848496 -0.50970197  0.9452734  ...,  0.99917698 -0.81992722\n",
      "  -0.37649083]\n",
      " [-0.09145284  0.00680947  0.67164588 ..., -0.92668557  0.46153927\n",
      "   0.74277234]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.02885439 -0.00585808 -0.04069098 ...,  0.00699186 -0.05404399\n",
      "   0.02202466]\n",
      " [ 0.06509807 -0.02223223  0.02077565 ..., -0.00037639  0.04783005\n",
      "  -0.00804034]\n",
      " [ 0.02742642  0.05705827 -0.03898749 ...,  0.00316557  0.06507974\n",
      "  -0.01836298]\n",
      " ..., \n",
      " [-0.00754741 -0.04928695  0.06990113 ..., -0.06414328 -0.06926721\n",
      "  -0.05963309]\n",
      " [-0.06505661 -0.0187963   0.05553678 ..., -0.00105456 -0.04487306\n",
      "   0.05926735]\n",
      " [ 0.05089028  0.03582108 -0.05539448 ...,  0.00139427  0.06008063\n",
      "   0.05134535]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "Shape:  (400, 800)\n",
      "[[ 0.06411717 -0.059903    0.05128169 ...,  0.03041378  0.05353859\n",
      "   0.0544473 ]\n",
      " [ 0.06030189  0.02777085  0.0119326  ..., -0.00028028 -0.04805488\n",
      "  -0.03846691]\n",
      " [-0.02244554  0.04396319  0.06136966 ...,  0.02755227  0.01427022\n",
      "   0.0558702 ]\n",
      " ..., \n",
      " [-0.0622274  -0.03975505 -0.03073074 ...,  0.0241474  -0.04965901\n",
      "  -0.00885601]\n",
      " [ 0.0227494  -0.0019692  -0.05720998 ..., -0.03900233 -0.02878788\n",
      "  -0.05673516]\n",
      " [-0.04740062  0.00044262  0.0118935  ...,  0.02134912  0.04072783\n",
      "   0.00831893]]\n",
      "Variable:  rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "Shape:  (800,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Variable:  W_out:0\n",
      "Shape:  (200, 10000)\n",
      "[[ 0.22999096 -0.98074627  0.2485311  ...,  0.31636119  0.64180779\n",
      "   0.48168755]\n",
      " [ 0.02860689  0.14025211  0.73703051 ...,  0.55039978 -0.11227965\n",
      "   0.06153202]\n",
      " [-0.74636269 -0.50103402  0.13248014 ..., -0.45557976  0.15847898\n",
      "   0.09724474]\n",
      " ..., \n",
      " [-0.21404767  0.9171977  -0.29322004 ..., -0.82013679  0.63434672\n",
      "  -0.56355882]\n",
      " [ 0.83722639 -0.26542854 -0.89726591 ..., -0.59170508  0.18550444\n",
      "   0.48118067]\n",
      " [ 0.55475283 -0.74218535 -0.24593616 ..., -0.88373137 -0.86611366\n",
      "  -0.00220013]]\n",
      "Variable:  b_out:0\n",
      "Shape:  (10000,)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    session.run(initializer)\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = session.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(\"Variable: \", k)\n",
    "        print(\"Shape: \", v.shape)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a few unit tests below to verify some *very* basic properties of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 3.315s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error messages are intentionally somewhat spare, and that passing tests are no guarantee of model correctness! Your best chance of success is through careful coding and understanding of how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 5 notebook.\n",
    "\n",
    "Particularly, `utils.rnnlm_batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:  \n",
    "*(Ignore the ugly formatter code.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout,\n",
    "                     lm.initial_h_:h}\n",
    "        cost, h, _ = session.run([loss, lm.final_h_, train_op],feed_dict=feed_dict)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to verify your implementation of `run_epoch`, and to test your RNN on a (very simple) toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 0]: seen 50 words at 43.1 wps, loss = 2.028\n",
      "[batch 98]: seen 4950 words at 2284.2 wps, loss = 1.114\n",
      "[batch 210]: seen 10550 words at 3329.2 wps, loss = 0.959\n",
      "[batch 346]: seen 17350 words at 4155.3 wps, loss = 1.024\n",
      "[batch 481]: seen 24100 words at 4656.4 wps, loss = 0.983\n",
      "[batch 615]: seen 30800 words at 4987.1 wps, loss = 0.943\n",
      "[batch 751]: seen 37600 words at 5237.5 wps, loss = 0.954\n",
      "[batch 887]: seen 44400 words at 5425.6 wps, loss = 0.964\n",
      "[batch 1024]: seen 51250 words at 5579.2 wps, loss = 0.954\n",
      "[batch 1160]: seen 58050 words at 5697.4 wps, loss = 0.943\n",
      "[batch 1296]: seen 64850 words at 5794.1 wps, loss = 0.930\n",
      "[batch 1431]: seen 71600 words at 5872.2 wps, loss = 0.921\n",
      "[batch 1567]: seen 78400 words at 5940.1 wps, loss = 0.917\n",
      "[batch 1702]: seen 85150 words at 5995.7 wps, loss = 0.905\n",
      "[batch 1839]: seen 92000 words at 6050.8 wps, loss = 0.909\n",
      "[batch 1976]: seen 98850 words at 6098.0 wps, loss = 0.914\n",
      "[batch 2112]: seen 105650 words at 6138.3 wps, loss = 0.906\n",
      "[batch 2245]: seen 112300 words at 6164.1 wps, loss = 0.893\n",
      "[batch 2379]: seen 119000 words at 6191.6 wps, loss = 0.883\n",
      "[batch 2516]: seen 125850 words at 6222.1 wps, loss = 0.881\n",
      "[batch 2650]: seen 132550 words at 6243.5 wps, loss = 0.886\n",
      "[batch 2786]: seen 139350 words at 6268.4 wps, loss = 0.880\n",
      "[batch 2919]: seen 146000 words at 6283.9 wps, loss = 0.875\n",
      "[batch 3055]: seen 152800 words at 6303.9 wps, loss = 0.870\n",
      "[batch 3189]: seen 159500 words at 6319.6 wps, loss = 0.874\n",
      "[batch 3323]: seen 166200 words at 6334.0 wps, loss = 0.878\n",
      "[batch 3456]: seen 172850 words at 6344.5 wps, loss = 0.889\n",
      "[batch 3587]: seen 179400 words at 6350.9 wps, loss = 0.886\n",
      "[batch 3717]: seen 185900 words at 6355.3 wps, loss = 0.880\n",
      "[batch 3844]: seen 192250 words at 6353.9 wps, loss = 0.873\n",
      "[batch 3970]: seen 198550 words at 6351.4 wps, loss = 0.870\n",
      "Train set: avg. loss: 0.018  (perplexity: 1.02)\n",
      "Test set: avg. loss: 0.038  (perplexity: 1.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 33.626s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as above, this is a *very* simple test case that does not guarantee model correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `/tmp/w266/a4_model/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in around 15 minutes on a single-core GCE instance, and reach a training set perplexity between 120-140.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Notes on Speed\n",
    "\n",
    "To speed things up, you may want to re-start your GCE instance with more CPUs. Using a 16-core machine will train *very* quickly if using a sampled softmax lost, almost as fast as a GPU. (Because of the sequential nature of the model, GPUs aren't actually much faster than CPUs for training and running RNNs.) The training code will print the words-per-second processed; with the default settings on a single core, you can expect around 8000 WPS, or up to more than 25000 WPS on a fast multi-core machine.\n",
    "\n",
    "You might also want to modify the code below to only run score_dataset at the very end, after all epochs are completed. This will speed things up significantly, since `score_dataset` uses the full softmax loss - and so often can take longer than a whole training epoch!\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "cp /tmp/w266/a4_model/rnnlm_trained* .\n",
    "git add rnnlm_trained*\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle. If you do also train a large model, please only submit the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/kalvin_kao/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv(review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_csv(business_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_review_df = review_df[review_df['stars']==5]\n",
    "five_star_review_series = five_star_review_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in five_star_review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "        #print(final_review)\n",
    "        review_list.append(final_review)\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed reviews\n",
    "#review_list = preprocess_review_series(five_star_review_series)\n",
    "review_list = preprocess_review_series(five_star_review_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49731951\n"
     ]
    }
   ],
   "source": [
    "#combined_review_list = [item for sublist in review_list for item in sublist]\n",
    "training_review_list = [item for sublist in review_list[:100000] for item in sublist]\n",
    "print(len(training_review_list))\n",
    "test_review_list = [item for sublist in review_list[100000:110000] for item in sublist]\n",
    "#unique_characters = list(set(combined_review_list))\n",
    "unique_characters = list(set(training_review_list + test_review_list))\n",
    "#len(unique_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary\n",
    "char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "#print(char_dict)\n",
    "#print(ids_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to flat (1D) np.array(int) of ids\n",
    "#add memory to VM and remove 1000 slice\n",
    "#combined_review_ids = [char_dict.get(token) for token in combined_review_list[:1000]]\n",
    "training_review_ids = [char_dict.get(token) for token in training_review_list]\n",
    "test_review_ids = [char_dict.get(token) for token in test_review_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array(training_review_ids)\n",
    "test_ids = np.array(test_review_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "#max_time = 25\n",
    "max_time = 250\n",
    "#batch_size = 100\n",
    "batch_size = 256\n",
    "#learning_rate = 0.01\n",
    "learning_rate = 0.004\n",
    "#num_epochs = 10\n",
    "num_epochs = 1\n",
    "\n",
    "# Model parameters\n",
    "#model_params = dict(V=vocab.size, \n",
    "                    #H=200, \n",
    "                    #softmax_ns=200,\n",
    "                    #num_layers=2)\n",
    "model_params = dict(V=len(unique_characters), \n",
    "                    H=1024, \n",
    "                    softmax_ns=len(unique_characters),\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/artificial_hotel_reviews/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalvin_kao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 0]: seen 64000 words at 2879.4 wps, loss = 8.205\n",
      "[batch 1]: seen 128000 words at 3188.1 wps, loss = 17.978\n",
      "[batch 2]: seen 192000 words at 3299.6 wps, loss = 20.423\n",
      "[batch 3]: seen 256000 words at 3352.6 wps, loss = 19.224\n",
      "[batch 4]: seen 320000 words at 3389.5 wps, loss = 17.527\n",
      "[batch 5]: seen 384000 words at 3424.9 wps, loss = 15.940\n",
      "[batch 6]: seen 448000 words at 3447.4 wps, loss = 14.605\n",
      "[batch 7]: seen 512000 words at 3463.1 wps, loss = 13.457\n",
      "[batch 8]: seen 576000 words at 3477.2 wps, loss = 12.466\n",
      "[batch 9]: seen 640000 words at 3486.8 wps, loss = 11.601\n",
      "[batch 10]: seen 704000 words at 3490.7 wps, loss = 10.863\n",
      "[batch 11]: seen 768000 words at 3498.8 wps, loss = 10.225\n",
      "[batch 12]: seen 832000 words at 3506.3 wps, loss = 9.670\n",
      "[batch 13]: seen 896000 words at 3510.4 wps, loss = 9.182\n",
      "[batch 14]: seen 960000 words at 3515.3 wps, loss = 8.754\n",
      "[batch 15]: seen 1024000 words at 3517.7 wps, loss = 8.377\n",
      "[batch 16]: seen 1088000 words at 3522.3 wps, loss = 8.040\n",
      "[batch 17]: seen 1152000 words at 3525.3 wps, loss = 7.740\n",
      "[batch 18]: seen 1216000 words at 3529.7 wps, loss = 7.469\n",
      "[batch 19]: seen 1280000 words at 3530.4 wps, loss = 7.224\n",
      "[batch 20]: seen 1344000 words at 3531.4 wps, loss = 7.000\n",
      "[batch 21]: seen 1408000 words at 3534.9 wps, loss = 6.795\n",
      "[batch 22]: seen 1472000 words at 3537.2 wps, loss = 6.606\n",
      "[batch 23]: seen 1536000 words at 3539.7 wps, loss = 6.432\n",
      "[batch 24]: seen 1600000 words at 3541.2 wps, loss = 6.271\n",
      "[batch 25]: seen 1664000 words at 3543.5 wps, loss = 6.122\n",
      "[batch 26]: seen 1728000 words at 3545.2 wps, loss = 5.984\n",
      "[batch 27]: seen 1792000 words at 3545.9 wps, loss = 5.854\n",
      "[batch 28]: seen 1856000 words at 3547.4 wps, loss = 5.732\n",
      "[batch 29]: seen 1920000 words at 3548.6 wps, loss = 5.619\n",
      "[batch 30]: seen 1984000 words at 3550.4 wps, loss = 5.512\n",
      "[batch 31]: seen 2048000 words at 3550.8 wps, loss = 5.411\n",
      "[batch 32]: seen 2112000 words at 3552.3 wps, loss = 5.316\n",
      "[batch 33]: seen 2176000 words at 3553.8 wps, loss = 5.227\n",
      "[batch 34]: seen 2240000 words at 3555.4 wps, loss = 5.142\n",
      "[batch 35]: seen 2304000 words at 3555.7 wps, loss = 5.062\n",
      "[batch 36]: seen 2368000 words at 3556.4 wps, loss = 4.986\n",
      "[batch 37]: seen 2432000 words at 3557.7 wps, loss = 4.914\n",
      "[batch 38]: seen 2496000 words at 3558.2 wps, loss = 4.845\n",
      "[batch 39]: seen 2560000 words at 3559.1 wps, loss = 4.779\n",
      "[batch 40]: seen 2624000 words at 3560.2 wps, loss = 4.717\n",
      "[batch 41]: seen 2688000 words at 3560.8 wps, loss = 4.657\n",
      "[batch 42]: seen 2752000 words at 3561.2 wps, loss = 4.599\n",
      "[batch 43]: seen 2816000 words at 3561.6 wps, loss = 4.544\n",
      "[batch 44]: seen 2880000 words at 3562.3 wps, loss = 4.491\n",
      "[batch 45]: seen 2944000 words at 3562.9 wps, loss = 4.440\n",
      "[batch 46]: seen 3008000 words at 3563.9 wps, loss = 4.392\n",
      "[batch 47]: seen 3072000 words at 3564.7 wps, loss = 4.345\n",
      "[batch 48]: seen 3136000 words at 3565.2 wps, loss = 4.300\n",
      "[batch 49]: seen 3200000 words at 3564.8 wps, loss = 4.256\n",
      "[batch 50]: seen 3264000 words at 3564.4 wps, loss = 4.215\n",
      "[batch 51]: seen 3328000 words at 3564.6 wps, loss = 4.174\n",
      "[batch 52]: seen 3392000 words at 3564.4 wps, loss = 4.135\n",
      "[batch 53]: seen 3456000 words at 3564.9 wps, loss = 4.098\n",
      "[batch 54]: seen 3520000 words at 3565.0 wps, loss = 4.061\n",
      "[batch 55]: seen 3584000 words at 3565.6 wps, loss = 4.026\n",
      "[batch 56]: seen 3648000 words at 3565.9 wps, loss = 3.992\n",
      "[batch 57]: seen 3712000 words at 3566.2 wps, loss = 3.960\n",
      "[batch 58]: seen 3776000 words at 3566.5 wps, loss = 3.928\n",
      "[batch 59]: seen 3840000 words at 3567.0 wps, loss = 3.897\n",
      "[batch 60]: seen 3904000 words at 3566.6 wps, loss = 3.867\n",
      "[batch 61]: seen 3968000 words at 3566.7 wps, loss = 3.838\n",
      "[batch 62]: seen 4032000 words at 3566.8 wps, loss = 3.810\n",
      "[batch 63]: seen 4096000 words at 3567.5 wps, loss = 3.782\n",
      "[batch 64]: seen 4160000 words at 3567.9 wps, loss = 3.755\n",
      "[batch 65]: seen 4224000 words at 3568.7 wps, loss = 3.730\n",
      "[batch 66]: seen 4288000 words at 3569.0 wps, loss = 3.705\n",
      "[batch 67]: seen 4352000 words at 3569.2 wps, loss = 3.680\n",
      "[batch 68]: seen 4416000 words at 3569.6 wps, loss = 3.656\n",
      "[batch 69]: seen 4480000 words at 3570.3 wps, loss = 3.633\n",
      "[batch 70]: seen 4544000 words at 3570.3 wps, loss = 3.610\n",
      "[batch 71]: seen 4608000 words at 3571.0 wps, loss = 3.588\n",
      "[batch 72]: seen 4672000 words at 3571.3 wps, loss = 3.567\n",
      "[batch 73]: seen 4736000 words at 3571.9 wps, loss = 3.545\n",
      "[batch 74]: seen 4800000 words at 3572.0 wps, loss = 3.525\n",
      "[batch 75]: seen 4864000 words at 3571.9 wps, loss = 3.505\n",
      "[batch 76]: seen 4928000 words at 3572.1 wps, loss = 3.485\n",
      "[batch 77]: seen 4992000 words at 3572.4 wps, loss = 3.466\n",
      "[batch 78]: seen 5056000 words at 3572.5 wps, loss = 3.448\n",
      "[batch 79]: seen 5120000 words at 3573.1 wps, loss = 3.430\n",
      "[batch 80]: seen 5184000 words at 3573.8 wps, loss = 3.412\n",
      "[batch 81]: seen 5248000 words at 3574.1 wps, loss = 3.394\n",
      "[batch 82]: seen 5312000 words at 3573.8 wps, loss = 3.377\n",
      "[batch 83]: seen 5376000 words at 3573.5 wps, loss = 3.361\n",
      "[batch 84]: seen 5440000 words at 3573.5 wps, loss = 3.344\n",
      "[batch 85]: seen 5504000 words at 3573.6 wps, loss = 3.328\n",
      "[batch 86]: seen 5568000 words at 3574.0 wps, loss = 3.313\n",
      "[batch 87]: seen 5632000 words at 3574.2 wps, loss = 3.298\n",
      "[batch 88]: seen 5696000 words at 3574.1 wps, loss = 3.283\n",
      "[batch 89]: seen 5760000 words at 3574.2 wps, loss = 3.268\n",
      "[batch 90]: seen 5824000 words at 3573.9 wps, loss = 3.254\n",
      "[batch 91]: seen 5888000 words at 3574.3 wps, loss = 3.239\n",
      "[batch 92]: seen 5952000 words at 3574.5 wps, loss = 3.226\n",
      "[batch 93]: seen 6016000 words at 3574.6 wps, loss = 3.212\n",
      "[batch 94]: seen 6080000 words at 3574.8 wps, loss = 3.199\n",
      "[batch 95]: seen 6144000 words at 3574.8 wps, loss = 3.186\n",
      "[batch 96]: seen 6208000 words at 3574.7 wps, loss = 3.173\n",
      "[batch 97]: seen 6272000 words at 3574.6 wps, loss = 3.161\n",
      "[batch 98]: seen 6336000 words at 3574.1 wps, loss = 3.148\n",
      "[batch 99]: seen 6400000 words at 3574.1 wps, loss = 3.136\n",
      "[batch 100]: seen 6464000 words at 3574.2 wps, loss = 3.124\n",
      "[batch 101]: seen 6528000 words at 3574.3 wps, loss = 3.112\n",
      "[batch 102]: seen 6592000 words at 3574.4 wps, loss = 3.101\n",
      "[batch 103]: seen 6656000 words at 3574.3 wps, loss = 3.089\n",
      "[batch 104]: seen 6720000 words at 3574.6 wps, loss = 3.078\n",
      "[batch 105]: seen 6784000 words at 3574.4 wps, loss = 3.067\n",
      "[batch 106]: seen 6848000 words at 3574.4 wps, loss = 3.056\n",
      "[batch 107]: seen 6912000 words at 3574.6 wps, loss = 3.046\n",
      "[batch 108]: seen 6976000 words at 3574.7 wps, loss = 3.035\n",
      "[batch 109]: seen 7040000 words at 3575.0 wps, loss = 3.025\n",
      "[batch 110]: seen 7104000 words at 3575.5 wps, loss = 3.015\n",
      "[batch 111]: seen 7168000 words at 3575.7 wps, loss = 3.005\n",
      "[batch 112]: seen 7232000 words at 3576.0 wps, loss = 2.995\n",
      "[batch 113]: seen 7296000 words at 3576.0 wps, loss = 2.986\n",
      "[batch 114]: seen 7360000 words at 3575.7 wps, loss = 2.976\n",
      "[batch 115]: seen 7424000 words at 3575.9 wps, loss = 2.967\n",
      "[batch 116]: seen 7488000 words at 3576.0 wps, loss = 2.958\n",
      "[batch 117]: seen 7552000 words at 3576.1 wps, loss = 2.948\n",
      "[batch 118]: seen 7616000 words at 3576.1 wps, loss = 2.939\n",
      "[batch 119]: seen 7680000 words at 3575.9 wps, loss = 2.930\n",
      "[batch 120]: seen 7744000 words at 3575.9 wps, loss = 2.922\n",
      "[batch 121]: seen 7808000 words at 3576.1 wps, loss = 2.913\n",
      "[batch 122]: seen 7872000 words at 3576.2 wps, loss = 2.905\n",
      "[batch 123]: seen 7936000 words at 3576.6 wps, loss = 2.897\n",
      "[batch 124]: seen 8000000 words at 3577.0 wps, loss = 2.888\n",
      "[batch 125]: seen 8064000 words at 3577.1 wps, loss = 2.880\n",
      "[batch 126]: seen 8128000 words at 3577.4 wps, loss = 2.872\n",
      "[batch 127]: seen 8192000 words at 3577.5 wps, loss = 2.865\n",
      "[batch 128]: seen 8256000 words at 3577.5 wps, loss = 2.857\n",
      "[batch 129]: seen 8320000 words at 3577.3 wps, loss = 2.849\n",
      "[batch 130]: seen 8384000 words at 3577.4 wps, loss = 2.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 131]: seen 8448000 words at 3577.3 wps, loss = 2.834\n",
      "[batch 132]: seen 8512000 words at 3577.3 wps, loss = 2.827\n",
      "[batch 133]: seen 8576000 words at 3577.3 wps, loss = 2.820\n",
      "[batch 134]: seen 8640000 words at 3577.4 wps, loss = 2.812\n",
      "[batch 135]: seen 8704000 words at 3577.3 wps, loss = 2.805\n",
      "[batch 136]: seen 8768000 words at 3577.1 wps, loss = 2.798\n",
      "[batch 137]: seen 8832000 words at 3577.1 wps, loss = 2.791\n",
      "[batch 138]: seen 8896000 words at 3576.9 wps, loss = 2.784\n",
      "[batch 139]: seen 8960000 words at 3576.9 wps, loss = 2.778\n",
      "[batch 140]: seen 9024000 words at 3577.1 wps, loss = 2.771\n",
      "[batch 141]: seen 9088000 words at 3577.3 wps, loss = 2.764\n",
      "[batch 142]: seen 9152000 words at 3577.0 wps, loss = 2.758\n",
      "[batch 143]: seen 9216000 words at 3576.7 wps, loss = 2.752\n",
      "[batch 144]: seen 9280000 words at 3576.3 wps, loss = 2.745\n",
      "[batch 145]: seen 9344000 words at 3576.1 wps, loss = 2.739\n",
      "[batch 146]: seen 9408000 words at 3576.1 wps, loss = 2.732\n",
      "[batch 147]: seen 9472000 words at 3576.4 wps, loss = 2.726\n",
      "[batch 148]: seen 9536000 words at 3576.6 wps, loss = 2.720\n",
      "[batch 149]: seen 9600000 words at 3576.7 wps, loss = 2.714\n",
      "[batch 150]: seen 9664000 words at 3576.6 wps, loss = 2.708\n",
      "[batch 151]: seen 9728000 words at 3576.5 wps, loss = 2.702\n",
      "[batch 152]: seen 9792000 words at 3576.5 wps, loss = 2.696\n",
      "[batch 153]: seen 9856000 words at 3576.7 wps, loss = 2.691\n",
      "[batch 154]: seen 9920000 words at 3576.7 wps, loss = 2.685\n",
      "[batch 155]: seen 9984000 words at 3576.6 wps, loss = 2.679\n",
      "[batch 156]: seen 10048000 words at 3576.6 wps, loss = 2.674\n",
      "[batch 157]: seen 10112000 words at 3576.3 wps, loss = 2.668\n",
      "[batch 158]: seen 10176000 words at 3576.3 wps, loss = 2.663\n",
      "[batch 159]: seen 10240000 words at 3575.9 wps, loss = 2.657\n",
      "[batch 160]: seen 10304000 words at 3576.1 wps, loss = 2.652\n",
      "[batch 161]: seen 10368000 words at 3576.2 wps, loss = 2.646\n",
      "[batch 162]: seen 10432000 words at 3576.1 wps, loss = 2.641\n",
      "[batch 163]: seen 10496000 words at 3576.1 wps, loss = 2.636\n",
      "[batch 164]: seen 10560000 words at 3576.2 wps, loss = 2.631\n",
      "[batch 165]: seen 10624000 words at 3576.2 wps, loss = 2.626\n",
      "[batch 166]: seen 10688000 words at 3576.3 wps, loss = 2.620\n",
      "[batch 167]: seen 10752000 words at 3576.4 wps, loss = 2.615\n",
      "[batch 168]: seen 10816000 words at 3576.4 wps, loss = 2.610\n",
      "[batch 169]: seen 10880000 words at 3576.2 wps, loss = 2.605\n",
      "[batch 170]: seen 10944000 words at 3576.1 wps, loss = 2.600\n",
      "[batch 171]: seen 11008000 words at 3576.0 wps, loss = 2.596\n",
      "[batch 172]: seen 11072000 words at 3576.0 wps, loss = 2.591\n",
      "[batch 173]: seen 11136000 words at 3575.9 wps, loss = 2.586\n",
      "[batch 174]: seen 11200000 words at 3575.9 wps, loss = 2.582\n",
      "[batch 175]: seen 11264000 words at 3575.7 wps, loss = 2.577\n",
      "[batch 176]: seen 11328000 words at 3575.6 wps, loss = 2.572\n",
      "[batch 177]: seen 11392000 words at 3575.6 wps, loss = 2.568\n",
      "[batch 178]: seen 11456000 words at 3575.7 wps, loss = 2.563\n",
      "[batch 179]: seen 11520000 words at 3575.6 wps, loss = 2.559\n",
      "[batch 180]: seen 11584000 words at 3575.6 wps, loss = 2.554\n",
      "[batch 181]: seen 11648000 words at 3575.7 wps, loss = 2.549\n",
      "[batch 182]: seen 11712000 words at 3575.6 wps, loss = 2.545\n",
      "[batch 183]: seen 11776000 words at 3575.5 wps, loss = 2.541\n",
      "[batch 184]: seen 11840000 words at 3575.4 wps, loss = 2.536\n",
      "[batch 185]: seen 11904000 words at 3575.3 wps, loss = 2.532\n",
      "[batch 186]: seen 11968000 words at 3575.1 wps, loss = 2.528\n",
      "[batch 187]: seen 12032000 words at 3575.2 wps, loss = 2.524\n",
      "[batch 188]: seen 12096000 words at 3575.0 wps, loss = 2.519\n",
      "[batch 189]: seen 12160000 words at 3574.9 wps, loss = 2.515\n",
      "[batch 190]: seen 12224000 words at 3575.1 wps, loss = 2.511\n",
      "[batch 191]: seen 12288000 words at 3575.2 wps, loss = 2.507\n",
      "[batch 192]: seen 12352000 words at 3575.1 wps, loss = 2.503\n",
      "[batch 193]: seen 12416000 words at 3574.9 wps, loss = 2.499\n",
      "[batch 194]: seen 12480000 words at 3574.6 wps, loss = 2.495\n",
      "[batch 195]: seen 12544000 words at 3574.3 wps, loss = 2.491\n",
      "[batch 196]: seen 12608000 words at 3574.3 wps, loss = 2.487\n",
      "[batch 197]: seen 12672000 words at 3574.5 wps, loss = 2.483\n",
      "[batch 198]: seen 12736000 words at 3574.5 wps, loss = 2.479\n",
      "[batch 199]: seen 12800000 words at 3574.5 wps, loss = 2.475\n",
      "[batch 200]: seen 12864000 words at 3574.4 wps, loss = 2.471\n",
      "[batch 201]: seen 12928000 words at 3574.3 wps, loss = 2.467\n",
      "[batch 202]: seen 12992000 words at 3574.1 wps, loss = 2.464\n",
      "[batch 203]: seen 13056000 words at 3574.1 wps, loss = 2.460\n",
      "[batch 204]: seen 13120000 words at 3574.1 wps, loss = 2.457\n",
      "[batch 205]: seen 13184000 words at 3574.2 wps, loss = 2.453\n",
      "[batch 206]: seen 13248000 words at 3574.2 wps, loss = 2.449\n",
      "[batch 207]: seen 13312000 words at 3574.2 wps, loss = 2.446\n",
      "[batch 208]: seen 13376000 words at 3574.0 wps, loss = 2.442\n",
      "[batch 209]: seen 13440000 words at 3574.0 wps, loss = 2.438\n",
      "[batch 210]: seen 13504000 words at 3574.2 wps, loss = 2.435\n",
      "[batch 211]: seen 13568000 words at 3574.1 wps, loss = 2.431\n",
      "[batch 212]: seen 13632000 words at 3574.2 wps, loss = 2.428\n",
      "[batch 213]: seen 13696000 words at 3574.2 wps, loss = 2.424\n",
      "[batch 214]: seen 13760000 words at 3574.1 wps, loss = 2.421\n",
      "[batch 215]: seen 13824000 words at 3574.2 wps, loss = 2.417\n",
      "[batch 216]: seen 13888000 words at 3574.1 wps, loss = 2.414\n",
      "[batch 217]: seen 13952000 words at 3573.9 wps, loss = 2.411\n",
      "[batch 218]: seen 14016000 words at 3574.0 wps, loss = 2.407\n",
      "[batch 219]: seen 14080000 words at 3573.8 wps, loss = 2.404\n",
      "[batch 220]: seen 14144000 words at 3574.0 wps, loss = 2.401\n",
      "[batch 221]: seen 14208000 words at 3574.1 wps, loss = 2.397\n",
      "[batch 222]: seen 14272000 words at 3574.2 wps, loss = 2.394\n",
      "[batch 223]: seen 14336000 words at 3574.3 wps, loss = 2.391\n",
      "[batch 224]: seen 14400000 words at 3574.2 wps, loss = 2.388\n",
      "[batch 225]: seen 14464000 words at 3574.2 wps, loss = 2.385\n",
      "[batch 226]: seen 14528000 words at 3574.3 wps, loss = 2.381\n",
      "[batch 227]: seen 14592000 words at 3574.4 wps, loss = 2.378\n",
      "[batch 228]: seen 14656000 words at 3574.5 wps, loss = 2.375\n",
      "[batch 229]: seen 14720000 words at 3574.5 wps, loss = 2.372\n",
      "[batch 230]: seen 14784000 words at 3574.5 wps, loss = 2.369\n",
      "[batch 231]: seen 14848000 words at 3574.5 wps, loss = 2.366\n",
      "[batch 232]: seen 14912000 words at 3574.5 wps, loss = 2.363\n",
      "[batch 233]: seen 14976000 words at 3574.7 wps, loss = 2.360\n",
      "[batch 234]: seen 15040000 words at 3574.9 wps, loss = 2.357\n",
      "[batch 235]: seen 15104000 words at 3574.9 wps, loss = 2.354\n",
      "[batch 236]: seen 15168000 words at 3574.7 wps, loss = 2.351\n",
      "[batch 237]: seen 15232000 words at 3574.7 wps, loss = 2.348\n",
      "[batch 238]: seen 15296000 words at 3574.7 wps, loss = 2.346\n",
      "[batch 239]: seen 15360000 words at 3574.8 wps, loss = 2.343\n",
      "[batch 240]: seen 15424000 words at 3575.0 wps, loss = 2.340\n",
      "[batch 241]: seen 15488000 words at 3575.1 wps, loss = 2.337\n",
      "[batch 242]: seen 15552000 words at 3575.3 wps, loss = 2.335\n",
      "[batch 243]: seen 15616000 words at 3575.4 wps, loss = 2.332\n",
      "[batch 244]: seen 15680000 words at 3575.5 wps, loss = 2.329\n",
      "[batch 245]: seen 15744000 words at 3575.5 wps, loss = 2.327\n",
      "[batch 246]: seen 15808000 words at 3575.4 wps, loss = 2.324\n",
      "[batch 247]: seen 15872000 words at 3575.4 wps, loss = 2.321\n",
      "[batch 248]: seen 15936000 words at 3575.3 wps, loss = 2.318\n",
      "[batch 249]: seen 16000000 words at 3575.3 wps, loss = 2.316\n",
      "[batch 250]: seen 16064000 words at 3575.4 wps, loss = 2.313\n",
      "[batch 251]: seen 16128000 words at 3575.5 wps, loss = 2.310\n",
      "[batch 252]: seen 16192000 words at 3575.5 wps, loss = 2.308\n",
      "[batch 253]: seen 16256000 words at 3575.6 wps, loss = 2.305\n",
      "[batch 254]: seen 16320000 words at 3575.6 wps, loss = 2.303\n",
      "[batch 255]: seen 16384000 words at 3575.5 wps, loss = 2.300\n",
      "[batch 256]: seen 16448000 words at 3575.5 wps, loss = 2.297\n",
      "[batch 257]: seen 16512000 words at 3575.6 wps, loss = 2.295\n",
      "[batch 258]: seen 16576000 words at 3575.6 wps, loss = 2.292\n",
      "[batch 259]: seen 16640000 words at 3575.6 wps, loss = 2.290\n",
      "[batch 260]: seen 16704000 words at 3575.8 wps, loss = 2.287\n",
      "[batch 261]: seen 16768000 words at 3575.8 wps, loss = 2.285\n",
      "[batch 262]: seen 16832000 words at 3575.8 wps, loss = 2.282\n",
      "[batch 263]: seen 16896000 words at 3575.8 wps, loss = 2.280\n",
      "[batch 264]: seen 16960000 words at 3575.9 wps, loss = 2.277\n",
      "[batch 265]: seen 17024000 words at 3575.9 wps, loss = 2.275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 266]: seen 17088000 words at 3576.0 wps, loss = 2.272\n",
      "[batch 267]: seen 17152000 words at 3576.0 wps, loss = 2.270\n",
      "[batch 268]: seen 17216000 words at 3576.1 wps, loss = 2.268\n",
      "[batch 269]: seen 17280000 words at 3576.2 wps, loss = 2.265\n",
      "[batch 270]: seen 17344000 words at 3576.1 wps, loss = 2.263\n",
      "[batch 271]: seen 17408000 words at 3576.0 wps, loss = 2.260\n",
      "[batch 272]: seen 17472000 words at 3576.0 wps, loss = 2.258\n",
      "[batch 273]: seen 17536000 words at 3576.0 wps, loss = 2.256\n",
      "[batch 274]: seen 17600000 words at 3576.1 wps, loss = 2.254\n",
      "[batch 275]: seen 17664000 words at 3576.2 wps, loss = 2.251\n",
      "[batch 276]: seen 17728000 words at 3576.3 wps, loss = 2.249\n",
      "[batch 277]: seen 17792000 words at 3576.2 wps, loss = 2.246\n",
      "[batch 278]: seen 17856000 words at 3576.1 wps, loss = 2.244\n",
      "[batch 279]: seen 17920000 words at 3576.1 wps, loss = 2.242\n",
      "[batch 280]: seen 17984000 words at 3576.3 wps, loss = 2.240\n",
      "[batch 281]: seen 18048000 words at 3576.4 wps, loss = 2.237\n",
      "[batch 282]: seen 18112000 words at 3576.4 wps, loss = 2.235\n",
      "[batch 283]: seen 18176000 words at 3576.4 wps, loss = 2.233\n",
      "[batch 284]: seen 18240000 words at 3576.3 wps, loss = 2.231\n",
      "[batch 285]: seen 18304000 words at 3576.3 wps, loss = 2.228\n",
      "[batch 286]: seen 18368000 words at 3576.4 wps, loss = 2.226\n",
      "[batch 287]: seen 18432000 words at 3576.4 wps, loss = 2.224\n",
      "[batch 288]: seen 18496000 words at 3576.4 wps, loss = 2.222\n",
      "[batch 289]: seen 18560000 words at 3576.5 wps, loss = 2.219\n",
      "[batch 290]: seen 18624000 words at 3576.5 wps, loss = 2.217\n",
      "[batch 291]: seen 18688000 words at 3576.5 wps, loss = 2.215\n",
      "[batch 292]: seen 18752000 words at 3576.6 wps, loss = 2.213\n",
      "[batch 293]: seen 18816000 words at 3576.5 wps, loss = 2.211\n",
      "[batch 294]: seen 18880000 words at 3576.5 wps, loss = 2.209\n",
      "[batch 295]: seen 18944000 words at 3576.5 wps, loss = 2.206\n",
      "[batch 296]: seen 19008000 words at 3576.5 wps, loss = 2.204\n",
      "[batch 297]: seen 19072000 words at 3576.5 wps, loss = 2.202\n",
      "[batch 298]: seen 19136000 words at 3576.5 wps, loss = 2.200\n",
      "[batch 299]: seen 19200000 words at 3576.5 wps, loss = 2.198\n",
      "[batch 300]: seen 19264000 words at 3576.6 wps, loss = 2.196\n",
      "[batch 301]: seen 19328000 words at 3576.7 wps, loss = 2.194\n",
      "[batch 302]: seen 19392000 words at 3576.8 wps, loss = 2.192\n",
      "[batch 303]: seen 19456000 words at 3576.8 wps, loss = 2.190\n",
      "[batch 304]: seen 19520000 words at 3576.9 wps, loss = 2.188\n",
      "[batch 305]: seen 19584000 words at 3576.9 wps, loss = 2.186\n",
      "[batch 306]: seen 19648000 words at 3577.0 wps, loss = 2.184\n",
      "[batch 307]: seen 19712000 words at 3577.0 wps, loss = 2.182\n",
      "[batch 308]: seen 19776000 words at 3577.0 wps, loss = 2.180\n",
      "[batch 309]: seen 19840000 words at 3576.7 wps, loss = 2.178\n",
      "[batch 310]: seen 19904000 words at 3576.4 wps, loss = 2.176\n",
      "[batch 311]: seen 19968000 words at 3576.3 wps, loss = 2.174\n",
      "[batch 312]: seen 20032000 words at 3576.2 wps, loss = 2.172\n",
      "[batch 313]: seen 20096000 words at 3576.1 wps, loss = 2.170\n",
      "[batch 314]: seen 20160000 words at 3576.1 wps, loss = 2.168\n",
      "[batch 315]: seen 20224000 words at 3576.2 wps, loss = 2.167\n",
      "[batch 316]: seen 20288000 words at 3576.3 wps, loss = 2.165\n",
      "[batch 317]: seen 20352000 words at 3576.2 wps, loss = 2.163\n",
      "[batch 318]: seen 20416000 words at 3576.1 wps, loss = 2.161\n",
      "[batch 319]: seen 20480000 words at 3576.1 wps, loss = 2.159\n",
      "[batch 320]: seen 20544000 words at 3576.1 wps, loss = 2.157\n",
      "[batch 321]: seen 20608000 words at 3576.1 wps, loss = 2.155\n",
      "[batch 322]: seen 20672000 words at 3576.0 wps, loss = 2.153\n",
      "[batch 323]: seen 20736000 words at 3576.0 wps, loss = 2.151\n",
      "[batch 324]: seen 20800000 words at 3576.0 wps, loss = 2.149\n",
      "[batch 325]: seen 20864000 words at 3576.0 wps, loss = 2.147\n",
      "[batch 326]: seen 20928000 words at 3575.9 wps, loss = 2.146\n",
      "[batch 327]: seen 20992000 words at 3576.0 wps, loss = 2.144\n",
      "[batch 328]: seen 21056000 words at 3576.1 wps, loss = 2.142\n",
      "[batch 329]: seen 21120000 words at 3576.1 wps, loss = 2.140\n",
      "[batch 330]: seen 21184000 words at 3576.1 wps, loss = 2.138\n",
      "[batch 331]: seen 21248000 words at 3575.9 wps, loss = 2.136\n",
      "[batch 332]: seen 21312000 words at 3575.9 wps, loss = 2.135\n",
      "[batch 333]: seen 21376000 words at 3575.8 wps, loss = 2.133\n",
      "[batch 334]: seen 21440000 words at 3575.9 wps, loss = 2.131\n",
      "[batch 335]: seen 21504000 words at 3575.8 wps, loss = 2.129\n",
      "[batch 336]: seen 21568000 words at 3575.8 wps, loss = 2.127\n",
      "[batch 337]: seen 21632000 words at 3575.8 wps, loss = 2.126\n",
      "[batch 338]: seen 21696000 words at 3575.7 wps, loss = 2.124\n",
      "[batch 339]: seen 21760000 words at 3575.5 wps, loss = 2.122\n",
      "[batch 340]: seen 21824000 words at 3575.5 wps, loss = 2.121\n",
      "[batch 341]: seen 21888000 words at 3575.5 wps, loss = 2.119\n",
      "[batch 342]: seen 21952000 words at 3575.5 wps, loss = 2.117\n",
      "[batch 343]: seen 22016000 words at 3575.3 wps, loss = 2.116\n",
      "[batch 344]: seen 22080000 words at 3575.2 wps, loss = 2.114\n",
      "[batch 345]: seen 22144000 words at 3575.2 wps, loss = 2.112\n",
      "[batch 346]: seen 22208000 words at 3575.0 wps, loss = 2.110\n",
      "[batch 347]: seen 22272000 words at 3574.9 wps, loss = 2.109\n",
      "[batch 348]: seen 22336000 words at 3574.8 wps, loss = 2.107\n",
      "[batch 349]: seen 22400000 words at 3574.7 wps, loss = 2.105\n",
      "[batch 350]: seen 22464000 words at 3574.5 wps, loss = 2.104\n",
      "[batch 351]: seen 22528000 words at 3574.5 wps, loss = 2.102\n",
      "[batch 352]: seen 22592000 words at 3574.4 wps, loss = 2.100\n",
      "[batch 353]: seen 22656000 words at 3574.3 wps, loss = 2.099\n",
      "[batch 354]: seen 22720000 words at 3574.1 wps, loss = 2.097\n",
      "[batch 355]: seen 22784000 words at 3573.9 wps, loss = 2.096\n",
      "[batch 356]: seen 22848000 words at 3573.8 wps, loss = 2.094\n",
      "[batch 357]: seen 22912000 words at 3573.8 wps, loss = 2.092\n",
      "[batch 358]: seen 22976000 words at 3573.7 wps, loss = 2.091\n",
      "[batch 359]: seen 23040000 words at 3573.5 wps, loss = 2.089\n",
      "[batch 360]: seen 23104000 words at 3573.5 wps, loss = 2.087\n",
      "[batch 361]: seen 23168000 words at 3573.5 wps, loss = 2.086\n",
      "[batch 362]: seen 23232000 words at 3573.5 wps, loss = 2.084\n",
      "[batch 363]: seen 23296000 words at 3573.4 wps, loss = 2.083\n",
      "[batch 364]: seen 23360000 words at 3573.2 wps, loss = 2.081\n",
      "[batch 365]: seen 23424000 words at 3573.0 wps, loss = 2.080\n",
      "[batch 366]: seen 23488000 words at 3572.8 wps, loss = 2.078\n",
      "[batch 367]: seen 23552000 words at 3572.7 wps, loss = 2.077\n",
      "[batch 368]: seen 23616000 words at 3572.8 wps, loss = 2.075\n",
      "[batch 369]: seen 23680000 words at 3572.7 wps, loss = 2.074\n",
      "[batch 370]: seen 23744000 words at 3572.5 wps, loss = 2.072\n",
      "[batch 371]: seen 23808000 words at 3572.5 wps, loss = 2.071\n",
      "[batch 372]: seen 23872000 words at 3572.3 wps, loss = 2.069\n",
      "[batch 373]: seen 23936000 words at 3572.1 wps, loss = 2.068\n",
      "[batch 374]: seen 24000000 words at 3572.0 wps, loss = 2.066\n",
      "[batch 375]: seen 24064000 words at 3572.0 wps, loss = 2.065\n",
      "[batch 376]: seen 24128000 words at 3571.8 wps, loss = 2.063\n",
      "[batch 377]: seen 24192000 words at 3571.6 wps, loss = 2.062\n",
      "[batch 378]: seen 24256000 words at 3571.5 wps, loss = 2.060\n",
      "[batch 379]: seen 24320000 words at 3571.4 wps, loss = 2.059\n",
      "[batch 380]: seen 24384000 words at 3571.3 wps, loss = 2.057\n",
      "[batch 381]: seen 24448000 words at 3571.2 wps, loss = 2.056\n",
      "[batch 382]: seen 24512000 words at 3571.1 wps, loss = 2.054\n",
      "[batch 383]: seen 24576000 words at 3571.0 wps, loss = 2.053\n",
      "[batch 384]: seen 24640000 words at 3570.9 wps, loss = 2.051\n",
      "[batch 385]: seen 24704000 words at 3570.8 wps, loss = 2.050\n",
      "[batch 386]: seen 24768000 words at 3570.6 wps, loss = 2.049\n",
      "[batch 387]: seen 24832000 words at 3570.5 wps, loss = 2.047\n",
      "[batch 388]: seen 24896000 words at 3570.3 wps, loss = 2.046\n",
      "[batch 389]: seen 24960000 words at 3570.2 wps, loss = 2.044\n",
      "[batch 390]: seen 25024000 words at 3570.0 wps, loss = 2.043\n",
      "[batch 391]: seen 25088000 words at 3570.0 wps, loss = 2.041\n",
      "[batch 392]: seen 25152000 words at 3570.0 wps, loss = 2.040\n",
      "[batch 393]: seen 25216000 words at 3570.1 wps, loss = 2.039\n",
      "[batch 394]: seen 25280000 words at 3570.0 wps, loss = 2.037\n",
      "[batch 395]: seen 25344000 words at 3570.0 wps, loss = 2.036\n",
      "[batch 396]: seen 25408000 words at 3569.9 wps, loss = 2.034\n",
      "[batch 397]: seen 25472000 words at 3569.8 wps, loss = 2.033\n",
      "[batch 398]: seen 25536000 words at 3569.8 wps, loss = 2.032\n",
      "[batch 399]: seen 25600000 words at 3569.7 wps, loss = 2.030\n",
      "[batch 400]: seen 25664000 words at 3569.7 wps, loss = 2.029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 401]: seen 25728000 words at 3569.7 wps, loss = 2.028\n",
      "[batch 402]: seen 25792000 words at 3569.6 wps, loss = 2.026\n",
      "[batch 403]: seen 25856000 words at 3569.5 wps, loss = 2.025\n",
      "[batch 404]: seen 25920000 words at 3569.3 wps, loss = 2.024\n",
      "[batch 405]: seen 25984000 words at 3569.3 wps, loss = 2.022\n",
      "[batch 406]: seen 26048000 words at 3569.3 wps, loss = 2.021\n",
      "[batch 407]: seen 26112000 words at 3569.2 wps, loss = 2.020\n",
      "[batch 408]: seen 26176000 words at 3569.0 wps, loss = 2.019\n",
      "[batch 409]: seen 26240000 words at 3569.1 wps, loss = 2.017\n",
      "[batch 410]: seen 26304000 words at 3568.9 wps, loss = 2.016\n",
      "[batch 411]: seen 26368000 words at 3568.9 wps, loss = 2.015\n",
      "[batch 412]: seen 26432000 words at 3568.8 wps, loss = 2.014\n",
      "[batch 413]: seen 26496000 words at 3568.9 wps, loss = 2.012\n",
      "[batch 414]: seen 26560000 words at 3568.8 wps, loss = 2.011\n",
      "[batch 415]: seen 26624000 words at 3568.8 wps, loss = 2.010\n",
      "[batch 416]: seen 26688000 words at 3568.7 wps, loss = 2.008\n",
      "[batch 417]: seen 26752000 words at 3568.6 wps, loss = 2.007\n",
      "[batch 418]: seen 26816000 words at 3568.5 wps, loss = 2.006\n",
      "[batch 419]: seen 26880000 words at 3568.4 wps, loss = 2.005\n",
      "[batch 420]: seen 26944000 words at 3568.4 wps, loss = 2.003\n",
      "[batch 421]: seen 27008000 words at 3568.4 wps, loss = 2.002\n",
      "[batch 422]: seen 27072000 words at 3568.4 wps, loss = 2.001\n",
      "[batch 423]: seen 27136000 words at 3568.3 wps, loss = 2.000\n",
      "[batch 424]: seen 27200000 words at 3568.1 wps, loss = 1.998\n",
      "[batch 425]: seen 27264000 words at 3568.0 wps, loss = 1.997\n",
      "[batch 426]: seen 27328000 words at 3567.9 wps, loss = 1.996\n",
      "[batch 427]: seen 27392000 words at 3567.9 wps, loss = 1.995\n",
      "[batch 428]: seen 27456000 words at 3567.8 wps, loss = 1.993\n",
      "[batch 429]: seen 27520000 words at 3567.8 wps, loss = 1.992\n",
      "[batch 430]: seen 27584000 words at 3567.6 wps, loss = 1.991\n",
      "[batch 431]: seen 27648000 words at 3567.4 wps, loss = 1.990\n",
      "[batch 432]: seen 27712000 words at 3567.3 wps, loss = 1.988\n",
      "[batch 433]: seen 27776000 words at 3567.1 wps, loss = 1.987\n",
      "[batch 434]: seen 27840000 words at 3567.1 wps, loss = 1.986\n",
      "[batch 435]: seen 27904000 words at 3567.1 wps, loss = 1.985\n",
      "[batch 436]: seen 27968000 words at 3567.0 wps, loss = 1.983\n",
      "[batch 437]: seen 28032000 words at 3566.9 wps, loss = 1.982\n",
      "[batch 438]: seen 28096000 words at 3566.7 wps, loss = 1.981\n",
      "[batch 439]: seen 28160000 words at 3566.6 wps, loss = 1.980\n",
      "[batch 440]: seen 28224000 words at 3566.5 wps, loss = 1.978\n",
      "[batch 441]: seen 28288000 words at 3566.5 wps, loss = 1.977\n",
      "[batch 442]: seen 28352000 words at 3566.5 wps, loss = 1.976\n",
      "[batch 443]: seen 28416000 words at 3566.4 wps, loss = 1.975\n",
      "[batch 444]: seen 28480000 words at 3566.3 wps, loss = 1.974\n",
      "[batch 445]: seen 28544000 words at 3566.1 wps, loss = 1.972\n",
      "[batch 446]: seen 28608000 words at 3565.9 wps, loss = 1.971\n",
      "[batch 447]: seen 28672000 words at 3565.9 wps, loss = 1.970\n",
      "[batch 448]: seen 28736000 words at 3565.8 wps, loss = 1.969\n",
      "[batch 449]: seen 28800000 words at 3565.7 wps, loss = 1.968\n",
      "[batch 450]: seen 28864000 words at 3565.5 wps, loss = 1.967\n",
      "[batch 451]: seen 28928000 words at 3565.4 wps, loss = 1.966\n",
      "[batch 452]: seen 28992000 words at 3565.3 wps, loss = 1.965\n",
      "[batch 453]: seen 29056000 words at 3565.3 wps, loss = 1.963\n",
      "[batch 454]: seen 29120000 words at 3565.2 wps, loss = 1.962\n",
      "[batch 455]: seen 29184000 words at 3564.7 wps, loss = 1.961\n",
      "[batch 456]: seen 29248000 words at 3564.3 wps, loss = 1.960\n",
      "[batch 457]: seen 29312000 words at 3564.2 wps, loss = 1.959\n",
      "[batch 458]: seen 29376000 words at 3564.1 wps, loss = 1.958\n",
      "[batch 459]: seen 29440000 words at 3564.0 wps, loss = 1.957\n",
      "[batch 460]: seen 29504000 words at 3563.9 wps, loss = 1.956\n",
      "[batch 461]: seen 29568000 words at 3563.9 wps, loss = 1.955\n",
      "[batch 462]: seen 29632000 words at 3563.9 wps, loss = 1.954\n",
      "[batch 463]: seen 29696000 words at 3563.9 wps, loss = 1.953\n",
      "[batch 464]: seen 29760000 words at 3563.9 wps, loss = 1.952\n",
      "[batch 465]: seen 29824000 words at 3563.9 wps, loss = 1.950\n",
      "[batch 466]: seen 29888000 words at 3563.9 wps, loss = 1.949\n",
      "[batch 467]: seen 29952000 words at 3563.8 wps, loss = 1.948\n",
      "[batch 468]: seen 30016000 words at 3563.7 wps, loss = 1.947\n",
      "[batch 469]: seen 30080000 words at 3563.6 wps, loss = 1.946\n",
      "[batch 470]: seen 30144000 words at 3563.6 wps, loss = 1.945\n",
      "[batch 471]: seen 30208000 words at 3563.5 wps, loss = 1.944\n",
      "[batch 472]: seen 30272000 words at 3563.6 wps, loss = 1.943\n",
      "[batch 473]: seen 30336000 words at 3563.6 wps, loss = 1.942\n",
      "[batch 474]: seen 30400000 words at 3563.5 wps, loss = 1.941\n",
      "[batch 475]: seen 30464000 words at 3563.5 wps, loss = 1.940\n",
      "[batch 476]: seen 30528000 words at 3563.4 wps, loss = 1.939\n",
      "[batch 477]: seen 30592000 words at 3563.4 wps, loss = 1.937\n",
      "[batch 478]: seen 30656000 words at 3563.3 wps, loss = 1.936\n",
      "[batch 479]: seen 30720000 words at 3563.4 wps, loss = 1.935\n",
      "[batch 480]: seen 30784000 words at 3563.4 wps, loss = 1.934\n",
      "[batch 481]: seen 30848000 words at 3563.4 wps, loss = 1.933\n",
      "[batch 482]: seen 30912000 words at 3563.4 wps, loss = 1.932\n",
      "[batch 483]: seen 30976000 words at 3563.3 wps, loss = 1.931\n",
      "[batch 484]: seen 31040000 words at 3563.2 wps, loss = 1.930\n",
      "[batch 485]: seen 31104000 words at 3563.1 wps, loss = 1.929\n",
      "[batch 486]: seen 31168000 words at 3563.1 wps, loss = 1.928\n",
      "[batch 487]: seen 31232000 words at 3563.1 wps, loss = 1.927\n",
      "[batch 488]: seen 31296000 words at 3563.0 wps, loss = 1.926\n",
      "[batch 489]: seen 31360000 words at 3563.0 wps, loss = 1.925\n",
      "[batch 490]: seen 31424000 words at 3562.9 wps, loss = 1.924\n",
      "[batch 491]: seen 31488000 words at 3563.0 wps, loss = 1.923\n",
      "[batch 492]: seen 31552000 words at 3562.9 wps, loss = 1.922\n",
      "[batch 493]: seen 31616000 words at 3562.8 wps, loss = 1.921\n",
      "[batch 494]: seen 31680000 words at 3562.7 wps, loss = 1.920\n",
      "[batch 495]: seen 31744000 words at 3562.5 wps, loss = 1.919\n",
      "[batch 496]: seen 31808000 words at 3562.4 wps, loss = 1.918\n",
      "[batch 497]: seen 31872000 words at 3562.4 wps, loss = 1.917\n",
      "[batch 498]: seen 31936000 words at 3562.4 wps, loss = 1.916\n",
      "[batch 499]: seen 32000000 words at 3562.3 wps, loss = 1.915\n",
      "[batch 500]: seen 32064000 words at 3562.2 wps, loss = 1.914\n",
      "[batch 501]: seen 32128000 words at 3562.1 wps, loss = 1.913\n",
      "[batch 502]: seen 32192000 words at 3562.1 wps, loss = 1.912\n",
      "[batch 503]: seen 32256000 words at 3562.0 wps, loss = 1.911\n",
      "[batch 504]: seen 32320000 words at 3561.8 wps, loss = 1.910\n",
      "[batch 505]: seen 32384000 words at 3561.7 wps, loss = 1.909\n",
      "[batch 506]: seen 32448000 words at 3561.7 wps, loss = 1.908\n",
      "[batch 507]: seen 32512000 words at 3561.6 wps, loss = 1.907\n",
      "[batch 508]: seen 32576000 words at 3561.5 wps, loss = 1.906\n",
      "[batch 509]: seen 32640000 words at 3561.5 wps, loss = 1.905\n",
      "[batch 510]: seen 32704000 words at 3561.5 wps, loss = 1.904\n",
      "[batch 511]: seen 32768000 words at 3561.4 wps, loss = 1.903\n",
      "[batch 512]: seen 32832000 words at 3561.2 wps, loss = 1.902\n",
      "[batch 513]: seen 32896000 words at 3561.1 wps, loss = 1.901\n",
      "[batch 514]: seen 32960000 words at 3561.0 wps, loss = 1.900\n",
      "[batch 515]: seen 33024000 words at 3560.9 wps, loss = 1.899\n",
      "[batch 516]: seen 33088000 words at 3560.7 wps, loss = 1.898\n",
      "[batch 517]: seen 33152000 words at 3560.6 wps, loss = 1.897\n",
      "[batch 518]: seen 33216000 words at 3560.5 wps, loss = 1.896\n",
      "[batch 519]: seen 33280000 words at 3560.4 wps, loss = 1.895\n",
      "[batch 520]: seen 33344000 words at 3560.2 wps, loss = 1.895\n",
      "[batch 521]: seen 33408000 words at 3560.2 wps, loss = 1.894\n",
      "[batch 522]: seen 33472000 words at 3560.2 wps, loss = 1.893\n",
      "[batch 523]: seen 33536000 words at 3560.0 wps, loss = 1.892\n",
      "[batch 524]: seen 33600000 words at 3560.0 wps, loss = 1.891\n",
      "[batch 525]: seen 33664000 words at 3560.0 wps, loss = 1.890\n",
      "[batch 526]: seen 33728000 words at 3560.0 wps, loss = 1.889\n",
      "[batch 527]: seen 33792000 words at 3560.0 wps, loss = 1.888\n",
      "[batch 528]: seen 33856000 words at 3559.9 wps, loss = 1.887\n",
      "[batch 529]: seen 33920000 words at 3559.8 wps, loss = 1.886\n",
      "[batch 530]: seen 33984000 words at 3559.6 wps, loss = 1.885\n",
      "[batch 531]: seen 34048000 words at 3559.6 wps, loss = 1.884\n",
      "[batch 532]: seen 34112000 words at 3559.5 wps, loss = 1.883\n",
      "[batch 533]: seen 34176000 words at 3559.5 wps, loss = 1.883\n",
      "[batch 534]: seen 34240000 words at 3559.5 wps, loss = 1.882\n",
      "[batch 535]: seen 34304000 words at 3559.5 wps, loss = 1.881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 536]: seen 34368000 words at 3559.5 wps, loss = 1.880\n",
      "[batch 537]: seen 34432000 words at 3559.4 wps, loss = 1.879\n",
      "[batch 538]: seen 34496000 words at 3559.3 wps, loss = 1.878\n",
      "[batch 539]: seen 34560000 words at 3559.3 wps, loss = 1.877\n",
      "[batch 540]: seen 34624000 words at 3559.2 wps, loss = 1.876\n",
      "[batch 541]: seen 34688000 words at 3559.3 wps, loss = 1.876\n",
      "[batch 542]: seen 34752000 words at 3559.2 wps, loss = 1.875\n",
      "[batch 543]: seen 34816000 words at 3559.2 wps, loss = 1.874\n",
      "[batch 544]: seen 34880000 words at 3559.2 wps, loss = 1.873\n",
      "[batch 545]: seen 34944000 words at 3559.2 wps, loss = 1.872\n",
      "[batch 546]: seen 35008000 words at 3559.1 wps, loss = 1.871\n",
      "[batch 547]: seen 35072000 words at 3559.2 wps, loss = 1.870\n",
      "[batch 548]: seen 35136000 words at 3559.2 wps, loss = 1.870\n",
      "[batch 549]: seen 35200000 words at 3559.2 wps, loss = 1.869\n",
      "[batch 550]: seen 35264000 words at 3559.2 wps, loss = 1.868\n",
      "[batch 551]: seen 35328000 words at 3559.3 wps, loss = 1.867\n",
      "[batch 552]: seen 35392000 words at 3559.2 wps, loss = 1.866\n",
      "[batch 553]: seen 35456000 words at 3559.2 wps, loss = 1.865\n",
      "[batch 554]: seen 35520000 words at 3559.1 wps, loss = 1.864\n",
      "[batch 555]: seen 35584000 words at 3559.0 wps, loss = 1.864\n",
      "[batch 556]: seen 35648000 words at 3559.1 wps, loss = 1.863\n",
      "[batch 557]: seen 35712000 words at 3559.0 wps, loss = 1.862\n",
      "[batch 558]: seen 35776000 words at 3558.9 wps, loss = 1.861\n",
      "[batch 559]: seen 35840000 words at 3558.9 wps, loss = 1.860\n",
      "[batch 560]: seen 35904000 words at 3558.8 wps, loss = 1.860\n",
      "[batch 561]: seen 35968000 words at 3558.6 wps, loss = 1.859\n",
      "[batch 562]: seen 36032000 words at 3558.6 wps, loss = 1.858\n",
      "[batch 563]: seen 36096000 words at 3558.6 wps, loss = 1.857\n",
      "[batch 564]: seen 36160000 words at 3558.6 wps, loss = 1.856\n",
      "[batch 565]: seen 36224000 words at 3558.6 wps, loss = 1.855\n",
      "[batch 566]: seen 36288000 words at 3558.6 wps, loss = 1.855\n",
      "[batch 567]: seen 36352000 words at 3558.6 wps, loss = 1.854\n",
      "[batch 568]: seen 36416000 words at 3558.6 wps, loss = 1.853\n",
      "[batch 569]: seen 36480000 words at 3558.5 wps, loss = 1.852\n",
      "[batch 570]: seen 36544000 words at 3558.5 wps, loss = 1.851\n",
      "[batch 571]: seen 36608000 words at 3558.4 wps, loss = 1.851\n",
      "[batch 572]: seen 36672000 words at 3558.2 wps, loss = 1.850\n",
      "[batch 573]: seen 36736000 words at 3558.1 wps, loss = 1.849\n",
      "[batch 574]: seen 36800000 words at 3558.2 wps, loss = 1.848\n",
      "[batch 575]: seen 36864000 words at 3558.2 wps, loss = 1.847\n",
      "[batch 576]: seen 36928000 words at 3558.2 wps, loss = 1.847\n",
      "[batch 577]: seen 36992000 words at 3558.2 wps, loss = 1.846\n",
      "[batch 578]: seen 37056000 words at 3558.2 wps, loss = 1.845\n",
      "[batch 579]: seen 37120000 words at 3558.2 wps, loss = 1.844\n",
      "[batch 580]: seen 37184000 words at 3558.2 wps, loss = 1.843\n",
      "[batch 581]: seen 37248000 words at 3558.2 wps, loss = 1.843\n",
      "[batch 582]: seen 37312000 words at 3558.1 wps, loss = 1.842\n",
      "[batch 583]: seen 37376000 words at 3558.2 wps, loss = 1.841\n",
      "[batch 584]: seen 37440000 words at 3558.1 wps, loss = 1.840\n",
      "[batch 585]: seen 37504000 words at 3558.1 wps, loss = 1.840\n",
      "[batch 586]: seen 37568000 words at 3558.1 wps, loss = 1.839\n",
      "[batch 587]: seen 37632000 words at 3558.0 wps, loss = 1.838\n",
      "[batch 588]: seen 37696000 words at 3558.0 wps, loss = 1.837\n",
      "[batch 589]: seen 37760000 words at 3557.9 wps, loss = 1.836\n",
      "[batch 590]: seen 37824000 words at 3557.9 wps, loss = 1.836\n",
      "[batch 591]: seen 37888000 words at 3557.9 wps, loss = 1.835\n",
      "[batch 592]: seen 37952000 words at 3557.8 wps, loss = 1.834\n",
      "[batch 593]: seen 38016000 words at 3557.8 wps, loss = 1.833\n",
      "[batch 594]: seen 38080000 words at 3557.7 wps, loss = 1.833\n",
      "[batch 595]: seen 38144000 words at 3557.7 wps, loss = 1.832\n",
      "[batch 596]: seen 38208000 words at 3557.6 wps, loss = 1.831\n",
      "[batch 597]: seen 38272000 words at 3557.6 wps, loss = 1.830\n",
      "[batch 598]: seen 38336000 words at 3557.6 wps, loss = 1.829\n",
      "[batch 599]: seen 38400000 words at 3557.6 wps, loss = 1.829\n",
      "[batch 600]: seen 38464000 words at 3557.6 wps, loss = 1.828\n",
      "[batch 601]: seen 38528000 words at 3557.6 wps, loss = 1.827\n",
      "[batch 602]: seen 38592000 words at 3557.5 wps, loss = 1.826\n",
      "[batch 603]: seen 38656000 words at 3557.4 wps, loss = 1.826\n",
      "[batch 604]: seen 38720000 words at 3557.3 wps, loss = 1.825\n",
      "[batch 605]: seen 38784000 words at 3557.3 wps, loss = 1.824\n",
      "[batch 606]: seen 38848000 words at 3557.2 wps, loss = 1.823\n",
      "[batch 607]: seen 38912000 words at 3557.1 wps, loss = 1.823\n",
      "[batch 608]: seen 38976000 words at 3557.0 wps, loss = 1.822\n",
      "[batch 609]: seen 39040000 words at 3556.9 wps, loss = 1.821\n",
      "[batch 610]: seen 39104000 words at 3556.9 wps, loss = 1.820\n",
      "[batch 611]: seen 39168000 words at 3556.8 wps, loss = 1.820\n",
      "[batch 612]: seen 39232000 words at 3556.8 wps, loss = 1.819\n",
      "[batch 613]: seen 39296000 words at 3556.8 wps, loss = 1.818\n",
      "[batch 614]: seen 39360000 words at 3556.8 wps, loss = 1.817\n",
      "[batch 615]: seen 39424000 words at 3556.8 wps, loss = 1.817\n",
      "[batch 616]: seen 39488000 words at 3556.7 wps, loss = 1.816\n",
      "[batch 617]: seen 39552000 words at 3556.6 wps, loss = 1.815\n",
      "[batch 618]: seen 39616000 words at 3556.5 wps, loss = 1.814\n",
      "[batch 619]: seen 39680000 words at 3556.4 wps, loss = 1.814\n",
      "[batch 620]: seen 39744000 words at 3556.4 wps, loss = 1.813\n",
      "[batch 621]: seen 39808000 words at 3556.4 wps, loss = 1.812\n",
      "[batch 622]: seen 39872000 words at 3556.3 wps, loss = 1.811\n",
      "[batch 623]: seen 39936000 words at 3556.3 wps, loss = 1.811\n",
      "[batch 624]: seen 40000000 words at 3556.3 wps, loss = 1.810\n",
      "[batch 625]: seen 40064000 words at 3556.2 wps, loss = 1.809\n",
      "[batch 626]: seen 40128000 words at 3556.3 wps, loss = 1.809\n",
      "[batch 627]: seen 40192000 words at 3556.3 wps, loss = 1.808\n",
      "[batch 628]: seen 40256000 words at 3556.3 wps, loss = 1.807\n",
      "[batch 629]: seen 40320000 words at 3556.2 wps, loss = 1.806\n",
      "[batch 630]: seen 40384000 words at 3556.1 wps, loss = 1.806\n",
      "[batch 631]: seen 40448000 words at 3556.1 wps, loss = 1.805\n",
      "[batch 632]: seen 40512000 words at 3556.1 wps, loss = 1.804\n",
      "[batch 633]: seen 40576000 words at 3556.0 wps, loss = 1.804\n",
      "[batch 634]: seen 40640000 words at 3556.0 wps, loss = 1.803\n",
      "[batch 635]: seen 40704000 words at 3556.0 wps, loss = 1.802\n",
      "[batch 636]: seen 40768000 words at 3556.0 wps, loss = 1.801\n",
      "[batch 637]: seen 40832000 words at 3556.0 wps, loss = 1.801\n",
      "[batch 638]: seen 40896000 words at 3556.0 wps, loss = 1.800\n",
      "[batch 639]: seen 40960000 words at 3556.0 wps, loss = 1.799\n",
      "[batch 640]: seen 41024000 words at 3555.9 wps, loss = 1.799\n",
      "[batch 641]: seen 41088000 words at 3555.9 wps, loss = 1.798\n",
      "[batch 642]: seen 41152000 words at 3555.8 wps, loss = 1.797\n",
      "[batch 643]: seen 41216000 words at 3555.8 wps, loss = 1.797\n",
      "[batch 644]: seen 41280000 words at 3555.8 wps, loss = 1.796\n",
      "[batch 645]: seen 41344000 words at 3555.7 wps, loss = 1.795\n",
      "[batch 646]: seen 41408000 words at 3555.6 wps, loss = 1.794\n",
      "[batch 647]: seen 41472000 words at 3555.6 wps, loss = 1.794\n",
      "[batch 648]: seen 41536000 words at 3555.6 wps, loss = 1.793\n",
      "[batch 649]: seen 41600000 words at 3555.6 wps, loss = 1.792\n",
      "[batch 650]: seen 41664000 words at 3555.6 wps, loss = 1.792\n",
      "[batch 651]: seen 41728000 words at 3555.6 wps, loss = 1.791\n",
      "[batch 652]: seen 41792000 words at 3555.6 wps, loss = 1.790\n",
      "[batch 653]: seen 41856000 words at 3555.5 wps, loss = 1.790\n",
      "[batch 654]: seen 41920000 words at 3555.6 wps, loss = 1.789\n",
      "[batch 655]: seen 41984000 words at 3555.5 wps, loss = 1.788\n",
      "[batch 656]: seen 42048000 words at 3555.5 wps, loss = 1.787\n",
      "[batch 657]: seen 42112000 words at 3555.5 wps, loss = 1.787\n",
      "[batch 658]: seen 42176000 words at 3555.5 wps, loss = 1.786\n",
      "[batch 659]: seen 42240000 words at 3555.5 wps, loss = 1.785\n",
      "[batch 660]: seen 42304000 words at 3555.4 wps, loss = 1.785\n",
      "[batch 661]: seen 42368000 words at 3555.3 wps, loss = 1.784\n",
      "[batch 662]: seen 42432000 words at 3555.4 wps, loss = 1.783\n",
      "[batch 663]: seen 42496000 words at 3555.4 wps, loss = 1.783\n",
      "[batch 664]: seen 42560000 words at 3555.4 wps, loss = 1.782\n",
      "[batch 665]: seen 42624000 words at 3555.5 wps, loss = 1.781\n",
      "[batch 666]: seen 42688000 words at 3555.5 wps, loss = 1.781\n",
      "[batch 667]: seen 42752000 words at 3555.6 wps, loss = 1.780\n",
      "[batch 668]: seen 42816000 words at 3555.6 wps, loss = 1.780\n",
      "[batch 669]: seen 42880000 words at 3555.6 wps, loss = 1.779\n",
      "[batch 670]: seen 42944000 words at 3555.6 wps, loss = 1.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 671]: seen 43008000 words at 3555.6 wps, loss = 1.778\n",
      "[batch 672]: seen 43072000 words at 3555.7 wps, loss = 1.777\n",
      "[batch 673]: seen 43136000 words at 3555.7 wps, loss = 1.776\n",
      "[batch 674]: seen 43200000 words at 3555.7 wps, loss = 1.776\n",
      "[batch 675]: seen 43264000 words at 3555.6 wps, loss = 1.775\n",
      "[batch 676]: seen 43328000 words at 3555.5 wps, loss = 1.774\n",
      "[batch 677]: seen 43392000 words at 3555.5 wps, loss = 1.774\n",
      "[batch 678]: seen 43456000 words at 3555.5 wps, loss = 1.773\n",
      "[batch 679]: seen 43520000 words at 3555.6 wps, loss = 1.772\n",
      "[batch 680]: seen 43584000 words at 3555.7 wps, loss = 1.772\n",
      "[batch 681]: seen 43648000 words at 3555.7 wps, loss = 1.771\n",
      "[batch 682]: seen 43712000 words at 3555.6 wps, loss = 1.771\n",
      "[batch 683]: seen 43776000 words at 3555.7 wps, loss = 1.770\n",
      "[batch 684]: seen 43840000 words at 3555.7 wps, loss = 1.769\n",
      "[batch 685]: seen 43904000 words at 3555.8 wps, loss = 1.769\n",
      "[batch 686]: seen 43968000 words at 3555.9 wps, loss = 1.768\n",
      "[batch 687]: seen 44032000 words at 3555.9 wps, loss = 1.768\n",
      "[batch 688]: seen 44096000 words at 3555.9 wps, loss = 1.767\n",
      "[batch 689]: seen 44160000 words at 3556.0 wps, loss = 1.766\n",
      "[batch 690]: seen 44224000 words at 3556.0 wps, loss = 1.766\n",
      "[batch 691]: seen 44288000 words at 3556.0 wps, loss = 1.765\n",
      "[batch 692]: seen 44352000 words at 3556.0 wps, loss = 1.765\n",
      "[batch 693]: seen 44416000 words at 3556.0 wps, loss = 1.764\n",
      "[batch 694]: seen 44480000 words at 3556.1 wps, loss = 1.763\n",
      "[batch 695]: seen 44544000 words at 3556.1 wps, loss = 1.763\n",
      "[batch 696]: seen 44608000 words at 3556.1 wps, loss = 1.762\n",
      "[batch 697]: seen 44672000 words at 3556.1 wps, loss = 1.762\n",
      "[batch 698]: seen 44736000 words at 3556.1 wps, loss = 1.761\n",
      "[batch 699]: seen 44800000 words at 3556.2 wps, loss = 1.760\n",
      "[batch 700]: seen 44864000 words at 3556.3 wps, loss = 1.760\n",
      "[batch 701]: seen 44928000 words at 3556.3 wps, loss = 1.759\n",
      "[batch 702]: seen 44992000 words at 3556.3 wps, loss = 1.759\n",
      "[batch 703]: seen 45056000 words at 3556.3 wps, loss = 1.758\n",
      "[batch 704]: seen 45120000 words at 3556.2 wps, loss = 1.758\n",
      "[batch 705]: seen 45184000 words at 3556.3 wps, loss = 1.757\n",
      "[batch 706]: seen 45248000 words at 3556.3 wps, loss = 1.757\n",
      "[batch 707]: seen 45312000 words at 3556.3 wps, loss = 1.756\n",
      "[batch 708]: seen 45376000 words at 3556.3 wps, loss = 1.755\n",
      "[batch 709]: seen 45440000 words at 3556.3 wps, loss = 1.755\n",
      "[batch 710]: seen 45504000 words at 3556.3 wps, loss = 1.754\n",
      "[batch 711]: seen 45568000 words at 3556.3 wps, loss = 1.754\n",
      "[batch 712]: seen 45632000 words at 3556.3 wps, loss = 1.753\n",
      "[batch 713]: seen 45696000 words at 3556.4 wps, loss = 1.753\n",
      "[batch 714]: seen 45760000 words at 3556.5 wps, loss = 1.752\n",
      "[batch 715]: seen 45824000 words at 3556.5 wps, loss = 1.751\n",
      "[batch 716]: seen 45888000 words at 3556.6 wps, loss = 1.751\n",
      "[batch 717]: seen 45952000 words at 3556.6 wps, loss = 1.750\n",
      "[batch 718]: seen 46016000 words at 3556.7 wps, loss = 1.750\n",
      "[batch 719]: seen 46080000 words at 3556.7 wps, loss = 1.749\n",
      "[batch 720]: seen 46144000 words at 3556.7 wps, loss = 1.748\n",
      "[batch 721]: seen 46208000 words at 3556.7 wps, loss = 1.748\n",
      "[batch 722]: seen 46272000 words at 3556.8 wps, loss = 1.747\n",
      "[batch 723]: seen 46336000 words at 3556.8 wps, loss = 1.747\n",
      "[batch 724]: seen 46400000 words at 3556.9 wps, loss = 1.746\n",
      "[batch 725]: seen 46464000 words at 3557.0 wps, loss = 1.746\n",
      "[batch 726]: seen 46528000 words at 3557.0 wps, loss = 1.745\n",
      "[batch 727]: seen 46592000 words at 3557.0 wps, loss = 1.744\n",
      "[batch 728]: seen 46656000 words at 3557.0 wps, loss = 1.744\n",
      "[batch 729]: seen 46720000 words at 3557.0 wps, loss = 1.743\n",
      "[batch 730]: seen 46784000 words at 3557.0 wps, loss = 1.743\n",
      "[batch 731]: seen 46848000 words at 3557.1 wps, loss = 1.742\n",
      "[batch 732]: seen 46912000 words at 3557.2 wps, loss = 1.742\n",
      "[batch 733]: seen 46976000 words at 3557.2 wps, loss = 1.741\n",
      "[batch 734]: seen 47040000 words at 3557.2 wps, loss = 1.741\n",
      "[batch 735]: seen 47104000 words at 3557.2 wps, loss = 1.740\n",
      "[batch 736]: seen 47168000 words at 3557.2 wps, loss = 1.739\n",
      "[batch 737]: seen 47232000 words at 3557.2 wps, loss = 1.739\n",
      "[batch 738]: seen 47296000 words at 3557.3 wps, loss = 1.738\n",
      "[batch 739]: seen 47360000 words at 3557.3 wps, loss = 1.738\n",
      "[batch 740]: seen 47424000 words at 3557.3 wps, loss = 1.737\n",
      "[batch 741]: seen 47488000 words at 3557.4 wps, loss = 1.737\n",
      "[batch 742]: seen 47552000 words at 3557.4 wps, loss = 1.736\n",
      "[batch 743]: seen 47616000 words at 3557.4 wps, loss = 1.736\n",
      "[batch 744]: seen 47680000 words at 3557.4 wps, loss = 1.735\n",
      "[batch 745]: seen 47744000 words at 3557.4 wps, loss = 1.735\n",
      "[batch 746]: seen 47808000 words at 3557.5 wps, loss = 1.734\n",
      "[batch 747]: seen 47872000 words at 3557.6 wps, loss = 1.733\n",
      "[batch 748]: seen 47936000 words at 3557.6 wps, loss = 1.733\n",
      "[batch 749]: seen 48000000 words at 3557.6 wps, loss = 1.732\n",
      "[batch 750]: seen 48064000 words at 3557.7 wps, loss = 1.732\n",
      "[batch 751]: seen 48128000 words at 3557.7 wps, loss = 1.731\n",
      "[batch 752]: seen 48192000 words at 3557.8 wps, loss = 1.731\n",
      "[batch 753]: seen 48256000 words at 3557.8 wps, loss = 1.730\n",
      "[batch 754]: seen 48320000 words at 3557.9 wps, loss = 1.730\n",
      "[batch 755]: seen 48384000 words at 3557.9 wps, loss = 1.729\n",
      "[batch 756]: seen 48448000 words at 3557.9 wps, loss = 1.729\n",
      "[batch 757]: seen 48512000 words at 3557.8 wps, loss = 1.728\n",
      "[batch 758]: seen 48576000 words at 3557.8 wps, loss = 1.727\n",
      "[batch 759]: seen 48640000 words at 3557.9 wps, loss = 1.727\n",
      "[batch 760]: seen 48704000 words at 3557.9 wps, loss = 1.726\n",
      "[batch 761]: seen 48768000 words at 3557.9 wps, loss = 1.726\n",
      "[batch 762]: seen 48832000 words at 3557.9 wps, loss = 1.725\n",
      "[batch 763]: seen 48896000 words at 3557.9 wps, loss = 1.725\n",
      "[batch 764]: seen 48960000 words at 3558.0 wps, loss = 1.724\n",
      "[batch 765]: seen 49024000 words at 3558.0 wps, loss = 1.724\n",
      "[batch 766]: seen 49088000 words at 3558.0 wps, loss = 1.723\n",
      "[batch 767]: seen 49152000 words at 3558.1 wps, loss = 1.723\n",
      "[batch 768]: seen 49216000 words at 3558.1 wps, loss = 1.722\n",
      "[batch 769]: seen 49280000 words at 3558.2 wps, loss = 1.722\n",
      "[batch 770]: seen 49344000 words at 3558.2 wps, loss = 1.721\n",
      "[batch 771]: seen 49408000 words at 3558.3 wps, loss = 1.721\n",
      "[batch 772]: seen 49472000 words at 3558.3 wps, loss = 1.720\n",
      "[batch 773]: seen 49536000 words at 3558.4 wps, loss = 1.720\n",
      "[batch 774]: seen 49600000 words at 3558.3 wps, loss = 1.719\n",
      "[batch 775]: seen 49664000 words at 3558.4 wps, loss = 1.719\n",
      "[batch 776]: seen 49728000 words at 3558.4 wps, loss = 1.718\n",
      "[epoch 1] Completed in 3:52:55\n",
      "[epoch 1] Train set: avg. loss: 1.233  (perplexity: 3.43)\n",
      "[epoch 1] Test set: avg. loss: 1.239  (perplexity: 3.45)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "    \n",
    "    #check trainable variables\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, batch_iterator=bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n",
    "    \n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = session.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "        #print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,1]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "    #lm.BuildSamplerGraph()\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "<SOR>though she was amazing. how to be happy a1 that th\n",
      "<SOR>a visit on yelp hotel, i always great and attorati\n",
      "<SOR>the other feep customer for the wait to me. i need\n",
      "<SOR>this only megan host itself was awesome enough. i \n",
      "<SOR>in the sauce in a portions and salad (awesome guy \n",
      "<SOR>this is the couple of your taken propressed by fav\n",
      "<SOR>and update i was rotate at the home crust tend wit\n",
      "<SOR>amazing beer.  my neighbor!!!!<EOR>\n",
      "<SOR>sons). i come back around that it was a must. the \n",
      "<SOR>how she can eat on the best amazing food deal (i c\n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "#max_steps = 20\n",
    "max_steps = 50\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    #w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    [char_dict.get(token) for token in test_review_list]\n",
    "    w = np.repeat([[char_dict.get('<SOR>')]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            #print(vocab.id_to_word[word_id], end=\" \")\n",
    "            print(ids_to_words[word_id], end=\"\")\n",
    "            #if (i != 0) and (word_id == vocab.START_ID):\n",
    "            if (i != 0) and (word_id == char_dict.get(\"<EOR>\")):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"once upon a time\" : -8.74\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.83\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = [\"is sentence nonsense this\", \"i drive a car\"]\n",
    "#load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -6.03\n",
      "\"the boy and the girl is\" : -5.90\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"the boy and the girl are\", \"the boy and the girl is\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 1. question(s)\n",
    "\n",
    "*According to the model, \"the boy and the girl are\" is more plausible, but the log probabilities of the two phrases are very close.  This is likely because \"is\" and \"are\" are used so similarly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -7.46\n",
      "\"peanuts are my favorite kind of vegetable\" : -7.29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"when I'm hungry I really prefer to eat\" : -7.49\n",
      "\"when I'm hungry I really prefer to drink\" : -7.52\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"peanuts are my favorite kind of nut\",\n",
    "         \"peanuts are my favorite kind of vegetable\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "sents = [\"when I'm hungry I really prefer to eat\",\n",
    "         \"when I'm hungry I really prefer to drink\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 2. question(s)\n",
    "\n",
    "*Of each pair, the sentences \"peanuts are my favorite kind of vegetable\" and \"when I'm hungry I really prefer to eat\" are more plausible according to the model.*\n",
    "\n",
    "*For the differing word in each pair of sentences, a 3-gram language model would use as context \"favorite kind of\" and \"really prefer to\", and it thus would not have enough context to correctly score the likely final word.*\n",
    "\n",
    "*For the differing word in each pair of sentences, a 5-gram language model would use as context \"are my favorite kind of\" and \"hungry I really prefer to\".  In the first pair of sentences, there is still not enough context to correctly score the next expected word.  However, in the second pair of sentences, a 5-gram language model would capture \"hungry\" in the context, and this may help it to score \"eat\" higher than \"drink\" as the next expected word, similarly to the RNN model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from Week 2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/artificial_hotel_reviews/a4_model/rnnlm_trained\n",
      "\"I have lots of plastic green square toys\" : -8.51\n",
      "\"I have lots of green square plastic toys\" : -8.56\n",
      "\"I have lots of green plastic square toys\" : -8.57\n",
      "\"I have lots of plastic square green toys\" : -8.64\n",
      "\"I have lots of square green plastic toys\" : -8.69\n",
      "\"I have lots of square plastic green toys\" : -8.71\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = prefix + list(adjs) + [noun]\n",
    "    inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
